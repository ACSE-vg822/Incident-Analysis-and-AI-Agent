Summary,Impact,Detection,Timeline,Conclusions,Actionables,Date,Component,Symptom,Service,UserImpact,RootCauseCategory,Filename
"The api-appserver cluster has been overwhelmed with expensive parsoid-batch calls from particular pages, resulting in brownouts and frontend availability at ~97%.","The mediawiki API has suffered the most impact, with internal services and external API users impacted as well. See [https://grafana.wikimedia.org/d/000000202/api-frontend-summary?orgId=1&from=1564756694178&to=1564759817502 API frontend graph], the slowdown was severe enough that a depression in request rates was observed.
[[File:2019-08-05-102230 1324x1080 scrot.png|thumb]]
<br />","Alerts for appservers ""high CPU"" went off on IRC during brownouts through the first half of the day (6-12 UTC) followed by general widespread alerts for affected internal systems (restbase, recommendation, mobileapps alerts went off) starting around 15 UTC","'''All times in UTC.'''

* 7:00 / 9:30 / 12:30 / 13.00 High CPU load API appservers alerts and recoveries for brownouts, no widespread impact
* 13:25 investigation on the general issue begins
* 13:45 weights of mw12[23].* found not to be aligned with the rest of the fleet and fixed
* 14:50 High CPU load on API appservers start coming in again, this time more severe and affecting other services
* 14:54 Low HTTP availability alerts fire, together with recommendation_api, restbase, mobileapps
* 14:55 Investigation on the ongoing incident begins
* 15:02 Recoveries start coming in by themselves, and investigation continues
* 15:34 Offending page identified as one on itwiki using Lua, and requested by Google's rest api crawler","API appservers are susceptible to storms of many long-running parsoid-batch requests, in particular external requests which end up exhausting the cluster's resources and affecting all API users.","''Explicit next steps to prevent this from happening again as much as possible, with Phabricator tasks linked for every step.''

'''NOTE''': Please add the [[phab:tag/wikimedia-incident/|#wikimedia-incident]] Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.
* Limit external requests concurrency/parallelism ({{Bug|T167906}})
* Icinga alerts for appserver high CPU should be less spammy ({{Bug|T228878}})
* Bigger idea: it would be nice if there was, aggregated across the whole cluster, some report available of what request types, users, user-agents, etc were using the most appserver CPU.
* …

{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}",2019-08-02,Api,Unknown,Mediawiki,Unknown,Unknown,2019-08-02_api-overload-parsoid.wikitext
"Table '''wb_terms''' in '''wikidatawiki''' that provides labels and description lookup of entities (but also getting labels and description of entities; works both ways) is being replaced by a set normalized tables we call '''TermStore'''. Part of '''TermStore''', is a class called <code>DatabaseTermIdsResolver</code>, picked up the right host for retrieving wikidata data in client wikis (including '''cawiki''') but didn't use the right db name -<code>wikidatawiki</code>- thus code was trying to get information from wrong database  -<code>cawiki</code>-. [https://gerrit.wikimedia.org/r/c/mediawiki/extensions/Wikibase/+/529113/1/client/includes/WikibaseClient.php The patch that fixes the issue] gives more in-depth information on it. It was deployed five hours earlier on client wikis (The patch that enabled it on client wiki is [https://gerrit.wikimedia.org/r/c/operations/mediawiki-config/+/527087 527087]) then after caches got invalidated and started to use the new store.",Some pages on '''cawiki''' were returning a 500 error due to failing to connect to the database. It potentially impacted all non-wikidata wikis but '''cawiki''' and '''ruwiki''' use wikidata extensively enough for this to cause issues (retrieving property term data is not very commons in client wikis),"We got the alert on IRC: <br>
 PROBLEM - MediaWiki exceptions and fatals per minute on graphite1004 is CRITICAL: CRITICAL: 90.00% of data above the critical threshold [50.0]","'''All times in UTC.'''

* 11:31 [https://gerrit.wikimedia.org/r/c/operations/mediawiki-config/+/527087 the config variable gets deployed]
* 17:26 '''OUTAGE BEGINS'''
* 17:33 Some mw* servers complain about having communication issues to s8 replicas 
* 17:42 Otto runs scap-file again as we were suspecting that a previous revert didn't make it to the intended servers; it didn't work:)
* 17:45 We find out that cawiki (s7) is trying to access the cawiki db on s8 (wikidata) replicas 
* 17:46 We notice that this affects maintly cawiki (catalan wikipedia)
* 17:53 We verify that some pages on cawiki return 500 errors due to database issues
* 18:00 Train gets blocked until this is resolved 
* 18:15 Effie restarts hhvm and php-fpm on app canary servers to rule out any possible cache corruptions (s7 (cawiki) and s8 are off by one); it didn't work:)
* 18:36 Reached out for more help 
* <snip mediwiki internals discussion>
* 18:57 Amir says he knows what it up
* 18:59 Reedy revert's said patch
* 19:15 Jenkins finally merges the patch and Reedy deploys to prod
* 19:17 '''OUTAGE ENDS''' (Voila)",We don't have proper knowledge of how wikidata works and interacts with wikis.,"''None for now - TBA''<br>
'''NOTE''': Please add the [[phab:tag/wikimedia-incident/|#wikimedia-incident]] Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.

{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}",2019-08-07,Database,Unknown,Database,Unknown,Unknown,2019-08-07_s8-cawiki-errors.wikitext
"For about '''30 minutes''', Logstash was not getting any messages from the MediaWiki servers.","During the Logstash outage, we were partly blind in terms of operational monitoring. It also meant developers were unable to use [[WikimediaDebug]], and unable to deploy new code for MediaWiki and most other services.

While this impacted scheduling and developer productivity, it did not directly affect end-users of any public services. Also, the logs were eventually recovered into Logstash after it was restarted (the Logstash-Kafka consumer picks up where it left off).",* Icinga alerts.,"''All times in UTC.''

* 23:24 <+icinga-wm> <code>PROBLEM - Too many messages in kafka logging-eqiad on icinga1001 is CRITICAL: cluster=misc exported_cluster=logging-eqiad group={logstash,logstash-codfw} instance=kafkamon1001:9501 job=burrow partition={0,1,2} site=eqiad topic=udp_localhost-info https://wikitech.wikimedia.org/wiki/Logstash%23Kafka_consumer_lag https://grafana.wikimedia.org/d/000000484/kafka-consumer-lag</code>
* 23:40 <fgiunchedi> Logstash stabilized and returned to normal","* Logstash consumer failed, which does not recover by itself. Details at https://phabricator.wikimedia.org/T230847#5427615","* None. See https://phabricator.wikimedia.org/T230847


[[Category:Incident documentation]]",2019-08-20,Mediawiki,Monitoring,Mediawiki,Unknown,Unknown,2019-08-20_logstash.wikitext
"A provider outage on our primary transport link between eqiad and codfw caused it to be in a constant flapping (going down and up) state.

This flapping caused routing re-convergence churn and packet loss between the two sites.

On the application level, this translated to elevated 5xx/s from Varnish from ulsfo, eqsin, and codfw from 21:20 to 21:55 UTC.  Varnish reported ""No backend"" for many of the requests.  Host checks in Icinga were flapping ""TTL exceeded"" and service checks flapping ""No route to host.""","Surfaced a bit more than 52,000 5xx responses.

https://grafana.wikimedia.org/d/000000479/frontend-traffic?panelId=2&fullscreen&from=1566594998796&to=1566597283616&var-status_type=5",Monitoring caught and reported the issue via SmokePing and Icinga.,"'''All times in UTC.'''

*2019-08-23 21:20 '''OUTAGE BEGINS'''
*21:25 Investigation begins
*21:33 Zayo (the link's provider) reports issue with service (email unnoticed)
*Lots of errors and recoveries - flapping
*21:41 Arzhel starts investigating
*21:46 Brandon called
*21:47 Decided to depool codfw (ended up not needing it)
*21:48 Arzhel promotes backup link to primary
*21:55 '''OUTAGE ENDS'''
*2019-08-25 01:37 Link stops flapping","=== What went well?===

*The root cause was quickly worked-around once the cause (network link) was identified.","'''NOTE''': Please add the [[phab:tag/wikimedia-incident/|#wikimedia-incident]] Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.

*Those two will help mitigate the consequences of an overly flapping link:
**Configure interface damping on primary links - [[phab:T196432|T196432]]
**ospf link-protection - [[phab:T167306|T167306]]
*This one will make it easier (down the road) to a non-netops to failover a link if the need arises:
**Configuration management for network operations - [[phab:T228388|T228388]]
*This one is about having better monitoring and alerting by replacing Smokeping by something Prometheus based
**Investigate/setup prometheus blackbox_exporter - [[phab:T169860|T169860]]
* 

{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}",2019-08-23,Unknown,Unknown,Unknown,Unknown,Unknown,2019-08-23_network_codfw.wikitext
"Kubernetes API server process failed to start, causing the collapse of many Kubernetes managed services and disabling launching or managing of containers for tools while the server was down.

Toolforge Kubernetes API services were down for several hours due to puppet changes that caused a CA certificate mismatch with the etcd servers.  Since it was precipitated by a restart of the service, the root cause took significant time to find and correct.  While it was down, Toolforge webservices using Kubernetes would fail to launch or restart and likely many services failed while the API server was not responding.","Toolforge Kubernetes API server was unresponsive which caused the cluster to appear completely down for all monitoring and use purposes. Users of Toolforge could not launch tools with the <code>webservice</code> command with the kubernetes backend or use kubectl to check status or manipulate their tools. A larger impact was caused when kube2proxy was unable to watch for Kubernetes changes.  It then ""started from scratch"" and crashed.  Since prometheus shows that the containers (and thus the tools themselves) were unimpacted, this seems to have taken all Kubernetes webservices offline at the proxy level.",The first notice was icinga alerting on the check against http://checker.tools.wmflabs.org/k8s/nodes/ready was our first notice.,"<!-- Tips/Reminders:
- Graphs of the error rate or other surrogate, if available, are useful as well.
- Please ensure to remove private information.
- You can link to a specific offset in SAL using the SAL tool: https://tools.wmflabs.org/sal/
  For example: https://tools.wmflabs.org/sal/production?q=synchronized&d=2012-01-01
A trivial example timeline is given below.
-->
* 18:30 maintain-kubeusers process restarts the Kubernetes API server, which fails to come back up -- kube2proxy also fails here
* 18:45 Alarms start going off for the toolschecker icinga check around ""All k8s nodes healthy""
* 18:55 It is theorized by Bryan and Andrew that puppet certs could somehow be involved, but this uses project internal certs, which complicates the possibility.
* 18:59 Bryan tries restarting the kube-apiserver process (which fails). Puppet certs are possibly written off because of the timing of the outage as well because those were changed the night before, not now.
* 19:01 Bryan notices the log message indicating the ultimate failure is: ""Failed to list *storage.StorageClass: client: etcd cluster is unavailable or misconfigured""
* 19:03 The team decides to be Brooke involved and contacts her.
* 19:07 Brooke notes that an etcd healthcheck reports success (also checks flannel in case there's some overall etcd issue--it's fine)
* 19:10 Brooke reboots the k8s master in case that helps here.
* 19:17 As Brooke comes up to speed on what's going on and has been done, she validates that the etcd database is all there, just in case.
* 19:19 An annoying behavior of the etcdctl command of the old version used on the tools-k8s-etcd servers is that it times out in 1 second. This fools the team into thinking the service is flapping or downward spiraling.
* 19:33 Brooke validates that the etcd and k8s certs never changed during puppet changes.
* 19:44 Brooke reproduces the same error message using etcd and in the etcd logs and wrongly presumes that the issue is in etcd because of this.
* 19:48 Toolforge homepage alerts as down
* 19:54 Because timeouts have been noticed, Brooke starts checking firewall stuff. The changes to the firewall don't line up with the outage and are about new puppetmasters, though.
* 19:55 Hieu starts working with Brooke on the etcd servers. They notice that timeouts do show up in the etcd logs as well as frequent raft elections.
* 20:18 Brooke reboots etcd cluster in case that might stop the churn
* 20:30 Brooke reboots the k8s master server again just to restart all services there.
* 20:50 Brooke stops etcd on tools-k8s-etcd-01 and takes a backup.
* 21:06 With a backup in place, the team reduces the cluster down to 1 node in order to elminate raft election flaps as a cause.
* 21:09 Hieu enables debugging on etcd.
* 21:24 Hieu notices that the etcdctl tool has tricked us into thinking that etcd was dropping off when really it was just slower than that tool's default timeouts.
* 21:25 The team begins trying to return etcd to its original size.
* 21:52 Brooke adds back the first node after checking a few things and discovers that she made a mistake.  This collapses the cluster (because in a 2 node etcd cluster, both nodes have to work).
* 22:09 After several experiments trying to get that node out of the cluster, Jason offers to take over for Brooke to get some rest (she was sick today).
* 00:25 Jason supports Brooke (who failed to log out anyway) with technical advice and general idiot-checks until she gets etcd working again at full capacity.  This required several experiments, but eventually called for a full disaster recovery from the backup (TODO: document this).
* 00:26 Jason and Brooke resume combing over the API server to find what changed or what is not working.
* 01:02 Brooke and Jason notice that the CA certificate did change the night before in some way that was hard to determine.
* 01:05 Brooke uses curl to highlight a reproducable error with CA trust on SSL calls from the kube API server that does not happen between etcd servers.
* 01:12 Hieu has rejoined the effort and starts looking for options to specify a CA in for etcd communication in case that's useful.
* 01:20 Alex Monk shows up and helps with methodically comparing the CA that might be used by the two servers.
* 01:28 After analyzing things a bit, Brooke swaps the CA with the one from the etcd servers at Alex's suggestion.  This works and restores communication and services.
* 01:31 Recovery alerts noted.
* 01:33 Brooke finds that puppet reverts the change and breaks things again.
* 01:36 Alex provides the possibility that we are running into the situation described here: https://phabricator.wikimedia.org/T148929#2817428
* 01:43 Based on the information and direction of Alex, Brooke places the etcd puppet CA cert in /var/lib/puppet/client/ssl/certs/ca.pem, which puppet picks up and fixes things in a stable fashion. Outage over.","""Typically badly-behaved things"" like etcd confused the solutions here. Puppet certs are dangerous. Very old Kubernetes versions have terrible log errors.","'''NOTE''': Please add the [[phab:tag/wikimedia-incident/|#wikimedia-incident]] Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.
* Document etcd cluster processes {{Phabricator|T232769}}
* Upgrade Toolforge Kubernetes (already underway) to improve etcd and Kubernetes error reporting -- and also not require restarting the API server to add a tool {{Phabricator|T214513}}
* Attempt to make kube2proxy more resilient to API server failures {{Phabricator|T232770}}
* Audit puppet CA certs.  There really shouldn't be more than one within the tools project. {{Phabricator|T232772}}
* Make Kubernetes control plane HA (already in PoC in toolsbeta, so the task is closed and waiting on the full upgrade deployment) {{Phabricator|T142862}}",2019-09-10,Kubernetes,Monitoring,Api,Unresponsive,Unknown,2019-09-10_toolforge-kubernetes.wikitext
"On Friday September 13, map servers were saturating CPU due to some badly formed requests that were not validated properly by the service. This led to partial unavailability of maps from [https://grafana.wikimedia.org/d/000000607/cluster-overview?orgId=1&var-datasource=eqiad%20prometheus%2Fops&var-cluster=maps&var-instance=All&from=1568338295941&to=1568397389789 ~4:30 UTC to ~14:30 UTC]. Situation was resolved by validating traffic at the caching layer.","Service was degraded for ~9h.

Thanks to tiles high cache hit ratio, only ~2% of requests were affected according to [https://turnilo.wikimedia.org/#webrequest_sampled_128/4/N4IgbglgzgrghgGwgLzgFwgewHYgFwhLYCmAtAMYAWcATmiADQgYC2xyOx+IAomuQHoAqgBUAwoxAAzCAjTEaUfAG1QaAJ4AHLgVZcmNYlO4B9E3sl6ASnGwBzYkryqQUNLXoEATAAYAjACcpD5BfgDMIj4+eFExPgB0UT4AWpLE2AAm3L6BwaEALJHRsVGJUakAvgC6FQxqWjquaDQQ9pKGxgQwLSaUmG6ScOQYONytkmCIMI4qICxwmlDxAO4QANYQbBkQcPGYNHYgVUzYmJ5SiFDE1UxQmkhoTi4a2txuLW1M22zYUFi4BEoaDQmhMbnQMCUt32nlAHW4lAgj0sDTeCggMy+EEMw3+3AyjnI6W2nxA2hamCyBBAtUImyR+D8Plq9Veuk2+hA33Sf1GBDMFiYdhothgCFoSPU3AACiIAKwAWUkUBh+DhRlM5g5KLZXI5vzxWJxIwBIDgUCJmVah1pSBYDLw2DFCBurnRM2c6qkCnSRIRSKh0n281hzFRBAJFzF9CYkwQ024NKYL0abHN3S4rs0rRIGQAIvreb8VMcyTniBkAMqqwEByQIYgOTJqvU/IsI4Gg8FoSH1+meJmxqYep0IBBMahQAByzvwFwQV1piLslCQK88o5dQA Turnilo]. Given the high number of tiles seen by a single user during a session, it is probably that most users were affected to some extend.","* [https://grafana.wikimedia.org/d/000000479/frontend-traffic?orgId=1&from=1568332800000&to=1568419200000&var-site=All&var-cache_type=upload&var-status_type=1&var-status_type=2&var-status_type=3&var-status_type=4 HTTP availability for Varnish] was flapping starting 4:26 UTC, getting worse by 6:49 UTC
* No page was sent, no direct alert pointing to maps / kartotherian explicitly","'''All times in UTC.'''

* ~04:20 '''OUTAGE BEGINS''' CPU saturated on maps servers (maps[12]00[1-4])
* 04:26 icinga alert about HTTP availability for Varnish
* 04:27 recovery of HTTP availability for Varnish
* 05:40 icinga alert about HTTP availability for Varnish
* 05:42 recovery of HTTP availability for Varnish
* 06:49 icinga alert about HTTP availability for Varnish, starts falling regularly from now on
* 06:52 maps identified as the cause of the above alert
* 07:15 Icinga alert for kartotherian LVS endpoint
* 08:33 kartotherian restarted on maps1003, with no effect
* 08:37 rolling restart of karotherian
* 08:45 stop tilerator on maps to help reduce load - no effect
* 08:57 kartotherian eqiad depooled, problem moves to codfw
* 08:57 identified increased occurrence of issue about parsing geojson [https://logstash.wikimedia.org/goto/6983c87f9f4f8c7bdaa16cc2a04907e8 in logs] (can't actually find that again, the graph now looks flat)
* 09:11 kartotherian eqiad repooled
* 09:24 deny access to /geoline on maps1004 - limited effect
* 09:38 deny access to /geoshape on maps1004 - seems to reduce CPU load
* 09:46 re-enabling /geoline on maps1004
* 09:54 /geoshape heavily throttled on varnish - seems to be effective ({{gerrit|536549}})
* 10:55 icinga alert for maps100[12] kartotherian endpoints health on maps1001 is CRITICAL
* 12:37 temp ban of class of urls on maps1003 nginx
* 12:56 banning more urls on maps1003
* 13:12 ban problematic URLs at varnish ({{gerrit|536583}})
* 13:38 ban problematic URLs at varnish ({{gerrit|536588}})
* 14:20 ban problematic URLs at varnish ({{gerrit|536595}})
* 14:30 '''OUTAGE ENDS'''","A [[gerrit:c/mediawiki/services/kartotherian/+/523743/4/packages/kartotherian/lib/util.js#130|bug]] was introduced when fixing linting issues to introduce the CI into the CI pipeline, this created a failure in the HTTP error handler making Kartotherian unable to validate request parameters that leads to high CPU cost and timeout. This needs to be addressed in Kartotherian itself ({{gerrit|536641}}).

The deploy of the code containing the bug occurred September 12 at 21:09 UTC.

The amount of support we have on maps does not match the exposure of the service. While the few people working on maps are dedicated to their work and doing their best, we have too many (bad) surprises. The technical stack has many known and unknown issues and our knowledge of that stack is insufficient.

The majority of maps traffic comes from other websites or apps reusing our tiles. This is allowed (at least to some extend) by [https://foundation.wikimedia.org/wiki/Maps_Terms_of_Use#Using_maps_in_third-party_services Maps Terms of Use] and was the original intent of the project. Given the amount of support we have at the moment, this might need to be revisited.","* Fix HTTP error handler in kartotherian - {{gerrit|536641}} (code merged, but needs to be tested and deployed)
*Improve testing tin kartotherian endpoints
* Review the amount of support Maps has in regard of its visibility and use cases

[[Category:Maps outages, 2019]]",2019-09-13,Cache,Unknown,Unknown,Unknown,Unknown,2019-09-13_maps.wikitext
"2 services (citoid, cxserver) in eqiad had partial failures. In the case of citoid, results were not augmented enough, in the case of cxserver, translations were not returned.","* Citoid was not augmenting results for citation with zotero data for 46 minutes.
* cxserver was not returning translations for usage in ContentTranslation in eqiad for 35 minutes.",Detection was by both by human as well as [[Icinga]].,"CoreDNS was enabled in codfw+eqiad today over the course of many hours (a process slowed down on purpose). 
Unfortunately [[Calico]] network policy rules were not correctly applied in eqiad (since the mid August migration to helmfile). The reason was the calico-policy-controller which does syncing between the Kubernetes NetworkPolicy objects and the Calico store, had configuration referencing the codfw datacenter. 

The CoreDNS change required all pods to be restarted for the change to be applied, which was done in a slow rolling restart for loop. When it was understood that there is an issue, it was stopped, saving the other services.

* 13:43 Citoid partial outage begins. No alerts, it is evident however in Grafana dashboards

* Citoid was the service that suffered the most timewise, from 13:43 - 14:29
https://grafana.wikimedia.org/d/NJkCVermz/citoid?refresh=5m&panelId=15&fullscreen&orgId=1&from=1568641410756&to=1568644177940&var-dc=eqiad%20prometheus%2Fk8s&var-service=citoid
* Cxserver was the other service that was hit, shortly after. Smaller time window, it did alert, although no SMS sent, as it was partial
https://grafana.wikimedia.org/d/F7rttgqmz/cxserver?refresh=1m&panelId=15&fullscreen&orgId=1&from=1568642277671&to=1568643894326&var-dc=eqiad%20prometheus%2Fk8s&var-service=cxserver
* Helmfile deploys were also problematic in eqiad due to tiller failing DNS requests to the Kubernetes API, touched one deploy, namely the first deploy of [[Wikifeeds]] in eqiad, no alert (and none seems to be required).
* 14:29 full recovery

The fix was merging the change mentioned below and applied using helmfile. Luckily enough, kube-system namespace was not touched yet by the breaking change, so there was no need to work around it.

'''All times in UTC.'''","''What weaknesses did we learn about and how can we address them?''

* Humans are the weakest link. The process went perfect in codfw and hence not everything was checked before applying it in eqiad.","* Fix the calico configuration https://gerrit.wikimedia.org/r/537121. {{done}}
* Monitor coreDNS. https://phabricator.wikimedia.org/T234545
* Monitor zotero indirectly via citoid https://phabricator.wikimedia.org/T234544

{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}",2019-09-16,Unknown,Unknown,Unknown,Unknown,Unknown,2019-09-16_kubernetes-dns.wikitext
"At 23:56 UTC on Friday 20th, the top of rack switch asw2-d2-eqiad went down causing all servers in rack D2 to go offline. As this switch was also row D virtual-chassis master and spine to the upstream routers all row D servers suffered a brief connectivity loss. Some services were still degraded after the situation stabilized but were eventually recovered.","#14 Hosts in D2 lost connectivity. (an-presto1001, an-worker1092, an-worker1093, analytics1076, backup1001, cloudelastic1004, cloudstore1008, cp1087, cp1088, flerovium, kafka-main1004, labstore1007, ms-be1043, ms-be1048)
#Row D hosts had their connectivity impacted (eg. packet loss) for a few seconds the time the virtual-chassis fails mastership to fpc7.
#Some services (e.g. AQS, mobileapps) continued to be impacted long after the initial blip, and even after recovery of the failed switch.  The general theme here for at least some of the nodejs services seems to be related to application-layer caching of transient DNS failures (from the very brief blip of their network reachability towards recursive DNS (and everything)), and they were fixed with service restarts.  Other related APIs and services (e.g. restbase and such, and MW APIs that interact with these things) also had alerts due to this indirectly.
#Logstash was overwhelmed by all the carnage and fell behind and threw a lot of alerts itself.  Since some services' alerting relies on logstash, this also confusingly caused logstash-derived alerts to persist long after the underlying issues were resolved.
#dbproxy1016 and dbproxy1017 (which live in D1 and D3, thus were indirectly affected by the spine failure) failed healthchecks on their primaries and failed over to their secondary choices (both of which were different ports on db1117).  We (eventually) reloaded haproxy on these hosts to recover towards the primaries.
#Cloud NFS clients lost access to dumps and scratch until those services were manually failed over. Toolforge Kubernetes clients saw steadily increasing load until they could mount the volumes again (switch recovery) even after failover because they do that until rebooted with a changed fstab.
#The maps Cloud VPS project lost NFS access to home directories and project dirs until manual failover.","The first alert came from Icinga showing commons was down, but '''the switch failure wasn't clear until the alert storm had settled and correlation could be drawn from down hosts in Icinga cross-referenced with Netbox.'''
    23:57 UTC - <+icinga-wm> PROBLEM - Host commons.wikimedia.org is DOWN: /bin/ping -n -U -w 15 -c 5 commons.wikimedia.org
    < Alert storm here >","''This is a step by step outline of what happened to cause the incident and how it was remedied.  Include the lead-up to the incident, as well as any epilogue, and clearly indicate when the user-visible outage began and ended.''
<!-- Tips/Reminders:
- Graphs of the error rate or other surrogate, if available, are useful as well.
- Please ensure to remove private information.
- You can link to a specific offset in SAL using the SAL tool: https://tools.wmflabs.org/sal/
  For example: https://tools.wmflabs.org/sal/production?q=synchronized&d=2012-01-01
A trivial example timeline is given below.
-->

'''All times in UTC.'''

*23:57 '''OUTAGE BEGINS''' <+icinga-wm> PROBLEM - Host commons.wikimedia.org is DOWN: /bin/ping -n -U -w 15 -c 5 commons.wikimedia.org
*Cascade of alerts
*23:58 <+icinga-wm> PROBLEM - Host asw2-d-eqiad is DOWN: PING CRITICAL - Packet loss = 100%
*23:59 <+icinga-wm> RECOVERY - Host asw2-d-eqiad is UP: PING OK - Packet loss = 0%, RTA = 0.80 ms
*00:07 Identified as limited to Eqiad D2
*00:17 < librenms-wmf> Critical Alert for device asw2-d-eqiad.mgmt.eqiad.wmnet - Juniper alarm active
*00:21 Chris and John called, John responds.
*00:27 Logstash identified as thrashing on non-UTF-8 messages
*00:47 AQS recovery
*01:06 John arrives onsite.
*01:15 Switch power cycled.
*01:35 Mobileapps recovery
*02:14 dbproxy failover recovery
*02:15 '''OUTAGE ENDS'''
*05:58 logstash caught up",''What weaknesses did we learn about and how can we address them?'',"*Investigate the switch failure - https://phabricator.wikimedia.org/T233645
*Make it easier to distinguish per-host icinga spam vs real whole-service-level icinga spam (cf. alerting infrastructure roadmap)
*Investigate solutions to non-UTF8 logging crashing logstash mutate plugin (and ultimately the pipeline) https://phabricator.wikimedia.org/T233662
*Emergency response to logstash being backlogged https://phabricator.wikimedia.org/T233735
*Fix MediaWiki spammy logs during such outages https://phabricator.wikimedia.org/T233739

{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}",2019-09-20,Network,Unknown,Caching,Unknown,Unknown,2019-09-20_d2_switch_failure.wikitext
s3 primary database master had a [https://en.wikipedia.org/wiki/Backup_battery RAID backup batery] failure which cause the host to completely crash. It had to be power cycle from the idrac.,"All the s3 wikis (https://raw.githubusercontent.com/wikimedia/operations-mediawiki-config/master/dblists/s3.dblist) went read-only as the master wasn't available for writes.
Reads were not affected, all the replicas were available.","* The problem was clear when we saw that db1075 reported HOST DOWN - however, that only sends an IRC alert, not a page. Masters should probably page for HOST DOWN.
* Alerts were sent to IRC and pages.
* Users reporting issues on #wikimedia-operations","'''All times in UTC.'''

* 18:40 '''OUTAGE BEGINS'''

 Sep 22 18:40:38 db1115 mysqld[1630]: OpenTable: (2003) Can't connect to MySQL server on 'db1075.eqiad.wmnet' (110 ""Connection timed out"")

* IRC logs from #wikimedia-operations:

 18:42:45 <icinga-wm> PROBLEM - Host db1075 is DOWN: PING CRITICAL - Packet loss = 100%

 18:47:03 <AntiComposite> I'm getting a warning on otrs-wiki about a high replag database lock

 18:47:40 <AntiComposite> It's also slower than usual

 18:48:52 <+icinga-wm> PROBLEM - MariaDB Slave IO: s3 on db2105 is CRITICAL: CRITICAL slave_io_state Slave_IO_Running: No, Errno: 2003, Errmsg: error reconnecting to master repl@db1075.eqiad.wmnet:3306 - retry-time: 60 maximum-retries: 86400 message: Cant connect to MySQL server on db1075.eqiad.wmnet (110 Connection timed out) https://wikitech.wikimedia.org/wiki/MariaDB/troubleshooting%23Depooling_a_slave

 18:49:22 <+icinga-wm> PROBLEM - MariaDB Slave IO: s3 on dbstore1004 is CRITICAL: CRITICAL slave_io_state Slave_IO_Running: No, Errno: 2003, Errmsg: error reconnecting to master repl@db1075.eqiad.wmnet:3306 - retry-time: 60 maximum-retries: 86400 message: Cant connect to MySQL server on db1075.eqiad.wmnet (110 Connection timed out) https://wikitech.wikimedia.org/wiki/MariaDB/troubleshooting%23Depooling_a_slave

 18:49:58 <+icinga-wm> PROBLEM - MariaDB Slave IO: s3 on db1095 is CRITICAL: CRITICAL slave_io_state Slave_IO_Running: No, Errno: 2003, Errmsg: error reconnecting to master repl@db1075.eqiad.wmnet:3306 - retry-time: 60 maximum-retries: 86400 message: Cant connect to MySQL server on db1075.eqiad.wmnet (110 Connection timed out) https://wikitech.wikimedia.org/wiki/MariaDB/troubleshooting%23Depooling_a_slave

 <More alerts arriving - not pasting them all here>

 18:55:51 <marostegui> I'm connecting

* 18:56 First SMS page sent

* 19:02 Host rebooted from the idrac after seeing it is a BBU issue and the host is not responsive

* 19:03 MySQL started and starts InnoDB recovery

* 19:04 Manual puppet run ran (but failed and went unnoticed)

* 19:06 MySQL finishes recovery

* 19:07 Manually removed read_only=ON from db1075

* 19:08 Lag still being reported

* 19:16 Manual puppet ran (success and pt-heartbeat starts)

* 19:16 '''OUTAGE ENDS'''","The master lost its BBU and that resulted on a completely host crash, which is something that has been seen before with HP hosts https://phabricator.wikimedia.org/T231638 https://phabricator.wikimedia.org/T225391

The master being unavailable means that writes cannot happen: https://grafana.wikimedia.org/d/000000278/mysql-aggregated?orgId=1&var-dc=eqiad%20prometheus%2Fops&var-group=core&var-shard=s3&var-role=master&from=1569174586143&to=1569181181947
[[File:S3-db1075.png|thumb|s3 master queries, that reflect the impact of the crash]]

This is is part of a batch of 6 servers, and 3 of them have already had BBU issues: https://phabricator.wikimedia.org/T233569 so we'd need to evaluate if what to do with then next.
Definitely replacing the current master and promoting another one which is not part of that batch is what is happening next: https://phabricator.wikimedia.org/T230783","*Implement (or refactor) a script to move replicas when the master is not available (this wasn't needed yesterday, but could be needed in future issues): https://phabricator.wikimedia.org/T196366
*Fix mediawiki heartbeat model, change pt-heartbeat model to not use super-user, avoid SPOF and switch automatically to the real master without puppet dependency: https://phabricator.wikimedia.org/T172497
*Decide what to do with the same batch of hosts that have already had BBU issues: https://phabricator.wikimedia.org/T233569
*Buy a new BBU for db1075 https://phabricator.wikimedia.org/T233567
*Remove db1075 from being a master https://phabricator.wikimedia.org/T230783
*Address mediawiki spam during readonly/master unavailable https://phabricator.wikimedia.org/T233623
*Make sure primary database masters page on HOST DOWN https://phabricator.wikimedia.org/T233684
*Better tracking of hardware errors in Netbox https://phabricator.wikimedia.org/T233774

{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}",2019-09-23,Mediawiki,Unknown,Mediawiki,Unknown,Unknown,2019-09-23_s3_primary_db_master_crash.wikitext
"During the upgrade of the WMCS OpenStack control plane, all cloud VMs presented with the wrong originating IP for outbound traffic.  That broke several services on many VMs, most importantly DNS, LDAP, and NFS.","The largest impact during this outage was NFS access from Toolforge.  That prevented many grid and k8s jobs from running properly, and also produced a torrent of alert emails.

CI tests produced incorrect failure messages for part of outage due to DNS failures.

Ssh access to most VMs was broken for about an hour.","The problem was immediately evident, as many of the issues produced shinken and icinga alerts.  The team working on the upgrade didn't respond immediately because some level of background alerting was already expected as part of the upgrade process.","'''All times in UTC, and approximate'''

* 14:00 Andrew begins upgrading OpenStack services on cloudcontrol1003, cloudcontrol1004, cloudnet1003, and cloudnet1004.  Those four hosts (as well as all cloudvirt hosts) are marked for two hours of downtime in icinga.  Horizon is put into maintenance mode.
* 14:00 - 15:30 various unexpected issues arise during upgrade (most importantly relating to the scripted Neutron schema upgrades failing) which extends the expected Horizon outage window.  Nothing user-facing (other than Horizon) is broken up to this point.
* 15:40 At this point, cloudcontrol1003 and cloudnet1003 are fully upgraded.  On Andrew's request, Arturo disables the currently-active neutron server on cloudnet1004, failing all network traffic over to cloudnet1003.  At this point, all VMs present to the outside internet (including WMF production) as originating from Neutron on cloudnet1003.
* 15:45 '''OUTAGE BEGINS'''
* 15:50 Lots of things are starting to break and throw alerts.  NFS, and the Cloud DNS recursors all use ACLs based on the origination IP of incoming traffic.  Because VMs are presenting with the wrong IP, they are unable to access either DNS or NFS.  Toolforge VMs cannot access NFS, ssh to VMs fails, and CI jobs fail due to DNS resolution failures.
* 16:00 The WMCS team identifies the issue as relating to the origination IP (thanks to additional logging channels on cloud-recursor0).  Andrew hacks the DNS recursors to allow traffic with any origination IP; this resolves some of the outage (DNS and LDAP) but NFS is still inaccessible.
* 16:20 (approximately) After a good deal of digging into logs and iptables rules, Jason determines that we should just restart all the Neutron serves.
* 16:30 '''OUTAGE for most VMs ENDS''' Once all the services are restarted, origination IPs return to normal and all communication works properly.  NFS clients nevertheless fail to reconnect to NFS servers, so the Toolforge outage continues.  Brooke and Hieu start restarting jobs and/or rebooting hosts to force NFS reconnects.
* 19:06 Krenair detects more VMs (most inside of Toolforge, some outside) with failing NFS connections and restarts them as well.  Among his reboots are is the Toolforge mail server which results in a huge backlog of pending emails (from the outage an hour previously) getting sent all at once.
* 19:15 Most toolforge VMs are rebooted, normal toolforge behavior is restored. '''USER-FACING TOOLFORGE OUTAGE ENDS'''
* 19:20 Final toolforge VM rebooted - k8s master
* 20:15 Bryan clears out the tail end of the pending mail from the queue, ending the deluge of alert emails","=== What went well? ===

* Mostly all-hands-on-deck situation and all WMCS people were involved in the operation. Team response went well.","Some Phabricator tickets opened as a result of this incident:

* Various user visible errors in Cloud VPS projects following OpenStack upgrade on 2019-10-07 https://phabricator.wikimedia.org/T234834
* CloudVPS: m5-master databases for openstack may require re-enconding https://phabricator.wikimedia.org/T234830
* nova-conductor running out of mysql connections https://phabricator.wikimedia.org/T234876
* CloudVPS: update DNS record for eqiad1 routing_source_ip https://phabricator.wikimedia.org/T234836

{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}",2019-10-07,Dns,Unknown,Unknown,Unknown,Unknown,2019-10-07_wmcs-network.wikitext
"An Equinix Singapore IXP peer flapped heavily, which overwhelmed the routing daemon on cr1-eqsin and caused all its BGP and OSPF sessions to flap or go down.

In addition to the external connectivity issues, as the primary transport link to codfw is on cr1, it caused the local caches to not be able to reach their peers in the main datacenters and serve 500 errors instead.","Estimated [https://logstash.wikimedia.org/goto/1c8019dc8c807c2cc81733a1ce6a47a0 ~170k errors] surfaced to users of eqsin PoP.

https://grafana.wikimedia.org/d/000000479/frontend-traffic?orgId=1&from=1571245200000&to=1571248800000&var-site=eqsin&var-cache_type=text&var-cache_type=upload&var-status_type=5
[[File:Matching Grafana screenshot.png|thumb]]","The following automated alerts got triggered:

*Varnish traffic drop between 30min ago and now at eqsin
*HTTP availability for Nginx -SSL terminators- at eqsin
*HTTP availability for Varnish at eqsin
*BFD status on cr1-codfw
*LVS HTTPS text-lb.eqsin.wikimedia.org - PAGE

This quickly pointed to an network issue in eqsin.

Was the alert volume manageable? yes

Did they point to the problem with as much accuracy as possible? yes","'''All times in UTC.'''

*17:15 SSL terminator alerts in eqsin fire, non-paging -- '''OUTAGE BEGINS'''
*17:28 First page fires -- LVS HTTPS text-lb.eqsin.wikimedia.org
*17:29 eqsin depooled
*17:29 Recovery on its own -- '''OUTAGE ENDS'''","===What went well?===

*The issue was quickly identified
*The issue recovered on its own","'''NOTE''': Please add the [[phab:tag/wikimedia-incident/|#wikimedia-incident]] Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.

*[[phab:T236878|T236878]] Improve resiliency of the eqsin transport link by either:
**Terminating it on cr2-eqsin
**Adding a 2nd link
**Configuring link damping
*Replace cr1-eqsin with a better router (next FY)

{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}",2019-10-16,Cache,Unknown,Unknown,Unknown,Unknown,2019-10-16_network_eqsin.wikitext
Wikidata API calls were not getting responses (getting timeouts) due to DB read load due to backported changes reducing deadlocks around writing to the new terms store for wikibase.,"Wikidata editors received timeouts to API requests, API response time for writes went through the roof. It seems like most edits from API calls were actually made, but the clients didn't get a response confirming that.","Humans in Telegram chat.
Confirmed by Addshore.","'''All times in UTC.'''

* 16:00 - Maintenance script migrating wb_terms data restarted
* 16:02	<ladsgroup@deploy1001>	Synchronized php-1.35.0-wmf.3/extensions/Wikibase: Wikibase deadlock reduction, [[gerrit:547243|Stop locking and use DISTINCT when finding used terms to delete]] (T236466) (duration: 01m 05s)
* 16:05	<ladsgroup@deploy1001>	Synchronized php-1.35.0-wmf.4/extensions/Wikibase: Wikibase deadlock reduction, [[gerrit:547244|Stop locking and use DISTINCT when finding used terms to delete]] (T234948) (duration: 01m 04s)
* 16:12 - Read rows on 1 db slave shot up https://phabricator.wikimedia.org/T236928#5620434
* ~16:18 - '''Edit rate on wikidata really started dropping'''
* 16:30 - Maintenance script migrating wb_terms data restarted (picking up code changes)?
* 16:38 - Reported timeout in UI editing on wikidata.org in Telegram chat
* 16:54 - Phabricator task created after Addshore spotted this message - https://phabricator.wikimedia.org/T236928
* 16:59	<jynus>	killed rebuildItemTerms on mwmaint1002
* ~17:00 Edit rate on wikidata recovering, but drops again - https://phabricator.wikimedia.org/T236928#5620364
* 17:26	<ladsgroup@deploy1001>	Synchronized php-1.35.0-wmf.3/extensions/Wikibase: Revert 16:02 UTC T236928 (duration: 01m 04s)
* ~17:26 '''Edit rate on wikidata recovering again'''
* 17:29	<ladsgroup@deploy1001>	Synchronized php-1.35.0-wmf.4/extensions/Wikibase: Revert 16:05 UTC T236928 (duration: 01m 05s)",* We could do with more alarms on things that often indicate a problem,"* {{done}} More wikidata alerting https://gerrit.wikimedia.org/r/#/c/547404/ https://grafana.wikimedia.org/d/TUJ0V-0Zk/wikidata-alerts
* {{done}} Add alerting to API response times for wikidata
* {{done}} Add alerting for wikidata edit rate (if below 100 per minute something somewhere is wrong)
* {{done}} Add alerting for MASSIVE database read rate on s8

{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}",2019-10-31,Api,Timeouts,Api,Unknown,Unknown,2019-10-31_wikidata.wikitext
Roll out of wmf.8 to group1 broke the world.,,"Initial indicators of the issue were picked up in logstash and via logspam-watch on mwlog1001.  A large number of Icinga alerts followed.

It seems likely that the primary issue was obscured during the initial deploy by a focus on Parsoid errors.","'''All times in UTC.'''

* 20:12 brennen: Train wmf.8 roll fowards from group0 to group1 as well (try 1) [https://tools.wmflabs.org/sal/log/AW7Sixq3vrJzePItk4h-]
* 20:12 Large amounts of logspam noticed, especially from Parsoid/PHP, and Icinga issues many alerts.
* 20:28 brennen: Train wmf.8 rolled back to just group0 [https://tools.wmflabs.org/sal/log/AW7SmYXk0fjmsHBaFE31]

[Fixes to exclude Parsoid/PHP]

* 23:30 brennen: Train wmf.8 roll fowards from group0 to group1 as well (try 2) [https://tools.wmflabs.org/sal/log/AW7TP9cdvrJzePItk_Z4]
* 23:30 '''OUTAGE BEGINS'''
* 23:30 Large spike in database errors in logstash ([https://phabricator.wikimedia.org/T239877 T239877]), shortly thereafter large amounts of Icinga alerts go off.
* 23:30+ Production group1 '''and''' group2 wikis become noticably sluggish, eventually stopping working entirely.
* 23:35 brennen: Attempted train wmf.8 roll back thwarted by canary failures [https://tools.wmflabs.org/sal/log/AW7TRL3cfYQT6VcD-zDz]
* 23:38 brennen: Train wmf.8 rolled back to just group0, again [https://tools.wmflabs.org/sal/log/AW7TR9Pk0fjmsHBaFLIt]
* 23:38 '''OUTAGE ENDS'''",,,2019-12-04,Unknown,Unknown,Unknown,Unknown,Unknown,2019-12-04_MediaWiki.wikitext
"In-progress Cite extension code unleashed a flood of ""PHP notice"" logspam.  These were all non-fatal errors, but at approximately 3,000 per hour put a burden on human log-watchers.  Hotfixes helped reducing the logspam, before the actual issue was found, fixed, and backported.","* The 1.35.0-wmf.8 train was delayed for several hours.  Deployers had to spend extra time attempting hotfixes.
* References disappeared from <code>&lt;references /></code> sections on an unknown number of content pages across the cluster.  This effect was limited to pages that displayed an error message before, or made clever use of failing parser functions to achieve certain effects.",{{ping|Brennen Bearnes}} noticed an increase in errors while monitoring <code>logspam-watch</code> during train deployment.,,"''What weaknesses did we learn about and how can we address them?''

The issue was a combination of several factors we never saw in any test or dev environment:
* Working with the Message class can cause it to request its own Parser, which is done with PHP's built-in <code>clone</code> command.
* This only happens if MessageCache hasn't been used yet in the requet.  Hence the majority of the failures came from the API, as this is much more likely there.
* Cloning an object in PHP will ""magically"" clone all of its properties, no matter if declared or dynamically created.
* The Cite extension uses a dynamic property <code>$parser->extCite</code> to store its instance, holding all state.
* A hook takes care that the Cite object is cloned as well when the Parser is cloned.
* Since {{gerrit|552546}}, the state was stored two levels deep, but cloning only performed a shallow copy.
* Cloning now resulted in two different Parsers with two different Cite instances that share the same state array.
* The fresh Message parser needs to clean its state, which cleans the state in all Cite instances.  Hence all previously seen <code>&lt;ref></code> got lost.
* The loop currently rendering a non-empty <code>&lt;references /></code> section starts to access keys in an empty array.
* This only happens when there is a reason to render a Message in the middle of this loop, e.g. because of a bad <code>&lt;ref></code>, or a template that renders an error.","'''NOTE''': Please add the [[phab:tag/wikimedia-incident/|#wikimedia-incident]] Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.
* {{phabT|240248}} – Original error report about the ""undefined index"" log spam. Resolved.
* {{phabT|240345}} – User reporting incomplete rendering of references. Resolved.
* {{gerrit|556209}} and {{gerrit|556329}} – Two very different integration tests are now able to prevent this from happening again.
* {{gerrit|556153}} – Extra time was spent on reworking the Cite codebase to not rely on cloning any more.
* {{phabT|240671}} – The QWERTY team will hold a retrospective meeting.

{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}",2019-12-10,Unknown,Unknown,Unknown,Unknown,Unknown,2019-12-10_Cite.wikitext
"On 2019-12-10, mainly between 22:26 and 22:39 UTC (but there were other smaller instances in the previous hour), Common wiki database replicas were lagging behind, causing slowdown, returning stale results, errors and, as a consequence of the original bug, category counts were updated incorrectly.","The user-noticeable impact was directly felt on Commons for a few minutes due to articles being removed from <code>Category:11</code>. However, the bug was present production on the following wikis during the given dates:

* testwiki: 2019-11-26 19:06 to 2019-12-10 21:16
* rest of group 0: 2019-11-26 19:46 to 2019-12-10 21:23
* group 1: 2019-12-04 20:12 to 2019-12-04 23:38, 2019-12-09 17:08 to 2019-12-11 23:18
* group 2: 2019-12-09 17:52 to 2019-12-12 20:18

While the other wikis did not suffer from obvious impact (lag), they may have suffered from incorrect category counts.","First notice was on IRC (see timeline).

There is an alert for lag, but it only triggers after 300 seconds for 10 checks (while slowdown happen after lag is higher than 1 second, and hard down is after 6 seconds). Top production lag was around 100 seconds. [JCrespo thinks (but is unsure) that the reason for this discrepancy is because SRE concerns are only about broken mysql replication and excessive (e.g. 1 hours) lag, and no one attending application problems currently receives these alerts].","[[File:Incident categorycounts.png|thumb|right|Database metrics during incident]]
* 22:12 - 22:14: Smaller spike of lag. Other less impacting occurences may have happened before then with no user-noticeable impact.
* 22:26: For the purposes of this document, outage will be considered starting here, when a more continuous lag spike starts. This is due to the following query running:
 UPDATE /* WikiPage::updateCategoryCounts */ `category` SET cat_pages = cat_pages - 1, cat_subcats = cat_subcats - 1 WHERE cat_title = 11

This query is incorrect, the 11 should be enclosed between quotes ('<nowiki/>'). This causes a slow query (not using and index) plus all categories starting with 11 to be updated, rather than just 1.

* 22:36: on IRC:

 2019-12-10 22:32:36 	<Amir1> 	I'm getting db locked for Special:UploadWizard on commons: https://commons.wikimedia.org/wiki/Special:UploadWizard
 2019-12-10 22:32:48 	<Amir1> 	non stop, for minutes now
 2019-12-10 22:33:09 	<Amir1> 	ok now
 2019-12-10 22:33:30 	<Reedy> 	https://commons.wikimedia.org/w/api.php?format=xml&action=query&meta=siteinfo&siprop=dbrepllag&sishowalldb=1
 2019-12-10 22:33:32 	<Reedy> 	There seems to be lag 
 2019-12-10 22:34:38 	<cdanis> 	there is a _lot_ of read traffic on s4 right now
 2019-12-10 22:35:01 	<wikibugs> 	(PS17) Jbond: puppet-merge: refactor [puppet] - https://gerrit.wikimedia.org/r/544214
 2019-12-10 22:35:02 	<cdanis> 	not sure where it is coming from, but, baseline is something like 200k rps, but it's getting 6-7M rps right now

* 22:39 lag ceases. cdanis, Apergos and jcrespo (maybe others) are responding to the issue. The long running query is detected through long running query monitoring and the above query is seen being executed, potentially many times. Long running query killer does not kill writes, as that can cause even a worse outage.

* Dec 11, 01:24: Ticket {{phabricator|T240405}} is created.
* Dec 11, 23:19: UTC wmf.10 with a fix is rolled into commons
* Dec 13, 18:31; The code point is tested and the query is correct",,"* [[phab:T240405|T240405]] Immediate issue to avoid recurrence of the same problem.
* [[phab:T221795|T221795]] To fix category counts
* [[phab:T108255|T108255]] Enable strict mode to prevent lose sql mode for MySQL
* TODO: Change some dangerous updates of a single row to be <code>LIMIT 1</code>?",2019-12-10,Unknown,Unknown,Unknown,Unknown,Unknown,2019-12-10_updateCategoryCounts.wikitext
"Between 2019-12-11 and 2019-12-17, the job queue was blocked by image annotation request jobs that were being enqueued by the MachineVision extension. These jobs were using the release timestamp feature of the [https://gerrit.wikimedia.org/r/plugins/gitiles/mediawiki/core/+/wmf/1.35.0-wmf.10/includes/jobqueue/IJobSpecification.php#49 job specification interface] that is not well supported by the [[Kafka Job Queue|Kafka job queue]]; release timestamps are implemented as blocking waits. These waits ended up blocking and causing a sizable backlog of a variety of jobs that are in the main pool of jobs not handled in job-specific Kafka topics. Jobs continued to be processed, but very slowly and with severe delays. No jobs appear to have been lost.

Phabricator task: https://phabricator.wikimedia.org/T240518","This delayed the execution of a wide variety of tasks that rely on the job queue, including but not limited to:
* Global renames
* Deleting translatable pages
* Echo notifications (https://phabricator.wikimedia.org/T240800)
* File uploads (https://phabricator.wikimedia.org/T240698)
* Recent changes processing
* MassMessages
* Pageview data publication (https://phabricator.wikimedia.org/T240803)",The issue was first reported by user [https://www.mediawiki.org/w/index.php?title=User:1997kB 1997kB] in https://phabricator.wikimedia.org/T240518. That issue specifically concerned delayed global renames. Several other delays in specific wiki functionality were reported over the next few days. The issue was not detected by any automated alerts.,"''This is a step by step outline of what happened to cause the incident and how it was remedied.  Include the lead-up to the incident, as well as any epilogue, and clearly indicate when the user-visible outage began and ended.''
<!-- Tips/Reminders:
- Graphs of the error rate or other surrogate, if available, are useful as well.
- Please ensure to remove private information.
- You can link to a specific offset in SAL using the SAL tool: https://tools.wmflabs.org/sal/
  For example: https://tools.wmflabs.org/sal/production?q=synchronized&d=2012-01-01
A trivial example timeline is given below.
-->

'''All times in UTC.'''

* 2019-12-11 20:39: '''OUTAGE BEGINS:''' The group restriction for MachineVision functionality is lifted, enabling it for all users. fetchGoogleCloudVisionAnnotations jobs are enqueued, with 48h delay implemented via release timestamp, for most bitmap images newly uploaded to Commons.
* 2019-12-12 00:05: User 1997kB reports delayed global rename execution in https://phabricator.wikimedia.org/T240518
* 2019-12-12 through 2019-12-15: Additional reports of functionality blocked by job queue delays
* 2019-12-16 06:24: Giuseppe identifies fetchGoogleCloudVisionAnnotations jobs failing with 500s
* 2019-12-16 16:12: Holger alerts Michael H. to a possible problem with fetchGoogleCloudVisionAnnotations jobs
* 2019-12-16 17:46: Michael H. disables new fetchGoogleCloudVisionAnnotation jobs from being enqueued
* 2019-12-16 19:17: Marko disables the job queue from consuming jobs from the fetchGoogleCloudVisionAnnotations topic
* 2019-12-16 19:31 & 19:54: Marko temporarily increases the job queue processing concurrency level to help process the backlog
* 2019-12-17 01:25: '''OUTAGE ENDS:''' All jobs have recovered to their baseline execution times
* 2019-12-18: Petr identifies the specific issue with Kafka job queue release timestamp support","''What weaknesses did we learn about and how can we address them?''

''The following sub-sections should have a couple brief bullet points each.''","''Explicit next steps to prevent this from happening again as much as possible, with Phabricator tasks linked for every step.''

'''NOTE''': Please add the [[phab:tag/wikimedia-incident/|#wikimedia-incident]] Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.
* Add alert(s) for unusual job processing backlog increases ([[phab:T242721]])
* Document the danger of the release timestamp feature in code and on-wiki ([[phab:T242722]])
* Kafka job queue should improve its handling of unknown new jobs ([[phab:T242726]])

{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}",2019-12-11,Unknown,Unknown,Unknown,Unknown,Unknown,2019-12-11_MachineVision%2Bcpjobqueue.wikitext
"In an attempt to deploy a change to block [https://phabricator.wikimedia.org/T241421 excessive scraping of the search API from a particular User-Agent running on AWS hosts], instead, by accident, all enwiki search API traffic was blocked.","In an interval of just under five hours, we wrongly returned HTTP 403 for 59.1k HTTP queries.  This is all global traffic against <code>Host: en.wikipedia.org</code> with a URI path of <code>/w/api.php</code> and a URI query that included the string <code>srsearch=</code>.  AIUI this affected both bots and also mobile app users, but not website users.

The particular AWS bot we were attempting to block was not active for these five hours.","Detection was a human report in #wikimedia-operations.  No automated detection.

(There is a larger architectural issue here about the 'proper' interface between Traffic and other services; see discussion below in actionables.)","'''All times in UTC.'''

* 2019-12-31 16:15 dcausse posts a diagnosis of elasticsearch elevated latency on [https://phabricator.wikimedia.org/T241421 T241421], and also mentions the excessive scraping in #mediawiki_security.
* 18:56: cdanis, after looking at query logs for a while, uploads the first patchset of [https://gerrit.wikimedia.org/r/c/operations/puppet/+/561300 I7970d3c9], then iterates several times on the criteria of the block, and its location within our VCL configs
* 19:42: cdanis self-+2s, merges, and deploys [https://gerrit.wikimedia.org/r/c/operations/puppet/+/561300 I7970d3c9], which in the process of writing it, was badly refactored to be far too broad '''OUTAGE BEGINS'''
* 20:12: After half an hour, Puppet should have run on all cp servers; outage at 100%.
* 2020-01-01 00:43: in #wikimedia-operations, dbrant reports Search API 403 errors.  Reedy and paladox notice and begin digging.
* 00:46: Reedy pings cdanis, who begins prepping a revert.
* 00:55: cdanis self-+2s and merges [https://gerrit.wikimedia.org/r/561321 I50a2cd79] to roll back the erroneous block, and begins a [[cumin]] run to invoke puppet on all text CPs
* 01:05: fix deployed to 100% of text CPs '''OUTAGE ENDS'''","=== What went well? ===
* Once recognized, outage was root-caused quickly.","There's two sections here: one for easy and obvious things, and another for larger ideas that require more discussion.

* Update [[Varnish#Blacklist_an_IP|docs]] to mention the recently-added <code>public_cloud_nets</code> IP list recently made available.  {{done}}
* Add some more documentation on how to write and run VTC tests.  Consider adding a test suite of common requests expected to return 200.",2019-12-31,Api,Unknown,Api,Unknown,Unknown,2019-12-31_search-api-traffic-block.wikitext
"A MediaWiki configuration change [https://gerrit.wikimedia.org/r/#/c/operations/mediawiki-config/+/562840/ making EventBus use TLS for eventgate-analytics] was merged. POST requests from MediaWiki application servers to eventgate-analytics started timing out. This resulted in HTTP server errors being served to users in all data centers. The very high rate of application server requests timing out caused troubles elsewhere in the stack, with 50% of the Varnish frontend caches in esams crashing due to memory allocation failures. Exacerbating the problems, although the Varnish frontend processes were automatically restarted, ATS-tls kept seeing them as down.","[[File:2020-01-08-appserver-errors.png|thumb]]
Between 15:06 and 15:32, ATS backend requests to the application servers resulted in multiple errors, between 3K and 7K 5xx per second. 
[https://grafana.wikimedia.org/d/000000479/frontend-traffic?orgId=1&fullscreen&panelId=14&from=1578491488740&to=1578498652787&var-site=All&var-cache_type=text&var-status_type=1&var-status_type=2&var-status_type=3&var-status_type=4 See Grafana].

[[File:2020-01-08-frontend-traffic.png|thumb]]
User facing impact has been two-fold: between 15:21 and 15:30 several requests received 502 error responses, with a peak of 2759 errors per second at 15:26. Between 15:06 and 15:45, many requests received no response at all. See [https://grafana.wikimedia.org/d/000000479/frontend-traffic?orgId=1&from=1578493626364&to=1578502050575&fullscreen&panelId=2 Grafana].

At the same time of this incident, one day before, the 2xx response rate was between 133k and 136k responses per second. During this incident it was ~100k rps between 15:11 and 15:21, and as low as 70k rps around 15:37.
  
All analytics events in the mediawiki.api-request and mediawiki.cirrussearch-request streams produced between 15:05 and 15:30 were lost.","The SRE team was first notified about the issue by various Icinga PHP7 rendering alerts on IRC, shortly followed by multiple pages regarding api.svc.eqiad.wmnet socket timeouts, as well as ATS TLS and Varnish reduced availability.","'''All times in UTC.'''
* 15:04: Scap sync started for wmf-config/ProductionServices.php: Make EventBus use TLS for eventgate-analytics - https://phabricator.wikimedia.org/T242224
* 15:06: varnish-fe crashes on both cp3050 and cp3054 (Cannot allocate memory)
* 15:08: First of many similar alerts on irc: PROBLEM - PHP7 rendering on mw1280 is CRITICAL: CRITICAL - Socket timeout after 10 seconds
* 15:09: varnish-fe crashes on cp3058 (Cannot allocate memory)
* 15:10: PROBLEM alert - api.svc.eqiad.wmnet/LVS HTTP IPv4 #page is CRITICAL
* 15:10: varnish-fe crashes on cp3062 (Cannot allocate memory)
* 15:10: <otto@deploy1001> Synchronized wmf-config/ProductionServices.php: Make EventBus use TLS for eventgate-analytics - https://phabricator.wikimedia.org/T242224 (duration: 06m 10s)
* 15:11: _joe_ points out on IRC that there are known issues with php-fpm+TLS, and asks ottomata to revert
* 15:11: ottomata tries reverting but the deployment fails
* 15:12: PROBLEM alert - icinga1001/ATS TLS has reduced HTTP availability #page is CRITICAL
* 15:17: PROBLEM alert - text-lb.esams.wikimedia.org_ipv6/LVS HTTPS IPv6 #page is CRITICAL
* 15:17: Cause of failure in reverting the change identified: php-fpm cache checks
* 15:19: RECOVERY alert - text-lb.esams.wikimedia.org_ipv6/LVS HTTPS IPv6 #page is OK
* 15:19: otto@deploy1001 sync-file aborted: REVERT Make EventBus use TLS for eventgate-analytics - T242224 (duration: 06m 33s)
* 15:19: ema notices that several varnish frontends in esams has crashed by looking at the icinga web ui (the events are reported as warnings)
* 15:20: varnish-fe crashes again on cp3058 (Cannot allocate memory)
* 15:20: ottomata tries syncing again
* 15:21: deployment still hanging on checking php-fpm cache
* 15:22: varnish-fe crashes again on cp3050 (Cannot allocate memory)
* 15:23: varnish-fe crashes again on cp3062 (Cannot allocate memory)
* 15:23: thcipriani mentions on irc that --no-php-restart needs to be passed to scap
* 15:24: ottomata tries running scap once again but gets an error: extra arguments found: --no-php-restart
* 15:25: _joe_ comments out temporarily the php-conditional restarts from scap, which were preventing the deployments from finishing
* 15:26: PROBLEM alert - icinga1001/Varnish has reduced HTTP availability #page is CRITICAL
* 15:26: Another deployment attempted, this time successfully: <otto@deploy1001> Synchronized wmf-config/ProductionServices.php: REVERT Make EventBus use TLS for eventgate-analytics - T242224 (duration: 00m 34s)	
* 15:26: RECOVERY alert - api.svc.eqiad.wmnet/LVS HTTP IPv4 #page is OK
* 15:29: RECOVERY alert - icinga1001/Varnish has reduced HTTP availability #page is OK
* 15:30: PROBLEM alert - text-lb.esams.wikimedia.org_ipv6/LVS HTTPS IPv6 #page is CRITICAL
* 15:30: RECOVERY alert - icinga1001/ATS TLS has reduced HTTP availability #page is OK
* 15:30: vgutierrez notices very high ats-tls CPU usage on various nodes
* 15:35: PROBLEM alert - text-lb.esams.wikimedia.org/LVS HTTPS IPv4 #page is CRITICAL
* 15:36: ats-tls restart on cp3050. Process up at 15:37:43
* 15:36: ats-tls restart on cp3056. Process up at 15:37:00
* 15:37: rolling restart of ATS backends in text esams to reclaim some memory by applying https://gerrit.wikimedia.org/r/#/c/operations/puppet/+/562849/
* 15:37: esams text traffic depooled in DNS
* 15:41: ats-tls restart on cp3052. Process up at 15:41:29
* 15:43: ats-tls restart on cp3054. Process up at 15:43:14
* 15:43: RECOVERY alert - text-lb.esams.wikimedia.org_ipv6/LVS HTTPS IPv6 #page is OK
* 15:43: RECOVERY alert - text-lb.esams.wikimedia.org/LVS HTTPS IPv4 #page is OK
* 15:45: ats-tls restart on cp3058. Process up at 15:46:45.
* 15:46: ats-tls restart on cp3062. Process up at 15:48:02.
* 16:00: esams text traffic re-pooled in DNS","=== What went well? ===
* Incident detection worked properly, the SRE team was quickly notified about the issue both on IRC and by SMS.
* Within 5 minutes from incident detection, a member of the SRE team identified the root cause and suggested a course of action.
* Communication was effective.","* {{Status|TODO}} Make scap skip restarting php-fpm when using --force https://phabricator.wikimedia.org/T243009
* {{Status|TODO}} Scap should be able to wait longer on the canaries https://phabricator.wikimedia.org/T217924
* {{Status|DONE}} varnish-fe should not crash trying to allocate memory https://phabricator.wikimedia.org/T242417
* {{Status|DONE}} varnish parent should be able to send signals to its child https://phabricator.wikimedia.org/T242411
* {{Status|DONE}} Raise severity of varnish child restart to critical: https://gerrit.wikimedia.org/r/#/c/operations/puppet/+/563174/
* {{Status|TODO}} ats-tls should immediately mark varnish as up after the latter restarts https://phabricator.wikimedia.org/T242620
* {{Status|TODO}} Per-cgroup CPU graphs would allow to quickly find out misbehaving processes: https://phabricator.wikimedia.org/T183146

{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}",2020-01-08,Api,Unknown,Mediawiki,Unknown,Unknown,2020-01-08_mw-api.wikitext
"'''Note: the corresponding maps outage is not treated here. Its timings will just be reported in the timeline for reference.'''

At 19:47 UTC, the news of Kobe Bryant's death was announced. This caused a surge in both edits and page views, causing a stampede of requests and a general slowdown of both the application servers and the apis. This caused both high contention in editing/parsing as expected, and can be seen by the number of poolcounter full queues, as well as high cpu usage, very high response times, and general unavailability of the MediaWiki application layer (both appservers and api). This was caused by a series of concauses:
* High contention in editing/parsing, resulting in a lot of workers blocked on waiting for poolcounter.
* Saturation of the network link of two memcached servers due to the high edit/reparse activity. This caused about 1/9th of the memcached keys to become unavailable
* A spike in latencies resulting from the failure of the memcached servers caused a rise in the cache-misses for babel calls from the appservers, flooding the api servers. pretty much as the same mechanism of [[Incident_documentation/20200204-app_server_latency|the incident of February 4]] (although the ignition of the incident was different).

Given the sustained higher level of requests, it took more than one hour for the clusters to recover.
[[File:Poolcounter full queues.png|thumb|Number of full queues on poolcounter during the incident.]]","1 hour of almost complete unavailability of the backend servers. During that period, a large number of requests to our sites will have timed out or got an error response.
As can be seen from the graph of all requests, the average request rate was just barely under what the normal rate is, but we can estimate there was a large number of requests that we weren't able to answer to. Almost 90% of the backend requests from ATS failed.  [[File:Overall_site_visits_kobe.png|thumb|Successful responses from all of our caching frontends during the incident.]]","The monitoring correctly and promptly detected the issue, and 3 minutes later we received the first page. Given most SRE were already around for a previous incident, most of us didn't need the page to be online.
<pre>
2020-01-26 19:52:33     +icinga-wm      PROBLEM - MediaWiki memcached error rate on icinga1001
2020-01-26 19:53:57     +icinga-wm      PROBLEM - High average GET latency for mw requests
2020-01-26 19:55:11     +icinga-wm      PROBLEM - LVS HTTPS IPv4 #page on text-lb.codfw.wikimedia.org
</pre>","'''All times in UTC.'''

* 19:50 Mcrouters on almost every host report failures connecting to mc1019 (mostly) and other servers. '''OUTAGE BEGINS'''
* 19.51 Latency spikes up on the appserver cluster. Cache hit ratio for Babel requests is reduced to about 25% of normal
* 19.52 Investigation begins
* 19.53 Latency spikes up on the api cluster too.
* 20.01 Cause is tentatively identified as a side effect of the news spreading and an editing stampede happening
* 20.08 The largest part of the memcached errors are recovered, but the latencies are still very high (p75 for the appservers at precisely 10 seconds)
* 20:13 Babel calls are identified as a possible cause. 
* 20:16 Developers are called in order to try to understand what is going on. Given it's sunday and most people are flying to All-Hands, this takes some time.
* 20:30 Attention also moves to other pages doing cross-cluster requests like <tt>ImagePage::printSharedImageText</tt>, and to the fact we're reaching the api for babel via the caching layer
* 20:35 Various mitigations are proposed - including setting <tt>fetchDescription => false</tt> in wmf-config/filebackend.php
* 20:35 Kartotherian reaches 100% cpu utilization, starts intermittently paging
* 20:41 It is noticed that some varnish-fes are constantly crashing
* 20:43 An anomalous amount of 302s are noticed being returned from the application servers. Upon investigation, those are shown to be mostly the Apple search interface searching for Kobe Bryant.
* 21:01 It gets decided to rollout a very aggressive timeout in MediaWiki core's HttpRequest request abstraction.
* 21:04 Most services recover before the fix is deployed. '''OUTAGE ENDS'''
* up to 24:00 - cpu on the maps cluster is still constantly at 100%.","=== What went well? ===
* Monitoring detected the outage quickly","* Reduce Wikibase calls to Babel when not needed https://phabricator.wikimedia.org/T243725 {{done}}
* Reconsider https://gerrit.wikimedia.org/r/#/c/operations/puppet/+/511751/ (apache forensic logging) 
* Proxy calls from MediaWiki to all https calls through a proxy https://phabricator.wikimedia.org/T244843
* Reduce read pressure on memcached servers by adding a machine-local Memcache instance https://phabricator.wikimedia.org/T244340
* Investigate why a slowdown in response times from the API causes a surge in cache misses for Babel data in Wikibase. See https://phabricator.wikimedia.org/T244877


{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}",2020-01-26,Server,Unknown,Caching,Unknown,Unknown,2020-01-26_app_server_latency.wikitext
"External demand for an expensive MW API query, caused the MW API web servers to become overall slower to respond to other queries as well. Some of these queries became sufficiently slow as to trigger our execution timeout of 60 seconds. The [[Services/Monitoring/recommendation api|Recommendation API service]] was partially unavailable for about '''35-40 minutes''' as it uses the MW API for part of its work.",,,"* (Grafana) RESTBase backend errors: <https://grafana.wikimedia.org/d/000000068/restbase?orgId=1&from=1580159395388&to=1580164542571>
* (Logstash) MediaWIki errors & timeouts: <https://logstash.wikimedia.org/goto/613fac39acd1638eca41fd1649afe1da>

Internal Google Doc: <https://docs.google.com/document/d/1H1HOA49S0pATzXlD1yQLQiSR12qsFkcagtLWg5Slz34/edit#>{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}",,,2020-01-27,Unknown,Unknown,Unknown,Unknown,Unknown,2020-01-27_app_server_latency.wikitext
"[https://www.mediawiki.org/wiki/Extension:Babel '''Babel'''] is a MediaWiki extension that displays infoboxes on user pages, expressing the user's proficiency in speaking one or more languages. When looking up language data for a user, it first consults the '''WANObjectCache''', backed by '''memcached'''. If that user's data isn't in the cache, the Babel extension makes an API call to itself (that is, from Babel on an appserver to Babel on an API server), which fetches the language data from the database and caches it for next time. The cache hit ratio is normally about 90%.

From 16:03 to 16:12 UTC on 2020-02-04, the WANObjectCache hit ratio for Babel keys dropped, bottoming at about 52%. [https://grafana.wikimedia.org/d/lqE4lcGWz/wanobjectcache-key-group?orgId=1&var-kClass=babel&from=1580826600000&to=1580842800000&fullscreen&panelId=12 (graph)] The total rate of cache lookup attempts -- that is, hits plus misses -- remained roughly constant at about 10,000 per minute [https://grafana.wikimedia.org/explore?orgId=1&left=%5B%221580826600000%22,%221580842800000%22,%22graphite%22,%7B%22datasource%22:%22graphite%22,%22target%22:%22sum(aliasByNode(scale(MediaWiki.wanobjectcache.babel.*.*.rate,%2060),%203,%204))%22,%22textEditor%22:true%7D,%7B%22mode%22:%22Metrics%22%7D,%7B%22ui%22:%5Btrue,true,false,%22none%22%5D%7D%5D (graph)], suggesting that either the Babel keys were suddenly evicted from the cache and had to be repopulated, or Babel traffic suddenly shifted to a new set of keys that was not yet cached. (TODO: Determine which, and clarify.)

Because of the increased cache misses, the rate of Babel API requests also increased. These requests were sent over HTTPS [https://gerrit.wikimedia.org/r/plugins/gitiles/operations/mediawiki-config/+/ca37195a5955db0605a62246aa74119b82e715d0/wmf-config/CommonSettings.php#2695 (CommonSettings.php)] so the extra computation due to TLS caused high CPU load on the appservers [https://grafana.wikimedia.org/d/000000607/cluster-overview?orgId=1&from=1580826600000&to=1580842800000&var-datasource=eqiad%20prometheus%2Fops&var-cluster=appserver&var-instance=All&fullscreen&panelId=2607 (graph)]. Each request hung for ten seconds waiting for a response before timing out [https://grafana.wikimedia.org/d/lqE4lcGWz/wanobjectcache-key-group?orgId=1&from=1580826600000&to=1580842800000&var-kClass=babel&fullscreen&panelId=13 (graph)], which tied up appserver resources and delayed other traffic. Appserver errors and latency recovered immediately when the Babel cache hit ratio returned to normal, but a [[Maps#Kartotherian|Kartotherian]] outage, which may have been triggered by the same spike in memcached misses, continued for some time.

It seems that the origin of the incident is a further slowdown of the appservers in response to a surge in traffic that happened around 16:01 [https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?orgId=1&from=1580826600000&to=1580842800000&fullscreen&panelId=17 (graph)]. While the surge seems relatively small, it corresponds with the initial slowdown of application servers.
TODO: Supplement graph links with inline static images.","About 12 million varnish-text requests were dropped or delayed in a ten-minute period, spanning all five caching data centers; that represents about 20% of traffic in that window.

TODO: Fill in the methodology behind that number, which is based on eyeballing the missing area under global text cache request rate [https://grafana.wikimedia.org/explore?orgId=1&left=%5B%221580826600000%22,%221580842800000%22,%22eqiad%20prometheus%2Fglobal%22,%7B%22expr%22:%22sum(job_method_status:varnish_requests:rate5m%7Bsite%3D~%5C%22codfw%7Ceqiad%7Ceqsin%7Cesams%7Culsfo%5C%22,%20job%3D~%5C%22varnish-(text)%5C%22,status%3D~%5C%22(1%7C2%7C3%7C4).*%5C%22,method!%3D%5C%22PURGE%5C%22%7D)%22,%22format%22:%22time_series%22,%22intervalFactor%22:2,%22legendFormat%22:%22varnish%2F%7B%7Bsite%7D%7D%22,%22target%22:%22%22,%22context%22:%22explore%22%7D,%7B%22mode%22:%22Metrics%22%7D,%7B%22ui%22:%5Btrue,true,true,%22none%22%5D%7D%5D (graph)].","The first Icinga CRITICAL alert in #wikimedia-operations was at 16:03 (for ""phpfpm_up reduced availability""). The first page was at 16:05 (""LVS HTTPS IPv6 #page on text-lb-ulsfo-wikimeda.org_ipv6 is CRITICAL: CRITICAL - Socket timeout after 10 seconds""). In all, we received 239 IRC-only alerts and 7 pages. (These numbers reflect only the MediaWiki latency issue, not the Kartotherian issue.)

All seven paging alerts were of the form ""LVS HTTPS IPv{4,6} #page on text-lb.{DATACENTER}.wikimedia.org_ipv{4,6} is CRITICAL: CRITICAL - Socket timeout after 10 seconds"" across all five caching data centers.","'''All times in UTC, 2020-02-04.'''

* 15:38 General appserver slowdown begins. 75th percentile latency increases from the usual ~300 ms to 1500-4000 ms. During this slowdown interval, users perceive slower load times but the site is fully available; error rates are not elevated. [https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?orgId=1&from=1580826600000&to=1580842800000&fullscreen&panelId=11 (graph)]. This is NOT considered as part of the main outage, but rather a degradation related to a spike in memcached requests.
* 16:01 The outage begins on the appservers, with a spike of slow requests, in response to what looks like a slight surge in external requests.
* 16:03 Babel cache misses begin to spike (timestamp based on Babel requests in API server logs). Appserver errors and latency immediately spike. '''OUTAGE BEGINS'''
* 16:03:31 First Icinga CRITICAL alert in #wikimedia-operations. '''OUTAGE DETECTED'''
* 16:05:12 First Icinga #page.
* 16:12 Babel cache misses return to normal (timestamp based on Babel requests in API server logs). Appserver errors immediately recover; latency returns to slowdown levels. '''OUTAGE ENDS'''
* 16:19 First alert related to Kartotherian. Kartotherian issues continued for some time after this outage, and won't be discussed here &ndash; see [[Incident documentation/20200204-maps|20200204-maps]].
* 16:52 ladsgroup deploys [https://gerrit.wikimedia.org/r/c/570084 #570084], reducing the overall rate of Babel calls. [https://tools.wmflabs.org/sal/log/AXARHfjffYQT6VcDBMQr (SAL)]
* 17:10 ladsgroup deploys [https://gerrit.wikimedia.org/r/c/570089 #570089], lowering the timeout from 10 seconds to 2 seconds. [https://tools.wmflabs.org/sal/log/AXARLqzMfYQT6VcDBPjo (SAL)]
* 17:34 Joe increases weight on mw12[3-5].* to 15 [https://tools.wmflabs.org/sal/log/AXARREbbvrJzePItncMm (SAL)]
* 17:42 General appserver slowdown ends. 75th percentile latency returns to normal.","''What weaknesses did we learn about and how can we address them?''

''The following sub-sections should have a couple brief bullet points each.''","TODO: This is an incomplete list of actionables, not all actionables have tasks, and not all tasks are tagged ""incident.""

* Eliminate all Mediawiki appserver self-calls over HTTPS. Short-term, move them to HTTP; longer-term, to Envoy or similar. https://phabricator.wikimedia.org/T244843
* Make timeouts on http calls short enough not to cause a cascading outage when one cluster is slow. Done for Babel.
* Implement some automated way of sharing incident docs with WMDE [[phab:T244395]]
* MediaWiki's HTTP call abstraction should pass through the X-Wikimedia-Debug header if it was set in the original request
* Fix a bad interaction where Wikidata/CachingPropertyInfoLookups don't actually cache data in WANObjectCache, leading to many repeated calls for the same memcached key on the same server. [[phab:T243955]] Done.
* Reduce read pressure on memcached servers by adding a machine-local Memcache instance [[phab:T244340]]
* Investigate and propose options on getting structured data from API and application server logs, to improve observability of exactly what's expensive on appservers. [[phab:T235773]]
* Investigate why a slowdown in response times from the API causes a surge in cache misses for Babel data in Wikibase. See https://phabricator.wikimedia.org/T244877",2020-02-04,Cache,Unknown,Caching,Unknown,Unknown,2020-02-04_app_server_latency.wikitext
"Maps servers fully saturated on CPU, resulting in an increase in user-experienced latency and request error rate.  In order to shed load and restore service for users of Wikimedia projects, traffic from non-Wikimedia sites was blocked (and as of 2020-03-04, is still blocked).

The proximate cause is not fully known, but a large part of it is likely fallout of [[Incident documentation/20200204-app server latency|a Mediawiki API outage]], as Maps servers need to fetch data from the Mediawiki API for some kinds of requests, and there have been previous Mediawiki incidents where similar Maps impact was seen.

The deeper causes involve a long-running lack of staffing on the software and its infrastructure, one of the manifestations of which is a lack of familiarity with the software and the infrastructure by the SRE team and most others involved in day-to-day operations.","All Maps users, including those of Wikimedia wikis and projects, were experiencing elevated latency and error rates from 16:03 until 21:20.

All Maps users, including those of Wikimedia wikis and projects, were unable to load tiles that were not already cached by our CDN, for approx 20 minutes from 21:00 to 21:20.

""External"" Maps users (those on non-Wikimedia wikis and projects &ndash; everything from travel agencies to Pokémon GO fan sites) are unable to load tiles that were not cached by our CDN starting at 21:00 and ongoing as of this writing.

About 1.44M errors were served to users for the duration of the outage.  To mitigate the outage, we disabled external traffic that accounts for about 36% of requests to Maps.","Automated detection via basic LVS/PyBal alerts.

However, note that users were already experiencing elevated latency and errors well before the extant alerts fired.","'''All times in UTC.'''
{{see also|Incident documentation/20200204-app server latency}}

* 15:38: Mediawiki appservers begin to slow down.
* 16:03: Large spike of appserver errors and latency. Maps begins serving a low rate of HTTP 503 and 504 errors, starting around 4rps and increasing to 50rps. '''OUTAGE BEGINS'''
** Kartotherian latency almost certainly increases as well, but there's no trusted monitoring on this.
* 16:09: Maps CPU usage in eqiad, already very hot under normal load at 70%+, skyrockets to 100%. [https://grafana.wikimedia.org/d/000000607/cluster-overview?orgId=1&var-datasource=eqiad%20prometheus%2Fops&var-cluster=maps&var-instance=All&from=1580831102595&to=1580835214485]
* 16:12: Appservers return to normal latency and error rate.
* 16:19: First alert related to Kartotherian/Maps: PROBLEM - PyBal backends health check on lvs1016 is CRITICAL: PYBAL CRITICAL - CRITICAL - kartotherian-ssl_443: Servers maps1004.eqiad.wmnet are marked down but pooled https://wikitech.wikimedia.org/wiki/PyBal
** This alert fires, then recovers, for several of the Maps servers.  This alert indicates that Maps servers are beginning to become so overloaded that they cannot reply to PyBal's healthchecking queries in a timely manner.
* 16:26: First page related to Kartotherian/Maps: PROBLEM - Kartotherian LVS eqiad #page on kartotherian.svc.eqiad.wmnet is CRITICAL: /v4/marker/pin-m-fuel+ffffff@2x.png (scaled pushpin marker with an icon) timed out before a response was received: /v4/marker/pin-m-fuel+ffffff.png (Untitled test) timed out before a response was received https://wikitech.wikimedia.org/wiki/Maps%23Kartotherian
** This alert indicates that zero Maps@eqiad servers are healthy enough to respond to PyBal's healthchecking queries in a timely manner.
* 16:45: akosiaris stops all kartotherian processes on maps servers, seeing an increase in requests to Maps prior to the start of [[Incident documentation/20200204-app server latency|20200204-app server latency]] &ndash; which leads to a belief that perhaps traffic from Maps to Mediawiki is the cause of the API server outage.
* 16:45: In addition to the lower rate of HTTP 503 and 504 errors, users begin experiencing many HTTP 502 errors.  Maps is serving approx 300rps of errors, or about 17% of Maps traffic. [https://turnilo.wikimedia.org/#webrequest_sampled_128/4/N4IgbglgzgrghgGwgLzgFwgewHYgFwhLYCmAtAMYAWcATmiADQgYC2xyOx+IAomuQHoAqgBUAwoxAAzCAjTEaUfAG1QaAJ4AHLgVZcmNYlO4B9E3sl6ASnGwBzYkryqQUNLXoEATAAZfpPwCAFhEARgBWPHCfPB8fADo4nwAtSWJsABNuX39AnxDQgE5YyLjEuNSAXwBdSoY1LR1XNBoIe0lDYwIYVpNKTDdJOHIMHG42yTBEGEcVEBY4TSh4gHcIAGsINgyIOHjMGjsQaqZsTE8pRChiGqYoTSQ0JxcNbW43VvamHbZsKCxcARKGg0JoTG50DAlHcDp5QJ1uJQIE9LI13goILNvhBDCMAdwMo5yOkdl8QNpWpgsgQQHVCFtkfhsDAEAg6g03rotvoQD90v8xgQzBYmHYaLYWbRkepuAAFMIAWUkUFh+HhRlM5m5qM5vO5f3x2Nxo0BIDgUGJmTaRzpSBYjLwzNZt1cGNmznVUgU6WJiOR0OkBwWcOYaIIhMuLPoTCmCBm3FpTFeTTY5p6XHZIE63uwvoIcDsYuIdnQxAyJkM9xw13BKB5UiD6DVod1EbgUcm0yaiZbKeIacMtJO5LaJAyABF9QK/iph5pR2WAMqqoH+yQIYsk5t8g2CkDA0Hg9xoKHrhmeJ0IGNdpygHzZOKSULcADM+UkXm4QS8hUkL6/j5MEE3DRJ+TDhCBPj/kwABskHAXS1BQAAciyCD4JcCDXIhEB2JQSB4ReaGVEAA===]
* [https://tools.wmflabs.org/sal/log/AXARSyl4fYQT6VcDBQ9t 17:41]: akosiaris restarts the Karotherian service on maps100*.  CPU consumption quickly returns to 100%.
* 17:41: HTTP 502 errors end.
* [https://tools.wmflabs.org/sal/log/AXARhYNI0fjmsHBaHrqS 18:45]: cdanis, believing the problem maybe has something to do with eqiad specifically, and not realizing that Maps is in a global capacity crunch, depools Maps@eqiad, forcing all load over to codfw -- which then also begins suffering from 100% cpu consumption.
* 19:43: cdanis blocks Maps requests with a Referer headers matching twpkinfo.com, a Pokémon GO fan site, which was approx 25% of maps load at the time https://gerrit.wikimedia.org/r/c/operations/puppet/+/570129
* [https://tools.wmflabs.org/sal/log/AXAR5tzBfYQT6VcDBYnF 20:31] shdubsh restarts Kartotherian on maps2001 after enabling extra logging for analysis
* 21:00: akosiaris modifies our CDN configuration to block all ''cache misses'' that are from non-WMF Referers: [https://gerrit.wikimedia.org/r/c/operations/puppet/+/570140 570140]
** As it is only CDN cache misses that are forwarded to the Maps servers and cause extra load, this allows ""free"" tiles (from a Maps CPU perspective) to continue loading on external sites, minimizing the impact while preserving the service
** However, this change is actually subtly erroneous, and despite multiple reviewers, we block all Maps cache misses, WMF site or not.
* 21:20: cdanis deploys [https://gerrit.wikimedia.org/r/c/operations/puppet/+/570143 570143] which allows akosiaris's change to work correctly.   '''OUTAGE ENDS'''
** Maps CPU usage in codfw returns to about 80% instead of 100% or 0%. [https://grafana.wikimedia.org/d/000000607/cluster-overview?orgId=1&var-datasource=codfw%20prometheus%2Fops&var-cluster=maps&var-instance=All&from=1580846774271&to=1580858098991]
* 21:49: cdanis deploys [https://gerrit.wikimedia.org/r/c/operations/puppet/+/570147 570147] which allows wmflabs.org sub-domains (forgotten in the original change) to continue to use Maps, as well as fixing some other edge cases.
* [https://tools.wmflabs.org/sal/log/AXASOy83fYQT6VcDBedg 22:03] cdanis repools Maps@eqiad.
** CPU usage in codfw drops to about 17% [https://grafana.wikimedia.org/d/000000607/cluster-overview?orgId=1&var-datasource=codfw%20prometheus%2Fops&var-cluster=maps&var-instance=All&from=1580846774271&to=1580858098991] and CPU usage in eqiad rises to about 40% [https://grafana.wikimedia.org/d/000000607/cluster-overview?orgId=1&var-datasource=eqiad%20prometheus%2Fops&var-cluster=maps&var-instance=All&from=1580846774271&to=1580858098991]","''What weaknesses did we learn about and how can we address them?''

''The following sub-sections should have a couple brief bullet points each.''","''Explicit next steps to prevent this from happening again as much as possible, with Phabricator tasks linked for every step.''

'''NOTE''': Please add the [[phab:tag/wikimedia-incident/|#wikimedia-incident]] Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.
* Rebalance Maps traffic between eqiad and codfw: eqiad is often much hotter than codfw, as it sees all the European load.  (TODO: Create task)
** Making more use of codfw (and ulsfo) in general is desired by the Traffic team; can potentially use Maps as a proving ground for a new geographical DNS mapping.
* Make a policy decision about whether or not to continue disallowing external sites to embed our maps.  If we decide to do so:
** Probably, begin disallowing cache hits as well as cache misses; it's confusing.
** Announce this publicly; remove Wikimedia from [[osmwiki:Tile servers|OSM's list of map tile providers]]; etc.
* Perform some capacity planning for Maps. [[phab:T228497]]
** This was an actionable in a few previous Maps outages, but probably didn't happen?
*** [[Incident documentation/20190308-maps]]
*** [[Incident documentation/20190715-maps]]
*** [[Incident documentation/20190913-maps]]
* Investigate why Maps overloads when appservers have high latency (or are returning errors)
** One of the Kartotherian log messages that was prominent in the incident was <code>Bad geojson - unknown type object</code> which seems to correlate with appserver trouble.
* Attempt to get some more staffing behind Maps?

{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}

[[Category:Maps outages, 2020]]",2020-02-04,Cache,Error rates,Cdn,Unknown,Unknown,2020-02-04_maps.wikitext
"''Widespread service timeouts starting immediately after moving group2 wikis to 1.35.0-wmf.18. Reverting to 1.35.0-wmf.16 immediately brought most services back to life.''

A single root cause was responsible for two incidents, this one on Thursday the 6th and again on Friday the 7th. See [[Incident_documentation/20200207-wikidata]] for more relevant details on this incident.","''All sites were unresponsive or at least partially offline for ~8 minutes.''  (Except for cache hits, which would be 'popular' pages for non-logged-in users.)",~100% of icinga alerts fired simultaneously.,"<!-- Tips/Reminders:
- Graphs of the error rate or other surrogate, if available, are useful as well.
- Please ensure to remove private information.
- You can link to a specific offset in SAL using the SAL tool: https://tools.wmflabs.org/sal/
  For example: https://tools.wmflabs.org/sal/production?q=synchronized&d=2012-01-01
A trivial example timeline is given below.
-->

'''All times in UTC.'''
* 20:22 or so: start of deploy of wmf.18 to group 2 wikis
* 20:24 Icinga starts alerting for High average GET latency for mw requests on appservers in eqiad and various other things
* 20:25 scap completes: <twentyafterfour@deploy1001> rebuilt and synchronized wikiversions files: all wikis to 1.35.0-wmf.18 refs T233866
* 20:25 Icinga starts alerting for restbase endpoint health on all restbase servers
* 20:28 Icinga starts alerting for Varnish HTTP text-frontend health on various cp servers and Apache HTTP on mw servers
* 20:29 Grafana is not available to European users (and SREs)
* 20:30 (twentyafterfour@deploy1001) Scap failed!: 9/11 canaries failed their endpoint checks(http://en.wikipedia.org)
* 20:30 --force is used to revert the Mediawiki deploy: <twentyafterfour> sync-wikiversions --force
* 20:31 Icinga recoveries start coming in for Varnish HTTP text-frontend, PHP7 rendering on appservers, etc, but Restbase alerts remain
* 20:45 Restbase is only reporting errors from wikifeeds (in k8s), restbase is restarted on restbase1016 and restbase1027 but that did not help recoveries
* 20:47 akosiaris kill all pods for wikifeeds in eqiad. They were in a throttled CPU downward spiral. https://grafana.wikimedia.org/d/35vIuGpZk/wikifeeds?orgId=1&var-dc=eqiad%20prometheus%2Fk8s&var-service=wikifeeds&fullscreen&panelId=28&from=1581020337234&to=1581022780963
* 20:52 Restbase alerts recovered","The root cause was a typo in a config setting for Wikibase.

On Jan 16 [https://wikitech.wikimedia.org/w/index.php?title=Deployments&diff=1850912&oldid=1850867] this config change {{gerrit|565074}} was deployed. This would have caused all Wikibase client wikis to read items from the new wb terms store. It did not take effect because of a typo in the name elsewhere in the config files. This means that the default setting in the Wikibase extension was used, with all clients reading from the old store.

On Jan 22 the default setting in the Wikibase extension was changed to have clients all read from the new store. This did not make it into a branch until wmf.18, because of All-Hands. It went live to groups 0 on Feb 4th, and group 1 on Feb 5th. This would have impacted Commons and Wikidata but they do very few Wikibase client reads compared to the bulk of the wikis.
The change went live to group 2 on Feb 6th, when we saw the outage.

A similar but smaller scale incident occurred the next day when trying to fix that config variable typo.
Friday's incident report: [[Incident documentation/20200207-wikidata]].

The similarity of the two incidents (onset, type of impact, graphs [https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?orgId=1&from=1581020400000&to=1581021600000], [https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?orgId=1&from=1581084790469&to=1581088067987]) led us to believe that the root cause of Friday's incident is also the root cause of Thursday's incident. The root cause of Friday's incident was narrowed down to the specific config change because only that single change was deployed at the time of the incident on Friday, which is unlike Thursday when we were routinely deploying a larger batch of changes during the MediaWiki train.

The Monday rollout of wmf.18 to group2 without incident confirms this understanding.","* [https://phabricator.wikimedia.org/T244535 T244535 - wikifeeds: Fix the CPU limits so that it doesn't get starved]
* When text-esams was down, Grafana was not available to European SREs. Workarounds below were mentioned:
** echo $(dig +short text-lb.eqsin.wikimedia.org) grafana.wikimedia.org | sudo tee -a /etc/hosts 
** ssh grafana1002.eqiad.wmnet -L3000:localhost:3000
* conversations about moving monitoring interfaces outside the normal traffic path (Herron). continue them and turn into a ticket
* [https://phabricator.wikimedia.org/T243009 T243009 - Make scap skip restarting php-fpm when using --force]
* [https://phabricator.wikimedia.org/T217924 T217924 - Make canary wait time configurable]
* [https://phabricator.wikimedia.org/T244544 T244544 - add a force-revert command to scap to shorten the time it takes to revert]
* [https://phabricator.wikimedia.org/T244533 T244533 - Slow query hitting commonswiki]
* [https://phabricator.wikimedia.org/T183999 T183999 - scap canary has a shifting baseline] (scap, why did canaries not catch the bad deploy (tangentially related))
* Consider moving to more of a continuous deployment model with only groups of related changes being deployed together ([[User:20after4]] is thinking about this)
* As an underling problem. Typos are really easy to happen on mediawiki-config.
**[https://phabricator.wikimedia.org/T223602 T183999 - Define variant Wikimedia production config in compiled, static files]
**[https://phabricator.wikimedia.org/T220775 T220775 - Consider creating a puppet-compiler equivalent for mediawiki-config.git]
{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}",2020-02-06,Cache,Unknown,Unknown,Unresponsive,Unknown,2020-02-06_mediawiki.wikitext
"A bot scraping zhwiki, which we have been monitoring for a while now, started making more expensive requests more aggressively. The bot was concealing itself by using a common User-Agent: <code>Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36</code>

Most requests were similar to:

 http://zh.wikipedia.org/w/api.php?action=parse&pageid=2996886&prop=text&wrapoutputclass=wiki-article&disableeditsection=true&mobileformat=true&mainpage=true&format=json
The <code>wrapoutclass</code> url parameter causes a request to bypass ''[[Parser_cache|parsercache]]''. To make matters worse, the scraper was going through the whole list of French localities on zhwiki, each of which made ample use of some known slow templates, originally seen on occitan wikipedia (euwiki), with the 36k entry table of localities. Each of those requests  required 15-60 seconds to parse.

Lastly, while we were investigating, an unscheduled deployment was pushed to production, to fix an [https://phabricator.wikimedia.org/T244529 UNB!] task. The deployment caused s8 to recive an influx of queries, so it was quickly reverted [[Incident_documentation/20200207-wikidata]].",API became almost unresponsive for about 10 minutes and. Application servers were unresponsive for another 10 minutes a little bit after.,"14:06:40 <+icinga-wm> PROBLEM - MediaWiki exceptions and fatals per minute on icinga1001
 14:06:41 <+icinga-wm> PROBLEM - High average POST latency for mw requests on api_appserver in eqiad on icinga1001 is CRITICAL:
 
 14:09:42 <+icinga-wm> PROBLEM - High average GET latency for mw requests on api_appserver in eqiad on icinga1001 is CRITICAL: 
 14:09:52 <+icinga-wm> PROBLEM - Apache HTTP on mw1290 is CRITICAL: CRITICAL
 
 14:17:07 <+icinga-wm> RECOVERY - Nginx local proxy to apache on mw1283 is OK: HTTP OK:","'''All times in UTC.'''

* 14:06 '''OUTAGE #1 BEGINS'''
We start parsing API logs, where we establish that it the zhwiki bot we have been monitoring, is making very expensive requests. The requests were both bypassing parsercache and included some infamous templates.  It is using a very common UA, one that is used by real users as well, so blocking would be not be easy.  

* 14:17 '''OUTAGE #1 ENDS'''
* 14:28  Amir contacts the community [https://zh.wikipedia.org/wiki/Wikipedia:互助客栈/技术/存档/2020年3月#A_technical_issue_with_articles_of_French_communes A_technical_issue_with_articles_of_French_communes]
* 14:46 Amit Emptified the templates [https://zh.wikipedia.org/wiki/Special:用户贡献/Amir_Sarabadani_(WMDE) https://zh.wikipedia.org/wiki/Special:用户贡献/Amir_Sarabadani_(WMDE)]

<br>
[[File:Requests.png|thumb|center]] [[File:Latency1.png|thumb|center]]

<br>",Templates issues are hard to debug.,"* Emptify the French Commune Data templates and contact the community (already done)

{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}",2020-02-07,Api,Unknown,Api,Unresponsive,Unknown,2020-02-07_mediawiki_API_down.wikitext
"Before the incident:

The configuration setting for Wikibase clients for which ranges of items to read from the old wb term store and which ones from the new, was not passed through from InitialiseSettings.php because of a typo. Instead, the default value in the Wikibase extension was used, which was to read all items from the old store for wmf.16, deployed to the group2 wikis, and to read only from the new store for wmf.18, deployed to the group 0 and 1 wikis. The group 0 and 1 wikis were potentially reading from unmigrated rows, causing pages with missing data to be rendered and cached for Commons and Wikidatawiki; see {{phab|T244529}} for the first report of the issue with group 0 and 1 wikis, and {{phab|T244697}} for a very detailed chart of which settings were in place when.

This was deemed UBN, and a deploy made to fix the configuration setting typo ({{gerrit|570892}}). This resulted in group 2 wikis reading from the new store for items with QID up to 8,000,000. This caused an inordinate load on servers ultimately resulting in an outage.

It is now clear that this is the same root cause as the previous day's outage; wmf.18 was deployed on Monday Feb 10 without the config change and there were no problems. See [[Incident documentation/20200206-mediawiki]] for details on the previous day's outage.

Most of the documentation of this issue is in the comments on {{phab|T244529}} and the follow-up task {{phab|T244697}}","Wikis were unavailable for users via eqiad for the period of the outage, about 8 minutes. Cached pages accessed via other sites should have been available.","Icinga alerted immediately, starting with MediaWiki exceptions and fatals per minute. The icinga-wm bot flooded out of the IRC channel due to too many reports.","''This is a step by step outline of what happened to cause the incident and how it was remedied.  Include the lead-up to the incident, as well as any epilogue, and clearly indicate when the user-visible outage began and ended.''
<!-- Tips/Reminders:
- Graphs of the error rate or other surrogate, if available, are useful as well.
- Please ensure to remove private information.
- You can link to a specific offset in SAL using the SAL tool: https://tools.wmflabs.org/sal/
  For example: https://tools.wmflabs.org/sal/production?q=synchronized&d=2012-01-01
A trivial example timeline is given below.
-->

'''All times in UTC.'''

* 14:38 <hoo@deploy1001> Synchronized wmf-config/Wikibase.php: Wikibase Client: Fix setting name typo (T244529) (duration: 01m 20s)00:00
* 14:40 logmsgbot: !log hoo@deploy1001 Scap failed!: 9/11 canaries failed their endpoint checks
* 14:40 first icinga alert: PROBLEM - MediaWiki exceptions and fatals per minute on icinga1001 is CRITICAL
* 14:40 many more icinga alerts: PHP7 rendering and Apache HTTP criticals
* 14:41 hoo: How do I force the deploy
* 14:41 icinga-wm floods out of the channel
* 14:43 <hoo@deploy1001> Synchronized wmf-config/Wikibase.php: REVERT: Wikibase Client: Fix setting name typo (T244529) (duration: 01m 40s)
* 14:45 icinga-wm returns to the channel, recovery messages begin

Note that after this, we still had the UBN bug to fix up, without causing another outage.",''What weaknesses did we learn about and how can we address them?'',"''Explicit next steps to prevent this from happening again as much as possible, with Phabricator tasks linked for every step.''

'''NOTE''': Please add the [[phab:tag/wikimedia-incident/|#wikimedia-incident]] Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.
* Make sure dashboards are up to date for new features/functionality  before planned transitions to them begin
* {{phab|T248866}} - Consider having a linter that could catch config file entries that set unused variables
* {{phab|T244697}} - Determine why switching group2 wikis to read from the new wb terms store caused the issue
* {{phab|T245046}} - Make sure that UBN/emergency deploys go through releng and SRE teams so that everyone is in the loop (and if SRE folks are in the middle of something urgent, they can ask deployers to wait a bit). See: [[Deployments/Emergencies]] in draft status.
* {{phab|T218412}} - Consider bundling config and branch versions together for deployments ([[User:20after4]] knows about this)
{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}",2020-02-07,Cache,Unknown,Unknown,Unknown,Unknown,2020-02-07_wikidata.wikitext
"During maintenance on our CDN edge cache layer in eqiad, all caching servers were accidentally depooled due to insufficient safeguards in tooling (and errors in following the maintenance procedure). Users whose traffic was routed to eqiad were unable to access wikis or any Wikimedia site for about 15 minutes.","All users of all services behind the eqiad caching layer for about 15 minutes until eqiad was depooled (and that change could propagate).

Estimate we lost about 30k RPS for slightly over 15 minutes: about 27M req lost, or just under 20% of total traffic at the time.",Both humans and monitoring detected the issue. Humans were slightly faster. Icinga paged and alerted several SREs.,'''All times in UTC.''',This outage could have been easily prevented if our infrastructure was more cautious about what it allowed.,"''Explicit next steps to prevent this from happening again as much as possible, with Phabricator tasks linked for every step.''

'''NOTE''': Please add the [[phab:tag/wikimedia-incident/|#wikimedia-incident]] Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.

* Create an automated alert for 'too many nodes depooled from a service' [[phab:T245058]]
* The <code>depool</code> & <code>confctl</code> commands should print warnings (or error out entirely, unless you override with e.g. a <code>--force</code> flag) if too many hosts are depooled from the same service. [[phab:T245059]]
* Investigate why Pybal didn't reject a configuration with only one server pooled, and send traffic to some of the depooled servers anyway. [[phab:T245060]]
* There should be an easy script for SREs and other technical contributors to override where their own traffic is routed for debugging tools (grafana/logstash/etc).  [[phab:T244761]]


{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}",2020-02-11,Caching,Unknown,Caching,Unknown,Unknown,2020-02-11_caching-proxies.wikitext
"A router hardware failure caused widespread issues. While a single hardware failure like that shouldn't cause issues, more hardware failure had preceded this one, lowering our capability to absorb failures.","All services were affected for all types of users for large portions of Europe and everyone reaching our projects via Amsterdam. That includes parts of Asia and Africa. It is difficult to gauge the exact numbers as it was highly related to the network path from the user to our infrastructure.

Eyeballing the [https://grafana.wikimedia.org/d/000000479/frontend-traffic?orgId=1&from=1582551010505&to=1582555314267 traffic graph] for the interval, we lost about 47M queries over an interval of approx. 20 minutes.",Humans detected this first. Icinga alerted 1 minute later with pages for cr2-esams arriving to multiple SREs,"'''All times in UTC.'''

* 13:55:55 Pages for cr2-esams arrive at multiple SRE people
* 13:56:37 bast3004 is also down for multiple SRE.
* 14:00:56 XioNoX: cr2-esams: 2020-02-24 13:52:02 UTC Major FPC 0 Major Errors - Lkup Error code: 0x40038
* 14:00:04 XioNoX: looks like a linecard failure as well
* 14:03:37 _joe_: esams depooled
* 14:12:05 paravoid: what *works* now?
* 14:14:06 paravoid all cr2-esams interfaces are down
* 14:15:22 paravoid the links to asw2 are et-1/0/0 & et-1/0/1, and show up as up, but are on FPC 1
* 14:15:42 _joe_: now we're in the business of restoring esams
* 14:18:36 bblack: normal volume on vfe-reported GET was 86K/s before the cr2 hit, then it was around 40K just before dns depool, latest sample is ~5K and dropping
* 14:19:41 paravoid: so we have two MX480s both with major FPC errors after a JunOS upgrade
* 14:19:49  paravoid: and a site down
* 14:19:52  XioNoX: paravoid: cr3 didn't get upgraded
* 14:20:23 bblack charting codfw+eqiad+esams aggregate reqs (those involved in the shuffle), we look to be back at ""normal"" total reqs as expected, at least roughly https://w.wiki/J65 
* 14:28:24 XioNoX: akosiaris: Service Request ID 2020-0224-0258 has been created. 
* 14:35:08 marostegui: so, are we fully up? (trying to document stuff)
* 14:35:43 akosiaris: marostegui: as far as I know and as far as users are concerned, yes
* 14:36:00 Outage reported as having ended.
* 14:42:01 paravoid: esams has two MX480s, and both failed today
* 14:42:13 paravoid: eqiad and codfw also have two MX480s for core routers
* 14:48:42 mark: so for RMA
* 14:48:46  mark: we can't swap line cards at esams
* 14:48:49 mark: the PDUs are in the way :)
* 21:40 volans| bblack, XioNoX: just noticed that the CA app email still complains about ns2 IP and in effect I cannot reach ns2
* 22:20 volans| Prefix 91.198.174.0/24: NOT ADVERTISED since 2020-02-24T22:20:13.69323Z, on_demand=True
* 22:50 XioNoX| so traffic does internet -> cr3-knams -> cr3-esams -> asw2 -> server
* 22:59 XioNoX| bblack, volans, ns2 is now redirected to eqiad",* esams is a SPOF effectively.,"* Restricted phabricator tasks [https://phabricator.wikimedia.org/ T246009] and [https://phabricator.wikimedia.org/ T245825] have been filed for replacing the FPCs on the routers after RMA process was recommendended by the vendor. Tasks are restricted per the usual policy for vendors

{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}",2020-02-24,Network,Unknown,Unknown,Unknown,Unknown,2020-02-24_esams.wikitext
"Logged-out users saw the user interface in their browser language (<code>Accept-Language</code> header) rather than the default wiki content language,
due to a configuration change inadvertently causing the <code>$wgULSLanguageDetection</code> setting to be unset and falling back to the default in the UniversalLanguageSelector extension.","Anonymous users were affected, registered users were not.

Appserver load was increased due to the cache being split by <code>Accept-Language</code>. Average response time also went up considerably [https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?orgId=1&from=1582584852340&to=1582643708598&var-datasource=eqiad%20prometheus%2Fops&var-cluster=appserver&var-method=GET&var-code=200].","Reported by users at [[phabricator:T246071]] and other tasks.
There were no alerts as far as {{U|Lucas Werkmeister (WMDE)}} is aware.

''If human only, an actionable should probably be ""add alerting"".''","''This is a step by step outline of what happened to cause the incident and how it was remedied.  Include the lead-up to the incident, as well as any epilogue, and clearly indicate when the user-visible outage began and ended.''
<!-- Tips/Reminders:
- Graphs of the error rate or other surrogate, if available, are useful as well.
- Please ensure to remove private information.
- You can link to a specific offset in SAL using the SAL tool: https://tools.wmflabs.org/sal/
  For example: https://tools.wmflabs.org/sal/production?q=synchronized&d=2012-01-01
A trivial example timeline is given below.
-->

'''All times in UTC.'''
* 00:35 Config patch to delete fixcopyrightwiki and related config was merged: https://gerrit.wikimedia.org/r/552549 
* 00:43 Synchronized dblists/all.dblist: T238803: Remove fixcopyrightwiki from all.dblist (duration: 00m 56s)
* 00:45 rebuilt and synchronized wikiversions files: T238803: Remove fixcopyrightwiki from wikiversions
* 00:46 [[toolforge:sal/log/AXB5zv8wfYQT6VcDaeZw|Synchronized dblists/]] ([[gerrit:552549|Delete fixcopyrightwiki]]). [https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?from=1582588800000&to=1582596000000&var-datasource=eqiad%20prometheus%2Fops&var-cluster=appserver&var-method=GET&fullscreen&panelId=9 Average response time] rises immediately (from ca. 180 ms to ca. 240 ms), though it’s not clear why. The issue may have begun here already.
* 00:51 Synchronized wmf-config/CommonSettings.php: Stop trying to read wmgUseSkinPerPage or wmgUseEUCopyrightCampaign (duration: 00m 55s)
* 00:51 Ran <code>DELETE FROM globalimagelinks WHERE gil_wiki='fixcopyrightwiki';</code> (1 row removed) T238803
* 00:53 '''ISSUE BEGINS''': [[toolforge:sal/log/AXB51VvNfYQT6VcDaejf|Synchronized wmf-config/InitialiseSettings.php]] [[gerrit:552549|Remove all IS config related to fixcopyrightwiki]] (duration: 00m 55s)
* 08:41 [[phabricator:T246071|T246071]] created
* 09:59 [[phabricator:T246081|T246081]] created (duplicate)
* 10:04 T246071 escalated to ""Unbreak Now!"" priority
* 10:41 likely cause (turned out to be correct) pointed out at [[phabricator:T246071#5915117|T246071#5915117]]
* 12:00 (ca.) [https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?from=1582617600000&to=1582646400000&var-datasource=eqiad%20prometheus%2Fops&var-cluster=appserver&var-method=GET&fullscreen&panelId=9 Average response time] begins to rise much further, not leveling until incident was resolved. Probably the first sync of the EU SWAT finished the previous sync from 00:53.
* 12:15 [[phabricator:T246095|T246095]] created (duplicate)
* 12:26 T246071 brought up on #mediawiki-i18n (IRC), asking for Language Team attention
* ~12:30 Average response time starts going up considerably [more traffic?]
* 13:01 T246071 brought up in #mediawiki-releng (IRC)
* 13:28 [[gerrit:574743]] uploaded
* 14:13 [[gerrit:574743]] merged
* 14:15 [[toolforge:sal/log/AXB8tD_t0fjmsHBahZmH|Synchronized wmf-config/InitialiseSettings.php]] ([[gerrit:574743|Reinstate wgULSLanguageDetection setting]])
* 14:35 '''ISSUE ENDS''': [[toolforge:sal/log/AXB8xlLMfYQT6VcDbCnZ|Synchronized php-1.35.0-wmf.20/extensions/Wikibase/lib]] ([[gerrit:574746|wbterms: only select entity terms that are requested]]), effectively finishing the previous sync ([[phabricator:T236104|T236104]])

Other links:
* [https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?orgId=1&from=1582581600000&to=1582642800000&var-datasource=eqiad%20prometheus%2Fops&var-cluster=appserver&var-method=GET Grafana]
* [https://tools.wmflabs.org/sal/production?p=0&q=Synchronized&d=2020-02-25 SAL of syncs]",''What weaknesses did we learn about and how can we address them?'',"''Explicit next steps to prevent this from happening again as much as possible, with Phabricator tasks linked for every step.''

'''NOTE''': Please add the [[phab:tag/wikimedia-incident/|#wikimedia-incident]] Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.
* {{done}} Remind more people of double sync workaround for [[phabricator:T236104|T236104]] – ops-l email by {{U|Ladsgroup}}, [[Special:Diff/1856681|SWAT documentation update]]
* [[phabricator:T236104|T236104]]
* [[phabricator:T246212|T246212]] - the setting was moved to CommonSettings.php and documented.
* …

{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}",2020-02-25,Cache,Unknown,Unknown,Unknown,Unknown,2020-02-25_mediawiki_interface_language.wikitext
Parsercache databases got overloaded due to a malfunctioning host which resulted on spikes of connections on the other 2 active hosts and increased latency on our mwapps servers.,"* Query latency was increased https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?orgId=1&var-datasource=eqiad%20prometheus%2Fops&var-cluster=appserver&var-method=GET&var-code=200&from=1584378662200&to=1584387599259&fullscreen&panelId=31
[[File:Application-servers-red-dashboard (1).png|thumb]]

* mw app servers got their workers saturated: https://grafana.wikimedia.org/d/000000550/mediawiki-application-servers?orgId=1&fullscreen&panelId=92&from=1584358493597&to=1584421429638

* Higher than usual response time https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?orgId=1&var-datasource=eqiad%20prometheus%2Fops&var-cluster=appserver&var-method=GET&var-code=200&from=1584378662200&to=1584387599259&fullscreen&panelId=9
[[File:High response time.png|thumb]]","Icinga paged for pc1008 host that was having performance degradation
 18:43:14 <+icinga-wm> PROBLEM - MariaDB Slave SQL: pc2 #page on pc1008 is CRITICAL: CRITICAL slave_sql_state could not connect 
 https://wikitech.wikimedia.org/wiki/MariaDB/troubleshooting%23Depooling_a_slave","'''All times in UTC.'''

* 18:00 ''' Degradation begins'''
* 18:00 pc1008 starts having performance issues and its disk latency starts increasing, connections start to pile up on pc1008
[[File:Mysql (14).png|thumb]]
[[File:Mysql (13).png|thumb]]
* 18:00 Other hosts (pc1007 and pc1009) also start suffering more idle connections as the result of pc1008 failing to handle connections as fast as usual
[[File:Mysql (15).png|thumb]]
* 18:00 Average response time increases
[[File:High response time.png|thumb]]
* 18:43  ''<+icinga-wm> PROBLEM - MariaDB Slave SQL: pc2 #page on pc1008 is CRITICAL: CRITICAL slave_sql_state could not connect'' 
* 18:43-19:44 A number of SREs and 2 DBAs respond and troubleshooting starts
* 19:11 DBAs Replace pc1008 with pc1010 (which is a spare for a different pc group, and has 1/3 of the key), but worth trying as there were no more ideas and pc1008 was checked for HW errors, misconfigurations and such and all looked fine anyways.
* 19:12 Response time, idle connections on other hosts, latency...they all start to get better
* 19:24 Values almost around the same before the incident (considering that 1/3 of the pc keys were gone)
* 19:24 '''Degradation stops'''","The hardware performance degradation was hard to detect via the usual checks: broken BBU, degraded RAID, disks with errors that hasn't removed from the RAID, memory issues.... 
As nothing appeared to be broken, DBAs didn't consider pc1008 as the core of the issue. 
The fact that all the parsercache showed similar connections spike pattern made us think that the problem was on the other side of the spectrum (MW). 

We later learned thanks to Brad, that parsercache has a ""double write"" behaviour we didn't know of and if one of those fails, the others keep hanging until the request is processed or shutdown.","* [RFC] improve parsercache replication, sharding and HA: https://phabricator.wikimedia.org/T133523
* Investigate pc1008 for possible hardware issues / performance under high load: https://phabricator.wikimedia.org/T247787
** Once pc1008 is back full - repool it to make sure it is fully fixed after re-creating the raid
** Purge pc1010 old rows once it is out of rotation
* Parsercache sudden increase of connections: https://phabricator.wikimedia.org/T247788#5976651

{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}",2020-03-19,Server,Unknown,Mediawiki,Unknown,Unknown,2020-03-19_parsercache.wikitext
"Unexpected loss of internal connectivity to codfw hosts and services for 5 minutes, creating user-visible failed queries for users whose traffic hits our eqsin and ulsfo edges, when they were using services that are active/active (Swift, Maps, Restbase API, ...)

Cause was [[phab:T248394|maintenance]] that required a linecard restart on cr1-codfw, which exposed a flaw in codfw's network design.

Loss of some external connectivity to codfw was expected, and the site was CDN-depooled before the maintenance began.  However, what was unanticipated was that cr1-codfw would hold VRRP mastership for the duration of the linecard restart, so it tried to act as the default gateway for all hosts in the cluster, while effectively being a black hole for routing anywhere outside the cluster (as the linecard being rebooted has both all the cross-cluster links and the router-to-router interconnect).

There was a second OSPF flap/convergence event around 12:22, however it doesn't seem to have been impactful.","~28k queries lost for queries terminated in ulsfo and eqsin against active/active services https://logstash.wikimedia.org/goto/bcab629e395fc8a71ef9ac5d525c1ec7

Although this was <1% of global HTTP traffic at the time, upload-lb requests in ulsfo and eqsin were very much affected -- so users of Wikimedia Commons images, or of map tiles whose traffic terminates in those datacenters.  Impact on upload-lb in ulsfo was ~10% of requests failed for the interval; in eqsin, about 1.5%.

This also created Kafka mirrormaker delays, the impact of which is TODO","Automated: Icinga pages for service IPs in codfw, in addition to alerts for socket timeouts against many hosts (especially appservers).

Since all codfw appservers could not be reached, there *would* have been lots of alert spam in #wikimedia-operations (one per appserver) -- except that icinga-wm got <tt>Excess Flood</tt>ed off of IRC.","see also:
* [https://phabricator.wikimedia.org/P10765 all OSPF-related logs] across all routers
* [https://phabricator.wikimedia.org/P10766 all 'neighbor up/down' logs] across all routers

'''All times in UTC.'''
* 11:35 <cdanis> depool codfw for router maintenance T248394 https://tools.wmflabs.org/sal/log/AXEReW8N0fjmsHBaqHGh

* 11:50:08  re0.cr1-codfw mgd[96118]: UI_CMDLINE_READ_LINE: User 'cdanis', command 'request chassis fpc restart slot 5 '   '''OUTAGE BEGINS'''
* 11:50:17 first socket timeout reported from icinga1001.  icinga2001 sees no socket timeouts or unreachable hosts at any point, as most of its traffic flows via the switches in codfw -- aside from frack hosts (as those also traverse the core router)
* 11:50:20 first user-visible HTTP 502 response
* 11:53:07 TTL exceeded event seen for cross-cluster traffic from icinga1001: icinga2001 icinga: HOST ALERT: pfw3-eqiad;DOWN;SOFT;1;CRITICAL - Time to live exceeded (208.80.154.219)
* 11:53:27 Icinga meta-monitoring fails against icinga2001 -- expected; external connectivity via some routes was likely to be impacted
* 11:54:48 first page sent: <tt>search.svc.codfw.wmnet;LVS HTTPS IPv4 #page;CRITICAL;HARD;3;CRITICAL - Socket timeout after 10 seconds</tt>
* 11:55:21 last socket timeout & first recovery reported from icinga1001   
* 11:55:41 final OSPF state change: <tt>cr1-codfw.wikimedia        RPD        RPD_OSPF_NBRUP: OSPF neighbor fe80::aad0:e5ff:fee3:87c5 (realm ipv6-unicast ae0.0 area 0.0.0.0) state changed from Loading to Full due to LoadDone (event reason: OSPF loading completed)</tt>
* 11:55:44 icinga1001 reports TTL exceeded events for eqsin hosts: <tt>Mar 25 11:55:44 icinga1001 icinga: HOST ALERT: upload-lb.eqsin.wikimedia.org;DOWN;SOFT;1;CRITICAL - Time to live exceeded (103.102.166.240)</tt> and also reports PING CRITICAL for text-lb.ulsfo.wikimedia.org_ipv6 and upload-lb.eqsin.wikimedia.org_ipv6
* 11:56:02 end of the penultimate spike of user-visible HTTP 502 responses
* 11:56:43 start of the final spike of user-visible HTTP 502 responses
* 11:56:54 end of the final spike of user-visible HTTP 502 responses '''OUTAGE ENDS'''
* 11:57:51 first recovery for the unreachable-from-icinga1001 ulsfo/eqsin hosts","=== What went well? ===
* automated monitoring detected the incident very well","''Explicit next steps to prevent this from happening again as much as possible, with Phabricator tasks linked for every step.''

'''NOTE''': Please add the [[phab:tag/wikimedia-incident/|#wikimedia-incident]] Phabricator project to these follow-up tasks and move them to the ""follow-up/actionable"" column.
* Add linecard diversity to the router-to-router interconnect in codfw.  [[phab:T248506]]
* Consider plumbing a backup router cross-connect via a new VLAN on the access switches.

{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}",2020-03-25,Unknown,Unknown,Unknown,Unknown,Unknown,2020-03-25_codfw-network.wikitext
"'''Impact:''' For about 20 minutes, almost all content page parses were broken. They received, along with any other user interaction with the <code>wb_items_per_site</code> table in some way, a database error message instead of page content. This affected many (but not all or even most) page views for logged-in users, and many (possibly most, but not all) edits made to wikis. For 19 hours, all users looking at pages rendered since the incident start received some incorrect metadata on pageviews (infobox content, cross-language links, wikdata item relation). Also, Wikidata's checks for duplicate site links failed, leading to many hundreds of duplicate items being created.

'''Cause:''' Wikidata's <code>wb_items_per_site</code> secondary table, used for the rendering of every page connected to Wikidata, was dropped by a mis-configured weekly cron script which executed the <code>update.php</code> code path, which itself had been misconfigured for eight years to drop this table. This immediately led to a <code>DBQueryError</code> on content page loads (reads). The table was re-created as blank, at which point pages began to paint again (though wrongly).

{{TOC|align=right}}",,"* First report to #wikimedia-operations at 23:03:57 UTC <code><NotASpy> just got an error - [Xou1IQpAIDEAAC74T5wAAAOQ] 2020-04-06 23:03:09: Fatal exception of type ""Wikimedia\Rdbms\DBQueryError""</code> ([https://wm-bot.wmflabs.org/logs/%23wikimedia-operations/20200406.txt log])
* icinga-wm reported problems a minute later at 23:05:33 UTC <code><icinga-wm> PROBLEM - MediaWiki exceptions and fatals per minute on icinga1001 is CRITICAL: cluster=logstash job=statsd_exporter level=ERROR site=eqiad https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops</code>

* There was '''not''' an alert that indicated this was happening for most parser cache renders, or that there was a high rate (>700/second) of Mediawiki serving 50X errors to the cache servers [https://grafana.wikimedia.org/d/000000479/frontend-traffic?orgId=1&from=1586211392174&to=1586219116263].  There should have been automated detection of such a condition, including paging SRE -- if this had happened at off-hours, it could have gone unnoticed by engineers for far too long.","'''All times in UTC.'''

; 2020-04-06
* 23:00 weekly cron from puppet <code>cron/wikibase/dumpwikibaserdf.sh</code> executes
* 23:02 The cron calls cron/wikibase/wikibasedumps-shared.sh, which calls <code>sql.php</code> to run a minor ''ad hoc'' query.
** Due to [[phab:T157651]], noted in February 2017, this ran <code>LoadExtensionSchemaUpdates</code>, bypassing the checks in <code>update.php</code> to prevent this ever happening in production
** Due to a bug in Wikibase's LoadExtensionSchemaUpdates code dating from eight years ago, this wrongly dropped the <code>wb_items_per_site</code> table. '''OUTAGE BEGINS'''
* 23:02:13: first log message indicating the table is missing
* 23:03:57: first user report of an issue: <code><NotASpy> just got an error - [Xou1IQpAIDEAAC74T5wAAAOQ] 2020-04-06 23:03:09: Fatal exception of type ""Wikimedia\Rdbms\DBQueryError""</code> ([https://wm-bot.wmflabs.org/logs/%23wikimedia-operations/20200406.txt log])
* 23:05:33: first automated report of the issue: <code><icinga-wm> PROBLEM - MediaWiki exceptions and fatals per minute on icinga1001 is CRITICAL</code>
* …
* 23:26 [https://tools.wmflabs.org/sal/log/AXFR0SJWfYQT6VcDjydX Amir re-creates the table as blank]. Users no longer receive error messages instead of content (but metadata might be inaccurate). '''OUTAGE ENDS, USER IMPACT CONTINUES'''
* 23:31 [https://tools.wmflabs.org/sal/log/AXFR1ZNU0fjmsHBaqKfP Amir triggers rebuildItemsPerSite to begin]
* 23:59 [https://tools.wmflabs.org/sal/log/AXFR7zek0fjmsHBaqKgO Adam removes broken code from Wikibase's LoadExtensionSchemaUpdates in production]

; 2020-04-07
* [https://tools.wmflabs.org/sal/log/AXFSLP6vvrJzePItJ9-3 01:06] - [https://tools.wmflabs.org/sal/log/AXFSLm6t0fjmsHBaqKhO 01:08] James F removes sql.php in production
* 10:27 [https://tools.wmflabs.org/sal/log/AXFULeYIvrJzePItJ-Ef jynus: starting recovery on db1099:3318]
* 10:41 [https://tools.wmflabs.org/sal/log/AXFUOu0dvrJzePItJ-Ep addshore deploy update to rebuildItemsPerSite.php script that allows lists of ids (needed for repopulating the gap)]
* 11:07 [https://tools.wmflabs.org/sal/log/AXFUUwr90fjmsHBaqKno jynus: starting recovery on all s8 hosts]
* 11:36 [https://tools.wmflabs.org/sal/log/AXFUbQ8lvrJzePItJ-FK Amir1: stopped the rebuilt script]
* 11:42 [https://tools.wmflabs.org/sal/log/AXFUc1200fjmsHBaqKn9 marostegui: Depool db1092, db1111, db1099:3318 for table rename]
* 11:45 [https://tools.wmflabs.org/sal/log/AXFUdafzvrJzePItJ-FP jynus: stopping s8 replication on db1116:3318, db1095:3318, db2079]
* 11:50 [https://tools.wmflabs.org/sal/log/AXFUejNF0fjmsHBaqKoF jynus: renaming wb_items_per_site_recovered to wb_items_per_site on s8]
** marostegui [https://tools.wmflabs.org/sal/log/AXFUerHM0fjmsHBaqKoH Repool db1092, db1111, db1099:3318 after table rename] '''USER IMPACT REDUCED'''
** User impact reduced here as now the bulk of the data is back but some will be outdated (by 14.5 hours)
** Bad entries of rendered pages also still exist in the parser cache
* 11:51 [https://tools.wmflabs.org/sal/log/AXFUe40MfYQT6VcDjymM marostegui: depool db1126]
* 11:52 [https://tools.wmflabs.org/sal/log/AXFUfBBBvrJzePItJ-Fc marostegui: repool db1126]
* 12:06 addshore: start rebuild script take 1
* 12:42 addshore: restart run 1 of the rebuild script
* 12:50 [https://tools.wmflabs.org/sal/log/AXFUsWuE0fjmsHBaqKp7 addshore: start run 2 of rebuild script (with no redirects in the list of items)]
* 13:23 rebuild of tables from maint script is done (addshore) '''USER IMPACT REDUCED'''
** At this stage the table contains mostly complete and up to date data (with a few thousand items with incorrectly stored links, see [[phab:T249613]])
* 13:55 addshore: bad sync of change to CommonSettings aborted and reverted
* 14:08 addshore, RejectParserCacheValue entries during wb_items_per_site drop incident (1/14.5h)
* 14:15 addshore, RejectParserCacheValue entries during wb_items_per_site drop incident (2/14.5h)
* 14:25 addshore, RejectParserCacheValue entries during wb_items_per_site drop incident (4/14.5h)
* 14:35 addshore, RejectParserCacheValue entries during wb_items_per_site drop incident (8/14.5h)
* 14:56 addshore, RejectParserCacheValue entries during wb_items_per_site drop incident (10/14.5h)
* 15:17 addshore, RejectParserCacheValue entries during wb_items_per_site drop incident (12/14.5h)
* 17:55 addshore, RejectParserCacheValue entries during wb_items_per_site drop incident (14.5/14.5h) '''USER IMPACT REDUCED'''
** At this stage all parser cache entries from the time that the table was missing or partial will be rejected
** For the next 24 hours, anonymous users may still be served badly-rendered pages that have persisted in the Varnish cache
* …

<!-- Reminder: No private information on this page! -->
<mark>TODO: Add epilogue/cleanup.</mark>

[https://grafana.wikimedia.org/d/000000102/production-logging?orgId=1&from=1586213400000&to=1586217000000 Mediawiki exceptions/fatals]

[https://grafana.wikimedia.org/d/000000479/frontend-traffic?orgId=1&from=1586213400000&to=1586217000000 Edge traffic error rate]","=== What went well? ===
* Outage cause was root-caused quickly
* WMF Backup/restore system worked as intended, in terms of data recovery
* Developers familiar with the relevant codebase were readily available and actively working on a fix/mitigation
* DBAs were responsive despite late night/early morning time for them","* Remove <code>ALTER</code> and <code>DROP</code> permissions from the <code>wikiadmin</code> user (which is used by automated crons).  Create a separate user for humans performing schema changes. https://phabricator.wikimedia.org/T249683
** Dumps processes probably shouldn't run with a SQL user that can alter any data, let alone drop tables.
** Potentially, permissions should be much more fine-grained:  separate users for maintenance scripts/crons; for testing ro queries; one for dumps generation; core vs extensions maintenance, etc.
** Maybe <code>DROP TABLE</code> should not granted to anyone except DBAs?
** Maybe ""Database::dropTable"" should check for production environment (similar to update.php) and fail if something like that happens.
* <code>sql.php</code> must re-implement <code>update.php</code>'s not-in-production checks if we're going to restore it.
* Having both <code>mwscript sql.php</code> and <code>sql</code> (aka <code>mwscript mysql.php</code>) which run ''very'' different code paths is confusing. Fix this.
* Change Wikidata cronjob to use <code>mwscript mysql.php</code> instead of <code>mwscript sql.php</code>: [[gerrit:587218]]
* <code>sql.php</code> must not run <code>LoadExtensionSchemaUpdates</code> hooks: [[phab:T157651]]
* Pursue restructuring Wikibase repository's SQL directory: [[phab:T205094]]
* Consider adding paging based on ats-be-perceived 50X error rate, which considering recent history looks to be a good signal. <mark>TODO make a task</mark>
* Wikibase schema updaters must not modify database directly [[phab:T249598]]
<mark>TODO: Add the [[phab:tag/wikimedia-incident/|#Wikimedia-Incident]] Phabricator tag to these tasks and move them to the ""Follow-up"" column.</mark>",2020-04-07,Unknown,Unknown,Mediawiki,Unknown,Unknown,2020-04-07_Wikidata%27s_wb_items_per_site_table_dropped.wikitext
"The virtual chassis link between asw2-d1-eqiad and asw2-d8-eqiad failed in two steps.

First on Friday the 1st where it was causing packet loss for hosts on D1 without any other signs of failures.

This packet loss caused connectivity issues between MediaWiki appservers (and API servers at a lower scale) and memcache servers. Resulting in a significant increase of MediaWiki exceptions being served to the users.

This got worked around for the weekend by depooling D1 servers. At this point the cause of the packet loss was unknown.

The day after, on Saturday, hosts in D8 started seeing the same issues as in D1. This time the switches were logging errors about the D1-D8 link. Disabling the link solved the issues.

'''Impact''': This had little to no effect on traffic ([[Media:20200501-vc-link-failure-Varnish_HTTP_Total.png|Varnish_HTTP_Total]]), error rates ([[Media:A0200501-vc-link-failure-ATS-TLS Availability.png|ATS availability]]) and latencies ([[Media:20200501-vc-link-failure-Navtiming_request.png|Navtiming requests]]) for anonymous users. A increase in error rates (~1% of requests had errors, with a short spike to ~7.5%, [[Media:20200501-vc-link-failure-Appserver_Errors.png|Appserver errors]]) and an increase in tail latency (around plus 100%-150%, [[Media:20200501-vc-link-failure-Appserver-p95.png|Appserver p95]]) has been observed for logged in users, though.

{{TOC|align=right}}",,"* <+icinga-wm> PROBLEM - MediaWiki exceptions and fatals per minute on icinga1001 is CRITICAL: cluster=logstash job=statsd_exporter level=ERROR site=eqiad https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops
* <+icinga-wm> PROBLEM - PHP7 rendering on mwXXXX is CRITICAL: CRITICAL - Socket timeout after 10 seconds

* Did the appropriate alert(s) fire? Yes
* Was the alert volume manageable? Yes
* Did they point to the problem with as much accuracy as possible? No
The root cause didn't generate any logs at first, and when it did, those logs didn't trigger alerts.","'''All times in UTC.'''
Friday 1st:
* 05:21 MW exceptions starts being reported on #wikimedia-operations <+icinga-wm> PROBLEM - MediaWiki exceptions and fatals per minute on icinga1001 is CRITICAL: cluster=logstash job=statsd_exporter level=ERROR site=eqiad https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops '''OUTAGE BEGINS'''
* 05:32 <+icinga-wm> PROBLEM - MediaWiki exceptions and fatals per minute on icinga1001 is CRITICAL: cluster=logstash job=statsd_exporter level=ERROR site=eqiad https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops
* 05:33 < marostegui> wow those fatals really increased
* 05:33 <+icinga-wm> PROBLEM - MediaWiki memcached error rate on icinga1001 is CRITICAL: 5009 gt 5000 https://wikitech.wikimedia.org/wiki/Memcached https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=1&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops
* Joe sends the following patches: https://gerrit.wikimedia.org/r/#/c/operations/puppet/+/593728/ https://gerrit.wikimedia.org/r/#/c/operations/puppet/+/593727/
* 7:40 Arzhel checks switches (especially D/D1) nothing out of ordinary
* Large but steady increase of TCP retransmits on mc1021/1029 - https://grafana.wikimedia.org/d/000000365/network-performances?panelId=15&fullscreen&orgId=1&from=now-7d&to=now&var-server=mc1029&var-datasource=eqiad%20prometheus%2Fops
* 8:29 <elukey> _joe_ there are some things that are off, namely a lot of  traffic patterns showing spikes every 10m, I am wondering if in some twisted way the 10m TTL of the  gutter is somehow exacerbating this problem
* 8:37 <elukey> so mw1331 for example doesn't show tkos, I think it is only the servers in D1
* 8:43 <_joe_> I will bring back mw1409 and mw1407 in the pool, and we can depool those servers in D1
* 8:54 <_joe_> !log depooled all servers in the app pool in rack D1 '''OUTAGE WORKED AROUND'''
* 8:55 +icinga-wm> IRC echo bot RECOVERY - MediaWiki exceptions and fatals per minute on icinga1001 is OK: All metrics within thresholds. https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops
* 19:57 rzl depools api servers in D1 (mw1356-1362) at _joe_’s suggestion, in response to flapping alerts like “PROBLEM - PHP7 rendering on mw1361 is CRITICAL: CRITICAL - Socket timeout after 10 seconds”
Saturday 2nd:
* (Overnight) Wall of flapping PROBLEM - PHP7 rendering on mwXXXX is CRITICAL: CRITICAL - Socket timeout after 10 seconds '''OUTAGE RESURFACE'''
* 06:42 Giuseppe and Luca start investigating 
* 06:52 Arzhel starts investigating
* 07:08 <XioNoX> asw2-d-eqiad> request virtual-chassis vc-port delete pic-slot 1 port 0 member 1 '''OUTAGE ENDS'''
* 07:49 <oblivian@cumin1001> 	conftool action : set/pooled=yes; selector: name=mw13(49|5[0-9]|6[0-2])\.eqiad\.wmnet","* Packet loss through Virtual Chassis Fabric are difficult to pinpoint
* Higher layers monitoring worked as expected
* From history, this failure scenario has a low probability of happening, and is now documented","* Either re-cable or cleanup disabled cable - https://phabricator.wikimedia.org/T251663
* Add log alerting for VC link failure - https://phabricator.wikimedia.org/T251663",2020-05-01,Unknown,Unknown,Mediawiki,Unknown,Unknown,2020-05-01_vc-link-failure.wikitext
"On May 5th, 2020, all Wikidata Query Service instances were down for about 5 minutes due to a failed deployment. The Failed deployment was a result of two factors - an incorrectly executed deployment procedure and a bug in the code. Deployment that day was itself a result of a failed deployment the day before (May 4th), which in turn was caused by double jars present in the deploy repo.

Bug in the code that ultimately brought down the service (Blazegraph) was the removal of serialVersionUID field from WKTSerializer class. It turned out to be needed by Blazegraph, but that that need wasn't caught by standard integration/unit tests. Effect on the instances updated with faulty code was that Blazegraph (WDQS's backend) couldn't start.

Deployment was stopped after some number of instances started to fail and the previous version was rolled back. Because of how WDQS metrics are presented (all of the errors are rolled into one metric, even ""standard"" ones like user throttling), we don't know the exact number of queries affected, but metrics do not show significant impact. In any case, visibility here is problematic and needs to be addressed).
{{TOC|align=right}}",,"The issue was detected by icinga and the alert was posted on #wikimedia-operations. 
 <pre class=""mw-collapsible mw-collapsed"">PROBLEM - Check systemd state on wdqs1003 is CRITICAL: CRITICAL - degraded: The system is operational but one or more units failed. https://wikitech.wikimedia.org/wiki/Monitoring/check_systemd_state
09:43 PROBLEM - Query Service HTTP Port on wdqs2003 is CRITICAL: HTTP CRITICAL: HTTP/1.1 500 Server Error - 9597 bytes in 0.006 second response time https://wikitech.wikimedia.org/wiki/Wikidata_query_service
09:43 PROBLEM - Query Service HTTP Port on wdqs2001 is CRITICAL: HTTP CRITICAL: HTTP/1.1 500 Server Error - 9597 bytes in 0.008 second response time https://wikitech.wikimedia.org/wiki/Wikidata_query_service
09:44 PROBLEM - Check systemd state on wdqs2003 is CRITICAL: CRITICAL - degraded: The system is operational but one or more units failed. https://wikitech.wikimedia.org/wiki/Monitoring/check_systemd_state
09:45 PROBLEM - PyBal backends health check on lvs2010 is CRITICAL: PYBAL CRITICAL - CRITICAL - wdqs-heavy-queries_8888: Servers wdqs2002.codfw.wmnet are marked down but pooled: wdqs-ssl_443: Servers wdqs2002.codfw.wmnet are marked down but pooled: wdqs-internal_80: Servers wdqs2004.codfw.wmnet are marked down but pooled: wdqs_80: Servers wdqs2002.codfw.wmnet are marked down but pooled https://wikitech.wikimedia.org/wiki/PyBal
09:45 PROBLEM - LVS HTTP codfw IPv4 #page on wdqs.svc.codfw.wmnet is CRITICAL: HTTP CRITICAL: HTTP/1.1 500 Server Error - 9597 bytes in 0.078 second response time https://wikitech.wikimedia.org/wiki/LVS%23Diagnosing_problems
09:45 PROBLEM - Check systemd state on wdqs2001 is CRITICAL: CRITICAL - degraded: The system is operational but one or more units failed. https://wikitech.wikimedia.org/wiki/Monitoring/check_systemd_state</pre>

Fortunately, preexisting warnings were enough to stop deployment and rollback servers already updated with the faulty version.

As for the faulty version itself, since manual tests weren't run after the canary (what should've happen), we didn't actually know that service was faulty. There are no automatic tests during deployment that showed us that that service didn't work properly.","SAL: https://sal.toolforge.org/production?p=0&q=&d=2020-05-05q=synchronized&d=2012-01-01

'''All times in UTC.'''

* 07:36 Deployment starts (set with the incorrect option for which groups to deploy) (https://sal.toolforge.org/log/Sc7D43EBLkHzneNN4wiv)
* 07:44 Nodes start to fail '''OUTAGE BEGINS'''
* 07:50 Deployment is stopped and nodes are starting to be rolled back (https://sal.toolforge.org/log/K2HQ43EBj_Bg1xd3Reki)
* 07:54 All nodes are rolled back and restarted - '''OUTAGE ENDS'''

Keep in mind that not all nodes failed, so the outage was partial.","Two main things that happened during the deployment:
* the issue with the faulty version wasn't noticed before the deployment
The problem here is that the version of WDQS was faulty from the start - it wouldn't run properly after updating on any server. Unfortunately, we do not have CI/CD test servers that would have allowed us to spot the issue sooner. It seems that having automated deployments of each master version would be beneficial - we could get information if something fails much faster, e.g. by having a set of smoke tests executed there (we already have such a suite). It is worth it to point out that we already have a server (wdqs1009) that is being updated automatically, but only with the deploy repo version and we tend to deploy immediately after updating the version on that one. Version update with the master version would be more beneficial in scenarios like this one.
* tests were not run after canary deployment
During the deployment there is a step when the process halts - after deploying a canary (single server). The deployer should at this point run the manual tests to verify correctness - that didn't happen. That was caused by human error, but there is no reason why tests - which are normally executed by a person - couldn't be executed as a part of the deployment process.

After the outage, in turn, it was hard to precisely assess the impact. While graphs ([https://grafana.wikimedia.org/d/000000489/wikidata-query-service?orgId=1&var-cluster_name=wdqs&refresh=1m dashboard]) show error rates, they also count in errors that normally happen (like rejections because of throttling).","* Update documentation to describe manual testing procedure after canary deployment <mark>DONE</mark>
* Create automatically updated CI test environment ([[phab:T252503]])
* Include smoke tests as a part of deployment procedure ([[phab:T252504]])
* Improve outage visibility to see actual impact of an outage ([[phab:T252508]])",2020-05-05,Unknown,Monitoring,Wikidata,Unknown,Unknown,2020-05-05_wdqs-deploy.wikitext
"An external client issued a really large number of requests for what appears to be nonexistent original images uploaded to commons that caused the thumbor service to return 503s and eventually paged.

'''Impact''': 
Approximately 291,000 thumb-nailing/resizing for images (which is what the thumbor service is for) requests failed and were returned as 503s (an HTTP error code). 76,000 of those from the offending IP. The incident affected eqiad and esams, so predominantly Europe and East Coast of the American Continents.

{{TOC|align=right}}",,"We were paged by icinga, roughly 40 mins after the behavior started. The alert was the LVS one, about the entirety of the service. No alerts prior to that, despite the event having started 30+ minutes earlier.","'''All timelines are on 2020-05-11 and are UTC'''

We were paged by icinga, roughly 40 mins after the behavior started.

* 12:11 approximately: Thumbor is starting to return a large number of 404s and a little bit later of 503s. Peaks are at 75rps and 104 rps respectively. Incident begins. It will take another 36 minutes before it becomes critical
* 12:47: SRE gates paged. Multiple people respond
* ~12:50: It is noted in graphs that thumbor is serving a lot of 404s and 503s[1] to the caching proxies. Latencies have skyrocketed. From 1s to 12.5s for the p75[2] and from 5s to 15s[3] for the p98
* 12:54: A single IP gets noticed for having requested 20x the number of requests the 2nd in order has.
* 13:02: A rule is put in the caching proxies to block the aforementioned IP
* 13:08: It becomes apparent that block isn’t working.
* 13:13: The block is fixed and set correctly. However the block is for the IPv6 only (as the IPv4 one isn't known)
* 13:18: It becomes apparent that swift is setting Cache-control: no-cache when returning 404s which disallows caching, even for a short amount of time on the caching proxies, which would have worked as a back pressure mechanism
* 13:19: The offender falls back to their IPv4 address now that their IPv6 is banned.
* 13:23: Questions on why Thumbor’s rate-limit for originals isn’t kicking in
* 13:26: Blocks are updated with the IPv4 address. This time around the solution seems to hold
* 13:26: Incident ends
* 13:27: Question about the haproxy queue not having anything in it. The premise of the queue is to buffer requests when thumbor is under stress, which it did not do for some reason","The current per-IP throttling implemented in Thumbor is inadequate. It attempts to create a system where a given IP can only use X workers and have a queue of Y, but since it's based on PoolCounters requests beyond the X limit are actually served by other Thumbor workers. We end up with a situation where the client can keep a lot of workers busy… waiting on its own throttling lock. This wasn't the intention and as such the PoolCounter-based throttle is broken.","* Add a Cache-Control header to 404 responses coming from Thumbor/Swift Proxy if there isn't one yet https://phabricator.wikimedia.org/T252425
** Thumbor 2.8 upgrade solved this issue.

* Consider a very short term cache (5-10 min?) of 404’s for thumbnails, bearing in mind the possibility for cache pollution attacks https://phabricator.wikimedia.org/T259033

* Define who is in charge of basic Thumbor maintenance.

* Consider adding alerting for Thumbor query success rate, or for p50/p75 latency.  https://phabricator.wikimedia.org/T290034

* Lower poolcounter per-IP limits in Thumbor as much as possible while not breaking the Commons new uploads page too much. https://phabricator.wikimedia.org/T252426
** Changed: worker:4, maxqueue:500, timeout:8 to worker:4, maxqueue:50, timeout: 1",2020-05-11,Unknown,Unknown,Unknown,Unknown,Unknown,2020-05-11_thumbor.wikitext
"A change was made to the Memcached configuration of Wikimedia Commons. The change would distribute cache keys in a more optimal way ([[phab:T252564|T252564]]). It had the expected impact that there would be a temporarily increase in re-computations, and therefore it was applied to only one big wiki at a time. Each wiki has its own cache namespace, and there is a global namespace for shared cache keys. Global keys must be distributed in the same way across all wikis (regardless of the wikis' own cache configuration), to avoid a split-brain scenario.

It turned out that for many years, the cache keys relating to thumbnail metadata were wrongly marked as ""local"" instead of ""global"". Which meant that Commons' now used the new distribution, while Wikipedia used the old still, thus a split-brain scenario.

From first report to resolution took 4 hours. Total time the issue is presumed to have existed is 21 hours.

'''Impact''': About 10% of media uploads hit a race condition causing them to wrongly return a cached impression of the file not existing and thus unable to be inserted into articles. No data was lost. All affected files eventually became accessible either by themselves after a cache churn, or when our fix was deployed.

{{TOC|align=right}}",,"The first known report (retroactively speaking) was in the Commons Village Pump. The first time we were aware of it was through a user report on Phabiricator.

As far as we know, no alerts were fired at any time relating to this issue.

* Memcached was working fine from a service perspective. Usage levels and usage patterns were also normal.
* MediaWiki was operating fine.
* The frontends and health checks were all fine.

The one area were the issue may've been noticable to our monitoring is the HTTP traffic breakdown by status code.

When a user tries to acces a Commons file description page locally on a wiki, they would have gotten a 404 Not Found for the affected files. If this issue affected more than 10% of uploads, and if accessing such pages directly was commonly done by users for some reason, then it would have likely showed in the HTTP 40x respoonse monitoring. However, neither was the case.","'''All times in UTC.'''

Week of 11 May 2020

* Tue 2020-05-12 19:05 <hashar@deploy1001> synchronized wikiversions files: group0 wikis to 1.35.0-wmf.32
* Wed 2020-05-13 19:08 <hashar@deploy1001> synchronized wikiversions files: group1 wikis to 1.35.0-wmf.32
* Wed 2020-05-13 The train is blocked from deploying to group2 (Wikipedia) due to a database performance regression. ([[phab:T249964|T249964]])
* Fri 2020-05-15 A solution for the performance regression has been found, however no major deploys on Friday.

Week of 18 May 2020

* Mon 2020-05-18 The patch for the performance regression is deployed. Impact to be confirmed next week when the train is deployed to group2/Wikipedia.
* There is no train schedule this week due to (virtual) team offsites.
* Production continues to be on 1.35.0-'''wmf.31''' for Wikipedia, and the newer 1.35.0-'''wmf.32''' for non-Wikipedia. 

Fri 2020-05-22:

* 00:28 Configuration [[gerrit:597895|change 597895]] ""Enable coalesceKeys for non-global on commonswiki"" is deployed.
* 00:28 '''PROBLEM BEGINS'''
*…
* 16:39 A [[c:Commons:Village_pump/Archive/2020/05#Uploaded_images_are_not_showing_on_Wikipedias%2C_and_Wikidata|thread in the Village Pump]] on Commons reports that some recently uploaded images can't be seen from Wikipedia.
*18:08 Task [[phab:T253405|T253405]] was created from the VP thread.
*18:16 Task [[phab:T253408|T253408]] reports a similar issue (duplicate).
*19:00 CDanis looks into the issue but finds the earliest reported uploads seem to (now) work fine.
*19:11 CDanis confirmed more recent reports and finds them to be reproducible. He also notices there is no correlation with the software version these wikis run (it seems to affect both Wiktionary and Wikinews on the newer version, and Wikipedia on last-week's version).
*…
*20:02 RLazarus, CDanis and AaronSchulz are actively investigating.
*20:10 Looking a probable causes in the upload system.
*20:20 Looking a probable causes in the JobQueue, ChangeProp, chunked-uploading in UploadWizard.
*20:29 Aaron finds the root cause in <code>ForeignDBViaLBRepo::getSharedCacheKey</code> which is forging local cache keys on behalf of Commons, from the execution context of another wiki.
*21:19 Aaron uploads a code fix, [[gerrit:#/c/mediawiki/core/+/598118/|https://gerrit.wikimedia.org/r/598118]].
*21:23 Krinkle has reviewed the fix while Aaron is re-creating the code fix for wmf.31 and for wmf.32. This was non-trivial because larger cross-cutting refactors landed both between wmf.31 and wmf.32, and in master since. And also because despite having no deployments in over a week, production was mid-train.
*22:02 Aaron uploads code fixes [[gerrit:#/c/mediawiki/core/+/598122/|to wmf.31]] and [[gerrit:#/c/mediawiki/core/+/598120/|to wmf.32]].
*22:24 Krinkle deployed the fixes. '''PROBLEM ASSUMED SOLVED'''.","=== What went well? ===
* The Memcached configuration change was applied to Beta Cluster first. But (see below).","* Adopt a policy to by default prevent the train from pausing mid-way over a weekend. By Friday, either roll out or roll back. Weekend incident investigation should not have to deal with multi-version. Even if the train is not longer blocked and there simply wasn't time to roll out completely, either rollback or roll out anyway. – [[phab:T260401|T260401]]
* Determine stewardship for UploadWizard extension. – [[phab:T240281|T240281]]
* Determine stewardship for MW Core's upload API.  – [[phab:T240281|T240281]]
* Determine stewardship for MW Core's FileRepo backend.  – [[phab:T240281|T240281]]
* Strengthen FileRepo logic for cache keys to be less fragile and duplicated. – [[phab:T261534|T261534]]
* Fix FileRepo cache statistics to follow current conventions to avoid confusing our monitoring dashboards  – [[phab:T261534|T261534]]
* Consider having some amount of regular QA for uploading and multimedia in Beta and prod. – [[phab:T260402|T260402]]",2020-05-22,Unknown,Monitoring,Mediawiki,Unknown,Unknown,2020-05-22_thumbnails.wikitext
"s4 primary database master (db1138) had a hardware memory issue, mysqld process crashed and came back as read-only for 8 minutes.

'''Impact''': commonswiki didn't accept writes for 8 minutes. Reads remained unaffected

{{TOC|align=right}}",,,"'''All times in UTC.'''

* 01:33 First signs of issues on the DIMM are logged on the hosts's IDRAC
* 20:21 mysql process crashes '''OUTAGE BEGINS'''
* 20:24 First page arrives: PROBLEM - MariaDB read only s4 #page on db1138 is CRITICAL: CRIT: read_only: True, expected False: 
* 20:24 A bunch of SREs and a DBA start investigating and the problem is quickly found as a memory DIMM failure and mysql process crashed
* 20:28 <@marostegui> !log Decrease  innodb poolsize on s4 master and restart mysql
* 20:29 MySQL comes back and read_only is manually set to OFF
* 20:29 '''OUTAGE ENDS'''","This was a hard to avoid crash - hardware crash on a memory DIMM.

Masters start as read-only by default (to avoid letting more writes go through after a crash, until we are fully sure data and host are ok and still able to take the master role).

We did see traces of issues on the idrac's error log, if we could alert on those, maybe we could have performed a master failover before this host crashes. If this crash happens on a slave, the impact wouldn't have been as big, as slaves are read-only by default and MW would have depooled the host automatically.","* [DONE] Documentation on how to proceed if a master pages for read-only = ON: https://phabricator.wikimedia.org/T253832
* [DONE] Failover db1138 to its candidate master (scheduled for Friday 29th at 05:00 AM UTC): https://phabricator.wikimedia.org/T253808
* [DONE] Replace failed DIMM on that host: https://phabricator.wikimedia.org/T253808
* Alert on ECC warnings in SEL https://phabricator.wikimedia.org/T253810
* Create a script to move replicas between hosts when the master isn't available https://phabricator.wikimedia.org/T196366",2020-05-28,Unknown,Unknown,Unknown,Unknown,Unknown,2020-05-28_commons_read-only.wikitext
"A change was deployed to puppet which inadvertently deleted the private repo from all puppet backend servers and puppet standalone servers.  A few standalone servers in the Cloud environment maintain secrets by applying local commits to the labs/private repo.  This event caused all secrets to be deleted required manual restoration

'''Impact''': Any cloud environments which had added private secrets would have reverted to using the dummy secrets in the labs/private repo

{{TOC|align=right}}",,The issue was noticed by a member of the Cloud Services team.,"'''All timelines are on 2020-06-04 and are UTC'''
* 10:12: merge [[gerrit:601712|change]] to puppet-merge
* 10:12: '''OUTAGE Begins''' Once this change is merged the private repo will be removed the next time puppet is run (anytime between now and 30 mins)
* 10:32: [[gerrit:602317|change is reverted]] for unrelated reason, to fix a number of syntax errors
* 10:37: [[gerrit:602327|change redeployed]] with syntax errors fixed
* 10:58: jbond realises the private repo has been erroneously removed from stand-alone masters and [https://gerrit.wikimedia.org/r/c/operations/puppet/+/602341/2/modules/puppetmaster/manifests/gitclone.pp applies a fix]
* 10:58: jbond unaware real secrets where stored in some private repos did not realize the changes also affected puppet masters on WMCS projects leading to this incident
* 12:24: SAL <code>!log</code> did not work in #wikimedia-operations (worked at 12:18).
* 12:28: Arturo notices SAL does not work anymore (in #wikimedia-cloud)
* 12:32 <arturo> we don't have any [local] commit in labs/private in tools-puppetmaster-02
* 12:37: cloud engineers notice missing data in tools private repo and inquire about recent changes
* 12:42: confirmation that all private commits had been lost
* 12:55: explored option on using a temporary copy of the git repo created by <code>git-sync</code>, however that script deletes the temporary copy
* 12:58: start investigating if we can use <code>/var/log/puppet.log</code> to recover lost secrets
* 13:05: Ensure puppet is disabled on all cloud nodes
* 13:09: efforts made to use block level recovery to save data to an nfs mount
* 13:13: Bryan makes Antoine aware of the issue in #wikimedia-releng which would affect the CI and deployment-prep puppetmasters
* 13:18: Antoine backup private.git on integration/deployment-prep, disable puppet on them.
* 13:29: [[phab:T254473|confirmation]] that deployment-prep and integrations where uneffected due to a merge conflict causing puppet updates to fail
* 13:39: Start meet up call to discuss next steps (Bryan, Arturo, John, Antoine [just at the beginning])
* 13:40: add toolforge-k8s-prometheus private key
* 13:43: reset root@wmflabs.org password for Project-proxy-dns-manager
* 13:45: start collection a copy of /var/log/puppet.log from all servers using cloud cumin
* 14:00: Start producing results from puppet.log files
* 14:03: commit Elasticsearch users and passwords to tools
* 14:19: Add keepalived password to tools
* 14:36: add k8s/kubeadm encryption key
* 14:52: add toolsview mysql password
* 15:07: Add docker private information to tools.
* 15:25: add puppet and puppetdb related secrets
* 15:29: use scp to copy all puppet.log files locally and confirm we have all secrets
* 16:14: add private password for tools-dns-manager for acme-chief
* 16:16: secrets for the acme-chief tools account
* 16:39: failry confident all 'urgent' breakages are now resolved
* 16:40 (Voila) '''OUTAGE ENDS'''","=== What went well? ===
* Cloud services and SRE foundations worked well to resolve the issue",* https://phabricator.wikimedia.org/T254491,2020-06-04,Unknown,Unknown,Unknown,Unknown,Unknown,2020-06-04_cloud-private-repo.wikitext
"The sessionstore service suffered an outage that lead to the inability of logged-in users to submit edits. The root cause of the outage was insufficient capacity to respond to a sudden increase of requests reaching mediawiki.

<mark>TODO: These are raw numbers, to be used as input to a calculation of actual user impact that hasn't been performed yet. More to come here.</mark>

Sessionstore impact: About 32 million requests were lost between 18:36 and 19:20. Based on the pre-incident steady state of about 85% 404s from sessionstore, about 4.8 million requests were lost that ''would'' have returned with 2xx status. [https://grafana.wikimedia.org/d/000001590/sessionstore?orgId=1&from=1591900200000&to=1591903800000 (source)]. It's important to point out that 404s from sessionstore are from readers and hence are totally expected. <mark>TODO: Clarify why the error rate is normally so high, and whether it's the total or 2xx requests that reflect actual impact here.</mark>

MediaWiki-reported save failures: A total of 8,953 during the same period (all edit.failures.session_loss), which is unrealistically low, given that we believe ''all'' logged-in edits failed during the outage. [https://grafana.wikimedia.org/d/000000208/edit-count?panelId=13&fullscreen&orgId=1&from=1591900200000&to=1591903800000 (source)]

Deficit in MediaWiki-reported edits: The trough in successful edits between 18:36 and 19:00 can be seen in MediaWiki-reported stats, as well as an increase from 19:00 to about 20:30, as users—presumably both humans and bots—retried their edits that failed during the outage. During the outage, the deficit is about 24,000 edits; for the total window from 18:36 to 20:30, the deficit is about 18,000 edits (that is, about 6k edits were effectively delayed rather than dropped). [https://grafana.wikimedia.org/d/000000208/edit-count?panelId=8&fullscreen&orgId=1&from=1591887600000&to=1591912800000 (source)]

Editors were affected on all wikis in all geographic regions by being unable to edit, login or logout of the sites. Readers were completely unaffected.
{{TOC|align=right}}",,"Detection was automated, with the first IRC alert about five minutes after the kask pods initially crashed, and the first (and only) page about 30 seconds later. Icinga's full transcript from #wikimedia-operations is below, comprising 39 total alerts.

Note that the ""MediaWiki exceptions and fatals per minute"" alerts persisted, spuriously, for some hours after the underlying problem was solved. This was due to Logstash's delay in processing MediaWiki log entries: the alert reacts to the rate of log entry ''ingestion'', not log entry ''production'', so when Logstash is behind, the alert is behind too.

<pre class=""mw-collapsible mw-collapsed"">
18:41:46 <icinga-wm> PROBLEM - Prometheus jobs reduced availability on icinga1001 is CRITICAL: job={swagger_check_eventgate_analytics_cluster_eqiad,swagger_check_eventgate_analytics_external_cluster_eqiad,swagger_check_eventgate_main_cluster_eqiad,swagger_check_mathoid_cluster_eqiad,swagger_check_sessionstore_eqiad} site=eqiad https://wikitech.wikimedia.org/wiki/Prometheus%23Prometheus_job_unavailable https://grafana.wikimedia.org/d/NEJu05xZz/prometh
18:42:08 <icinga-wm> PROBLEM - PyBal backends health check on lvs1015 is CRITICAL: PYBAL CRITICAL - CRITICAL - sessionstore_8081: Servers kubernetes1001.eqiad.wmnet, kubernetes1003.eqiad.wmnet, kubernetes1005.eqiad.wmnet, kubernetes1006.eqiad.wmnet are marked down but pooled https://wikitech.wikimedia.org/wiki/PyBal
18:42:19 <icinga-wm> PROBLEM - LVS sessionstore eqiad port 8081/tcp - Session store- sessionstore.svc.eqiad.wmnet IPv4 #page on sessionstore.svc.eqiad.wmnet is CRITICAL: CRITICAL - Socket timeout after 10 seconds https://wikitech.wikimedia.org/wiki/LVS%23Diagnosing_problems
18:42:20 <icinga-wm> PROBLEM - Cxserver LVS eqiad on cxserver.svc.eqiad.wmnet is CRITICAL: /v1/mt/{from}/{to}{/provider} (Machine translate an HTML fragment using TestClient.) timed out before a response was received: / (root with wrong query param) timed out before a response was received: /v1/dictionary/{word}/{from}/{to}{/provider} (Fetch dictionary meaning without specifying a provider) timed out before a response was received: /v2/suggest/source
18:42:20 <icinga-wm> ggest a source title to use for translation) timed out before a response was received: /v1/list/pair/{from}/{to} (Get the tools between two language pairs) timed out before a response was received: /_info/name (retrieve service name) timed out before a response was received: /v1/page/{language}/{title}{/revision} (Fetch enwiki protected page) timed out before a response was received https://wikitech.wikimedia.org/wiki/CX
18:42:22 <icinga-wm> PROBLEM - restbase endpoints health on restbase1021 is CRITICAL: /en.wikipedia.org/v1/feed/featured/{yyyy}/{mm}/{dd} (Retrieve aggregated feed content for April 29, 2016) timed out before a response was received https://wikitech.wikimedia.org/wiki/Services/Monitoring/restbase
18:42:26 <icinga-wm> PROBLEM - restbase endpoints health on restbase1027 is CRITICAL: /en.wikipedia.org/v1/feed/featured/{yyyy}/{mm}/{dd} (Retrieve aggregated feed content for April 29, 2016) timed out before a response was received https://wikitech.wikimedia.org/wiki/Services/Monitoring/restbase
18:42:26 <icinga-wm> PROBLEM - restbase endpoints health on restbase-dev1005 is CRITICAL: /en.wikipedia.org/v1/feed/featured/{yyyy}/{mm}/{dd} (Retrieve aggregated feed content for April 29, 2016) timed out before a response was received https://wikitech.wikimedia.org/wiki/Services/Monitoring/restbase
18:42:28 <icinga-wm> PROBLEM - PyBal backends health check on lvs1016 is CRITICAL: PYBAL CRITICAL - CRITICAL - sessionstore_8081: Servers kubernetes1003.eqiad.wmnet, kubernetes1004.eqiad.wmnet, kubernetes1005.eqiad.wmnet are marked down but pooled https://wikitech.wikimedia.org/wiki/PyBal
18:42:28 <icinga-wm> PROBLEM - MediaWiki exceptions and fatals per minute on icinga1001 is CRITICAL: cluster=logstash job=statsd_exporter level=ERROR site=eqiad https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops
18:42:38 <icinga-wm> PROBLEM - LVS wikifeeds eqiad port 8889/tcp - A node webservice supporting featured wiki content feeds. termbox.svc.eqiad.wmnet IPv4 on wikifeeds.svc.eqiad.wmnet is CRITICAL: CRITICAL - Socket timeout after 10 seconds https://wikitech.wikimedia.org/wiki/LVS%23Diagnosing_problems
18:42:38 <icinga-wm> PROBLEM - Citoid LVS eqiad on citoid.svc.eqiad.wmnet is CRITICAL: /api (bad URL) timed out before a response was received: /api (Zotero and citoid alive) timed out before a response was received https://wikitech.wikimedia.org/wiki/Citoid
18:42:42 <icinga-wm> PROBLEM - LVS echostore eqiad port 8082/tcp - Echo store- echostore.svc.eqiad.wmnet IPv4 on echostore.svc.eqiad.wmnet is CRITICAL: CRITICAL - Socket timeout after 10 seconds https://wikitech.wikimedia.org/wiki/LVS%23Diagnosing_problems
18:42:44 <icinga-wm> PROBLEM - restbase endpoints health on restbase1018 is CRITICAL: /en.wikipedia.org/v1/feed/featured/{yyyy}/{mm}/{dd} (Retrieve aggregated feed content for April 29, 2016) timed out before a response was received https://wikitech.wikimedia.org/wiki/Services/Monitoring/restbase
18:42:54 <icinga-wm> PROBLEM - eventgate-main LVS eqiad on eventgate-main.svc.eqiad.wmnet is CRITICAL: / (root with no query params) timed out before a response was received: / (root with wrong query param) timed out before a response was received https://wikitech.wikimedia.org/wiki/Event_Platform/EventGate
18:42:56 <icinga-wm> PROBLEM - eventgate-logging-external LVS eqiad on eventgate-logging-external.svc.eqiad.wmnet is CRITICAL: / (root with no query params) timed out before a response was received: / (root with wrong query param) timed out before a response was received: /robots.txt (robots.txt check) timed out before a response was received https://wikitech.wikimedia.org/wiki/Event_Platform/EventGate
18:43:02 <icinga-wm> PROBLEM - restbase endpoints health on restbase-dev1004 is CRITICAL: /en.wikipedia.org/v1/feed/featured/{yyyy}/{mm}/{dd} (Retrieve aggregated feed content for April 29, 2016) timed out before a response was received https://wikitech.wikimedia.org/wiki/Services/Monitoring/restbase
18:43:14 <icinga-wm> PROBLEM - Host kubernetes1003 is DOWN: PING CRITICAL - Packet loss = 100%
18:43:18 <icinga-wm> PROBLEM - eventgate-analytics-external LVS eqiad on eventgate-analytics-external.svc.eqiad.wmnet is CRITICAL: WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None)) after connection broken by ConnectTimeoutError(urllib3.connection.VerifiedHTTPSConnection object at 0x7efce6308518, Connection to eventgate-analytics-external.svc.eqiad.wmnet timed out. (connect timeout=15)): /?spec https://wi
18:43:18 <icinga-wm> org/wiki/Event_Platform/EventGate
18:43:48 <icinga-wm> PROBLEM - Host kubernetes1005 is DOWN: PING CRITICAL - Packet loss = 100%
18:43:56 <icinga-wm> PROBLEM - Host kubernetes1001 is DOWN: PING CRITICAL - Packet loss = 100%
18:44:08 <icinga-wm> RECOVERY - restbase endpoints health on restbase1021 is OK: All endpoints are healthy https://wikitech.wikimedia.org/wiki/Services/Monitoring/restbase
18:44:12 <icinga-wm> RECOVERY - restbase endpoints health on restbase1027 is OK: All endpoints are healthy https://wikitech.wikimedia.org/wiki/Services/Monitoring/restbase
18:44:12 <icinga-wm> RECOVERY - restbase endpoints health on restbase-dev1005 is OK: All endpoints are healthy https://wikitech.wikimedia.org/wiki/Services/Monitoring/restbase
18:44:14 <icinga-wm> RECOVERY - Host kubernetes1003 is UP: PING WARNING - Packet loss = 33%, RTA = 47.03 ms
18:44:18 <icinga-wm> RECOVERY - LVS wikifeeds eqiad port 8889/tcp - A node webservice supporting featured wiki content feeds. termbox.svc.eqiad.wmnet IPv4 on wikifeeds.svc.eqiad.wmnet is OK: HTTP OK: HTTP/1.1 200 OK - 945 bytes in 0.003 second response time https://wikitech.wikimedia.org/wiki/LVS%23Diagnosing_problems
18:44:20 <icinga-wm> RECOVERY - Host kubernetes1005 is UP: PING OK - Packet loss = 0%, RTA = 0.19 ms
18:44:20 <icinga-wm> RECOVERY - LVS echostore eqiad port 8082/tcp - Echo store- echostore.svc.eqiad.wmnet IPv4 on echostore.svc.eqiad.wmnet is OK: HTTP OK: Status line output matched 200 - 258 bytes in 0.016 second response time https://wikitech.wikimedia.org/wiki/LVS%23Diagnosing_problems
18:44:22 <icinga-wm> RECOVERY - Citoid LVS eqiad on citoid.svc.eqiad.wmnet is OK: All endpoints are healthy https://wikitech.wikimedia.org/wiki/Citoid
18:44:28 <icinga-wm> RECOVERY - restbase endpoints health on restbase1018 is OK: All endpoints are healthy https://wikitech.wikimedia.org/wiki/Services/Monitoring/restbase
18:44:28 <icinga-wm> RECOVERY - eventgate-analytics-external LVS eqiad on eventgate-analytics-external.svc.eqiad.wmnet is OK: All endpoints are healthy https://wikitech.wikimedia.org/wiki/Event_Platform/EventGate
18:44:34 <icinga-wm> RECOVERY - Host kubernetes1001 is UP: PING OK - Packet loss = 0%, RTA = 0.18 ms
18:44:34 <icinga-wm> RECOVERY - eventgate-main LVS eqiad on eventgate-main.svc.eqiad.wmnet is OK: All endpoints are healthy https://wikitech.wikimedia.org/wiki/Event_Platform/EventGate
18:44:38 <icinga-wm> RECOVERY - eventgate-logging-external LVS eqiad on eventgate-logging-external.svc.eqiad.wmnet is OK: All endpoints are healthy https://wikitech.wikimedia.org/wiki/Event_Platform/EventGate
18:44:44 <icinga-wm> RECOVERY - restbase endpoints health on restbase-dev1004 is OK: All endpoints are healthy https://wikitech.wikimedia.org/wiki/Services/Monitoring/restbase
18:45:04 <icinga-wm> PROBLEM - Host kubernetes1003 is DOWN: PING CRITICAL - Packet loss = 100%
18:46:10 <icinga-wm> RECOVERY - Host kubernetes1003 is UP: PING OK - Packet loss = 0%, RTA = 54.98 ms
18:47:04 <icinga-wm> PROBLEM - MediaWiki edit session loss on graphite1004 is CRITICAL: CRITICAL: 60.00% of data above the critical threshold [50.0] https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/dashboard/db/edit-count?panelId=13&fullscreen&orgId=1
18:47:28 <icinga-wm> PROBLEM - k8s API server requests latencies on argon is CRITICAL: instance=10.64.32.133:6443 verb=LIST https://wikitech.wikimedia.org/wiki/Kubernetes https://grafana.wikimedia.org/dashboard/db/kubernetes-api
18:49:18 <icinga-wm> RECOVERY - k8s API server requests latencies on argon is OK: All metrics within thresholds. https://wikitech.wikimedia.org/wiki/Kubernetes https://grafana.wikimedia.org/dashboard/db/kubernetes-api
18:52:10 <icinga-wm> PROBLEM - Host kubernetes1003 is DOWN: PING CRITICAL - Packet loss = 100%
18:54:16 <icinga-wm> PROBLEM - Host kubernetes1001 is DOWN: PING CRITICAL - Packet loss = 100%
18:54:46 <icinga-wm> PROBLEM - Host kubernetes1005 is DOWN: PING CRITICAL - Packet loss = 100%
18:55:06 <icinga-wm> RECOVERY - Host kubernetes1003 is UP: PING OK - Packet loss = 0%, RTA = 0.25 ms
18:55:06 <icinga-wm> RECOVERY - Host kubernetes1001 is UP: PING OK - Packet loss = 0%, RTA = 0.23 ms
18:55:14 <icinga-wm> PROBLEM - Cxserver LVS eqiad on cxserver.svc.eqiad.wmnet is CRITICAL: /v2/suggest/source/{title}/{to} (Suggest a source title to use for translation) timed out before a response was received https://wikitech.wikimedia.org/wiki/CX
18:55:16 <icinga-wm> PROBLEM - Too many messages in kafka logging-eqiad on icinga1001 is CRITICAL: cluster=misc exported_cluster=logging-eqiad group={logstash,logstash-codfw,logstash7-codfw,logstash7-eqiad} instance=kafkamon1001:9501 job=burrow partition={0,1,2,3,4,5} site=eqiad topic=udp_localhost-err https://wikitech.wikimedia.org/wiki/Logstash%23Kafka_consumer_lag https://grafana.wikimedia.org/d/000000484/kafka-consumer-lag?from=now-3h&to=now&orgI
18:55:16 <icinga-wm> e=eqiad+prometheus/ops&var-cluster=logging-eqiad&var-topic=All&var-consumer_group=All
18:55:18 <icinga-wm> PROBLEM - MediaWiki exceptions and fatals per minute on icinga1001 is CRITICAL: cluster=logstash job=statsd_exporter level=ERROR site=eqiad https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops
18:55:20 <icinga-wm> RECOVERY - Host kubernetes1005 is UP: PING OK - Packet loss = 0%, RTA = 0.18 ms
18:58:34 <icinga-wm> PROBLEM - Host kubernetes1001 is DOWN: PING CRITICAL - Packet loss = 100%
18:58:38 <icinga-wm> PROBLEM - Host kubernetes1006 is DOWN: PING CRITICAL - Packet loss = 100%
18:58:48 <icinga-wm> RECOVERY - Cxserver LVS eqiad on cxserver.svc.eqiad.wmnet is OK: All endpoints are healthy https://wikitech.wikimedia.org/wiki/CX
18:59:02 <icinga-wm> RECOVERY - Host kubernetes1006 is UP: PING OK - Packet loss = 0%, RTA = 0.23 ms
18:59:04 <icinga-wm> RECOVERY - Host kubernetes1001 is UP: PING OK - Packet loss = 0%, RTA = 0.21 ms
19:00:48 <icinga-wm> RECOVERY - MediaWiki exceptions and fatals per minute on icinga1001 is OK: All metrics within thresholds. https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops
19:02:18 <icinga-wm> RECOVERY - PyBal backends health check on lvs1015 is OK: PYBAL OK - All pools are healthy https://wikitech.wikimedia.org/wiki/PyBal
19:02:21 <icinga-wm> RECOVERY - LVS sessionstore eqiad port 8081/tcp - Session store- sessionstore.svc.eqiad.wmnet IPv4 #page on sessionstore.svc.eqiad.wmnet is OK: HTTP OK: Status line output matched 200 - 258 bytes in 0.012 second response time https://wikitech.wikimedia.org/wiki/LVS%23Diagnosing_problems
19:02:36 <icinga-wm> RECOVERY - PyBal backends health check on lvs1016 is OK: PYBAL OK - All pools are healthy https://wikitech.wikimedia.org/wiki/PyBal
19:03:44 <icinga-wm> RECOVERY - Prometheus jobs reduced availability on icinga1001 is OK: All metrics within thresholds. https://wikitech.wikimedia.org/wiki/Prometheus%23Prometheus_job_unavailable https://grafana.wikimedia.org/d/NEJu05xZz/prometheus-targets
19:04:26 <icinga-wm> PROBLEM - MediaWiki exceptions and fatals per minute on icinga1001 is CRITICAL: cluster=logstash job=statsd_exporter level=ERROR site=eqiad https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops
19:06:02 <icinga-wm> PROBLEM - High average GET latency for mw requests on api_appserver in codfw on icinga1001 is CRITICAL: cluster=api_appserver code=200 handler=proxy:unix:/run/php/fpm-www.sock https://wikitech.wikimedia.org/wiki/Monitoring/Missing_notes_link https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=9&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=codfw+prometheus/ops&var-cluster=api_appserver&var-m
19:10:02 <icinga-wm> PROBLEM - High average GET latency for mw requests on appserver in codfw on icinga1001 is CRITICAL: cluster=appserver code=200 handler=proxy:unix:/run/php/fpm-www.sock https://wikitech.wikimedia.org/wiki/Monitoring/Missing_notes_link https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=9&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=codfw+prometheus/ops&var-cluster=appserver&var-method=GET
19:11:28 <icinga-wm> RECOVERY - High average GET latency for mw requests on api_appserver in codfw on icinga1001 is OK: All metrics within thresholds. https://wikitech.wikimedia.org/wiki/Monitoring/Missing_notes_link https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=9&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=codfw+prometheus/ops&var-cluster=api_appserver&var-method=GET
19:11:50 <icinga-wm> RECOVERY - High average GET latency for mw requests on appserver in codfw on icinga1001 is OK: All metrics within thresholds. https://wikitech.wikimedia.org/wiki/Monitoring/Missing_notes_link https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=9&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=codfw+prometheus/ops&var-cluster=appserver&var-method=GET
19:18:54 <icinga-wm> RECOVERY - MediaWiki exceptions and fatals per minute on icinga1001 is OK: All metrics within thresholds. https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops
19:23:22 <icinga-wm> RECOVERY - MediaWiki edit session loss on graphite1004 is OK: OK: Less than 30.00% above the threshold [10.0] https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/dashboard/db/edit-count?panelId=13&fullscreen&orgId=1
19:24:18 <icinga-wm> PROBLEM - MediaWiki exceptions and fatals per minute on icinga1001 is CRITICAL: cluster=logstash job=statsd_exporter level=ERROR site=eqiad https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops
19:42:26 <icinga-wm> RECOVERY - MediaWiki exceptions and fatals per minute on icinga1001 is OK: All metrics within thresholds. https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops
19:43:14 <icinga-wm> PROBLEM - Logstash rate of ingestion percent change compared to yesterday on icinga1001 is CRITICAL: 413.5 ge 210 https://phabricator.wikimedia.org/T202307 https://grafana.wikimedia.org/dashboard/db/logstash?orgId=1&panelId=2&fullscreen
19:46:02 <icinga-wm> PROBLEM - MediaWiki exceptions and fatals per minute on icinga1001 is CRITICAL: cluster=logstash job=statsd_exporter level=ERROR site=eqiad https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops
19:55:07 <icinga-wm> PROBLEM - Citoid LVS eqiad on citoid.svc.eqiad.wmnet is CRITICAL: /api (bad URL) timed out before a response was received: /api (Zotero and citoid alive) timed out before a response was received https://wikitech.wikimedia.org/wiki/Citoid
19:56:11 <icinga-wm> RECOVERY - Citoid LVS eqiad on citoid.svc.eqiad.wmnet is OK: All endpoints are healthy https://wikitech.wikimedia.org/wiki/Citoid
20:04:21 <icinga-wm> PROBLEM - MediaWiki exceptions and fatals per minute on icinga1001 is CRITICAL: cluster=logstash job=statsd_exporter level=ERROR site=eqiad https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-alerts?panelId=2&fullscreen&orgId=1&var-datasource=eqiad+prometheus/ops
</pre>

A lot of alerts that fired were unrelated to sessionstore. However the total volume (39) was manageable. The first alerts pinpointed the problematic service accurately.","Due to the outage, stashbot was intermittently unable to edit the SAL. Every !log command from #wikimedia-operations is included here for completeness.

All times in UTC.","=== What went well? ===

* Outage was root caused quickly
* We were able to switchover temporarily to codfw (and back to eqiad) within 5mins each.
* It was during a timeframe many SREs were able to respond.
* Automated monitoring detected and informed of the incident quickly.
* We already had very good graphs and statistics built in our infrastructure","* Increase Logstash ingestion capacity / handle logspam situations better https://phabricator.wikimedia.org/T255243
* Increase capacity of the sessionstore dedicated kubernetes nodes https://phabricator.wikimedia.org/T256236 {{done}}
* Increase kubernetes capacity overall https://phabricator.wikimedia.org/T252185 (codfw) and https://phabricator.wikimedia.org/T241850 (eqiad) {{done}}
* Investigate the iowait issues plaguing kubernetes nodes since 2020-05-29. https://phabricator.wikimedia.org/T255975 {{done}}
* Investigate the apparent network connectivity loss for kubernetes100[1,3,5] during the incident
* Investigate adding resource utilization alerts to services hosted on kubernetes
* Adopt SLIs/SLOs for sessionstore https://phabricator.wikimedia.org/T256629",2020-06-11,Unknown,Monitoring,Mediawiki,Unknown,Unknown,2020-06-11_sessionstore%2Bkubernetes.wikitext
"Logstash indexes were temporarily stale with newer messages from Kafka not yet available. This was caused by <mark>TODO</mark>.

'''Impact''': Monitoring alerts that are based on Logstash missed potential errors. Developers were briefly unable to see the latest messages in Kibana. They may've also been reduced availability in querying older messages from Logstash.

{{TOC|align=right}}",,"<mark>Write how the issue was first detected.  Was automated monitoring first to detect it? Or a human reporting an error?</mark>

<mark>Copy the relevant alerts that fired in this section.</mark>

<mark>Did the appropriate alert(s) fire? Was the alert volume manageable? Did they point to the problem with as much accuracy as possible?</mark>

<mark>TODO: If human only, an actionable should probably be to ""add alerting"".</mark>","<mark>Write a step by step outline of what happened to cause the incident, and how it was remedied.  Include the lead-up to the incident, and any epilogue.</mark>

<mark>Consider including a graphs of the error rate or other surrogate.</mark>

<mark>Link to a specific offset in SAL using the SAL tool: https://tools.wmflabs.org/sal/, For example: https://tools.wmflabs.org/sal/production?q=synchronized&d=2012-01-01</mark>

'''All times in UTC.'''

* 00:00 (TODO) '''OUTAGE BEGINS'''
* 00:04 (Something something)
* 00:06 (Voila) '''OUTAGE ENDS'''
* 00:15 (post-outage cleanup finished)

<!-- Reminder: No private information on this page! -->
<mark>TODO: Clearly indicate when the user-visible outage began and ended.</mark>",<mark>What weaknesses did we learn about and how can we address them?</mark>,"<mark>Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.</mark>

* <mark>To do #1 (TODO: Create task)</mark>
* Create spiceracks cookbooks for restarting Logstash's Elastic cluster (we have ones for Cirrus' Elastic cluster) – https://phabricator.wikimedia.org/T255864

<mark>TODO: Add the [https://phabricator.wikimedia.org/project/view/4758/ #Wikimedia-Incident-Prevention] Phabricator tag to these tasks.</mark>",2020-06-19,Unknown,Monitoring,Unknown,Unknown,Unknown,2020-06-19_Logstash.wikitext
"See also <https://lists.wikimedia.org/pipermail/wikitech-ambassadors/2020-June/002316.html>

Documentation:

* User reports: [[phab:T257522|T257522]], [[phab:T256395|T256395]], [[phab:T255156|T255156]]

* [[gerrit:plugins/gitiles/operations/puppet/+/d9e65609939dac79cb6ae411c4e8084b7f61a0ac/modules/varnish/templates/text-backend.inc.vcl.erb#72|text-backend.inc.vcl.erb#72]]",,,[https://docs.google.com/document/d/1VOV_jMvJd8sRHYLJ3l-OYa5PERT4RVWDFbWqNRWK71A/edit?usp=drive_web&ouid=111695442409187585233 Restricted document].,,* https://phabricator.wikimedia.org/T260098 Understand beresp.ttl=0 issues in Varnish,2020-06-25,Unknown,Unknown,Unknown,Unknown,Unknown,2020-06-25_caching-sessions.wikitext
https://phabricator.wikimedia.org/T257062 Lilypond seemingly bypassing security restrictions.,,,,,"* Audit all mismatched/unrecognised wmf-config settings. – https://phabricator.wikimedia.org/T257079
* Lint against unrecognised wmf-config settings in CI. – https://phabricator.wikimedia.org/T248866",2020-07-03,Unknown,Unknown,Unknown,Unknown,Unknown,2020-07-03_Lilypond_down.wikitext
"On Sunday 5th at 11:22UTC, the primary hard drive of cr3-eqsin (one of the two Singapore POP routers) crashed.
This caused the router to reboot into its second disk, containing only a factory default configuration. Everything failed over cleanly to the redundant router.

'''Impact''': We lost at max ~15000 requests/s in a 7min window (see screenshot, and [https://grafana.wikimedia.org/d/000000479/frontend-traffic?panelId=2&fullscreen&orgId=1&from=1593943200000&to=1593953999000&var-site=eqsin&var-cache_type=text&var-cache_type=upload&var-status_type=1&var-status_type=2&var-status_type=3&var-status_type=4 graph]).
[[File:Screenshot 2020-07-07 Frontend Traffic - Grafana.png|thumb|Impact of cr3-eqsin crash on eqsin traffic]]
{{TOC|align=right}}",,"* Was automated monitoring first to detect it? Yes
* Did the appropriate alert(s) fire? Yes
* PROBLEM - Host cr3-eqsin is DOWN: PING CRITICAL - Packet loss = 100% (paging alert)
* Was the alert volume manageable? Yes, only relevant alerts fired
* Did they point to the problem with as much accuracy as possible? Yes, the router went down, and only the router down paging alert triggered","'''All times in UTC.'''

* 11:22 PROBLEM - Host cr3-eqsin is DOWN: PING CRITICAL - Packet loss = 100% (paging) '''OUTAGE BEGINS'''
* 11:25 SREs reports of connectivity issues to eqsin (too brief to trigger alerting)
* 11:27 Routing is done converging, no more reports of connectivity issue '''OUTAGE ENDS'''
* 11:35 DNS patch ready to depool eqsin (just in case, unused) - https://gerrit.wikimedia.org/r/c/operations/dns/+/609571/

Monday 06
* ~07:40 Router is brought back up on its backup disk '''Redundancy restored'''","* This outage showed that our hardware redundancy and failover are solid
* Juniper recently introduced a new feature: <code>vmhost snapshot</code> that would have prevented the lack of redundancy (but not the crash itself)","* cr3-eqsin disk 1 failure - https://phabricator.wikimedia.org/T257154
* Investigate Junos vmhost snapshot - https://phabricator.wikimedia.org/T257153",2020-07-05,Unknown,Monitoring,Unknown,Unknown,Unknown,2020-07-05_eqsin-router-crash.wikitext
"With the introduction of a number of changes to make HTTPS redirects unconditional<ref>https://phabricator.wikimedia.org/T256095</ref>, the the MediaWiki API virtual hosts returned a HTTP 302 redirect to the HTTPS edge (e.g. external) address for every request that doesn't include the <code>X-Forwarded-Proto: https</code><ref>https://en.wikipedia.org/wiki/X-Forwarded-For</ref> header. For instance a request to <code><nowiki>http://api-ro.discovery.wmnet</nowiki></code>  would return a 302 to  <code>Location: <nowiki>https://www.wikidata.org/w/index.php</nowiki></code>

This made some [[Termbox]] and [[Wikifeeds]] requests run into timeouts, because traffic towards MediaWiki via our edge (egress) is not allowed from the Kubernetes Clusters.

<br />
'''Impact''': 

With a total of ~15 hours the outage was very long. During this period around 12% of requests to Termbox to failed with HTTP 500 (~186.000 requests). For Wikifeeds, only a specific endpoint was affected but for that more or less every request took longer than 30s and should be considered a failure. That's a total of around 1.250.000 requests lost.

Since we have 2 layers of caching in our edges, the actual user impact (e.g. the Wikipedia mobile app homepage not loading) was smaller.

{{TOC|align=right}}",,"High latency in restbase was recognized by an SRE but was not classified as a critical because the number of requests timing out was acceptable and no #pages had fired.

Hours later another SRE reacted to the Icinga alerts and warnings, and verified that this is a user facing issue (Wikipedia mobile app main page not loading).","'''All times in UTC.'''
[[File:20200714-termbox and wikifeeds timeouts restbase.png|thumb|restbase endpoint latency]]
[[File:20200714-termbox and wikifeeds timeouts termbox.png|thumb|termbox latency]]
[[File:20200714-termbox and wikifeeds timeouts wikifeeds.png|thumb|wikifeeds latency]]
*2020-07-13 19:33 unconditional HTTPS redirect deployed for group1 wikis ([https://sal.toolforge.org/log/sdCqSXMBv7KcG9M-vLkx SAL]) '''OUTAGE BEGINS'''
*2020-07-13 22:27 unconditional HTTPS redirect deployed for all wikis ([https://sal.toolforge.org/log/R_5KSnMBj_Bg1xd3neAS SAL])
*2020-07-14 00:49 Phab ticket [[phab:T257887|""restbase: ""featured"" endpoint times out]]"" was created
*06:03 restbase Icinga alerts where noticed by another SRE who verified there was an impact on the Wikipedia mobile app '''INVESTIGATION STARTED'''
*06:15 SRE figured the deployment of [[gerrit:c/operations/mediawiki-config/+/612396|I80ca62643f5c]] might have caused the issue
*06:29 Full scap to sync new train branch to all app servers started (unrelated to this incident)
*06:54 Scap canary check fails due to 302/200 HTTPS mis-match
*07:48 SRE reverted [[gerrit:c/operations/mediawiki-config/+/612396|I80ca62643f5c]]
*07:53 Revert led to lots of PyBal backend health check alerts (because it was configured to check for HTTP 302)
*07:55 wikifeeds latency back to normal '''WIKIFEEDS/MAIN OUTAGE ENDS'''
*08:00 SRE reverted [[gerrit:c/operations/puppet/+/612449|Ib8c5d71bb69a]] to have PyBal check for HTTP 200 again
*08:06 PyBal restarts done, alerts stopped
*08:13 Re-re-start full scap to push out wmf.41 and switch testwikis to it [[phab:T256669|T256669]]
*09:20 Termbox was identified to still have issues as just the changes to group1 wikis where not reverted
*09:30 SRE provided patches for the Kubernetes deployments of Wikifeeds and Termbox avoiding the redirect to the edge by talking HTTPS to the internal API vhosts directly. This needed including the internal puppet CA into the deployments.
*10:16 Updated Termbox deployment rolled out to all clusters
*11:18 Updated Wikifeeds deployment rolled out to all clusters
*11:25 All serviced back to normal (except for slightly higher latency because of TLS handshake) '''OUTAGE ENDS'''
*12:00 scap to sync new train branch continued
*12:35 SRE fixed scap canary checks and PyBack backend helth checks (in preparation to undo the revert)
*12:57 SRE undid the reverts of  unconditional HTTPS redirects","Even though there was a minor user impact, which was evident in our graphs, it went unnoticed for hours. If Wikifeeds and Termbox were more heavily used, it is possible we would have caught this earlier.",* [[phab:T258692|Monitoring/Alerting for Wikipedia mobile app errors]],2020-07-14,Unknown,High latency,Unknown,Unknown,Unknown,2020-07-14_termbox_and_wikifeeds_timeouts.wikitext
"Various Wikidata Query Service instances were down (failing readiness probe). Nodes that were down had errors in their blazegraph logs, and the problem would not spontaneously resolve without a restart of blazegraph.<!-- Reminder: No private information on this page! -->

'''Impact''': There was a period of a few minutes where there was a full outage; all WDQS queries failed to complete during this window. There was a more extended period of degraded service where a subset of queries would fail depending on which instance received the request.

{{TOC|align=right}}",,"Issue was first detected when a critical alert was issued and plumbed through to IRC:

<code>PROBLEM - PyBal backends health check on lvs2009 is CRITICAL: PYBAL CRITICAL - CRITICAL - wdqs-heavy-queries_8888: Servers wdqs2003.codfw.wmnet, wdqs2002.codfw.wmnet are marked down but pooled: wdqs-ssl_443: Servers wdqs2003.codfw.wmnet, wdqs2001.codfw.wmnet, wdqs2002.codfw.wmnet are marked down but pooled: wdqs_80: Servers wdqs2003.codfw.wmnet, wdqs2002.codfw.wmnet are marked down but pooled</code>

Initial critical alerts that fired:

<code>PyBal backends health check on lvs2009</code>

<code>PyBal IPVS diff check on lvs2010</code>

<code>PyBal backends health check on lvs2010</code>

Alert volume was manageable and fairly clearly indicated the problem (too many nodes were down in order to maintain full availability). Alert could perhaps explicitly show the connection between ""nodes are marked down but not pooled"" and ""the aforementioned is occurring because too many nodes are down"".","Various Wikidata Query Service instances went down (failing readiness probe). Nodes that were down had errors in their blazegraph logs, and the problem would not spontaneously resolve without intervention.

Behavior was observed to be consistent with one or more “bad queries” being repeatedly ran; each instance of a query that was too taxing would result in the associated instance going offline. This explains the fact that not all servers failed simultaneously, yet servers tended to go down rapidly one after the other. It also explains why issues were isolated to a given DC: initially, requests for the suspected “offender” would have routed to codfw, but following manual DNS depool, requests would have routed to eqiad, mirroring the timing of instances going offline.

Outages were successfully combated in the short-term by restarting blazegraph on affected instances. Full resolution was reached by developers and SREs working together to identify problematic queries (particularly those that generated errors in Varnish) and tracing them to the agent submitting them, while blazegraph was restarted as needed to maintain service availability.

There were a few minutes of full service outage, and a larger period of partial outage (a given query might have failed depending on which <code>(datacenter, node)</code> it was routed to)

---

SAL log messages start on [https://sal.toolforge.org/production?p=1&q=&d=2020-07-23 this page] and continue through to [https://sal.toolforge.org/production?p=0&q=&d=2020-07-23 here]

The timings of when a given public-facing WDQS instance was rendered inoperable can be seen in [https://grafana.wikimedia.org/d/000000489/wikidata-query-service?panelId=7&fullscreen&orgId=1&from=1595524221587&to=1595531775603&var-cluster_name=wdqs this graph] of RDF triple count (no data => node unable to process requests)

'''All times in UTC.'''

Note: Full outage occurred approximately between '''17:50''' and '''17:59'''

* 17:11 First instance of failed readiness probe occurs on single codfw instance '''wdqs2001'''
* 17:16 Automated de-pool of '''wdqs2002''' fails due to “too many down”
* 17:24 Critical alert leads to '''icinga-wm''' posting in '''#wikimedia-operations''' for codfw, by this point all 4 codfw wdqs nodes are unable to respond to requests
* 17:35 Decision is made to DNS depool codfw to prevent user impact
* 17:38 Noted that blazegraph is “stuck” on affected nodes
* 17:48 Manual '''restart of blazegraph''' on '''wdqs2001''' after pulling stack trace
* 17:49 Critical alert leads to '''icinga-wm''' posting in '''#wikimedia-operations''' for '''eqiad''', all 4 nodes down for wdqs-heavy-queries_8888 and wdqs-ssl_443, 3/4 servers down for wdqs_80
* 17:50 First confirmation of user impact, simple queries fail to work entirely
* 17:51 '''wdqs2001''' is operational following the manual restart of blazegraph
* 17:52 codfw is DNS ""repooled"" to allow the (now partially healthy) datacenter to accept requests
* 17:58 Single eqiad instance '''wdqs1003''' has blazegraph restarted to restore service for eqiad, remaining non-restarted eqiad and codfw nodes have their blazegraph restarted to restore full service
* 17:59 '''Outage resolved''' for first wave, queries working
[Following this point, there were a couple more waves of partial outages which were combatted with further blazegraph restarts while others worked to identify problematic queries and trace them to their origin]
* 18:44 Suspected IP address blocked
(No further partial outages occurred after this time)


<!-- Reminder: No private information on this page! -->","It's known that WDQS presents many unique scaling challenges, and that exposing a public endpoint comes with risks.

We have throttling in place. This state is maintained within each server rather than being shared; as a result, blazegraph restarts will clear any throttling behavior automatically put in place.

We don't think implementing shared state is a good idea because it won't fully solve the problem and makes the system more difficult to reason about (due to persisting state across restarts).

Ultimately this issue is more related to expectations of service availability / performance: while there are technical measures we can put in place to minimize ""blast radius"", '''there will always be potential for user behavior to topple the system''', thus communication should center around the level of availability of this service. As such one of our action items is to define clear SLOs and an error budget for Wikidata Query Service. The amount of effort required to support the required level of availability needs to be assessed.","* [https://phabricator.wikimedia.org/T258739 T258739] wdqs admins should have access to nginx logs, jstack on wdqs machines
* [https://phabricator.wikimedia.org/T258754 T258754] Define SLOs, error budget for WDQS

Note: We discussed submitting an upstream bug report for [https://phabricator.wikimedia.org/T242453 T242453], but relevant context here is that Blazegraph was acquired by Amazon. As a result, while our e-mails are still responded to, we don't expect there to be engineering resources available to help troubleshoot a concurrency/deadlock issue, as Blazegraph is no longer under active development. As such we have not submitted an upstream bug report because we don't expect the time investment of a proper bug report to be fruitful.",2020-07-23,Unknown,Unknown,Unknown,Unknown,Unknown,2020-07-23_wdqs-outage.wikitext
Restricted. Details at https://phabricator.wikimedia.org/T260329.,,,,,"* https://phabricator.wikimedia.org/T260329
* https://phabricator.wikimedia.org/T260281",2020-08-12,Unknown,Unknown,Unknown,Unknown,Unknown,2020-08-12_appservers_oom.wikitext
"Users of Jio ISP (India, AS 55836) unable to reach Wikimedia sites.

Caused by a misconfiguration in Jio's own RPKI records, which can only be fixed on their end.  Their network engineers are aware and working on the issue.

As temporary workaround, Wikimedia changed its network advertisements to accept invalid RPKI to let Jio costumers reach our sites again.",,,,,"* https://phabricator.wikimedia.org/T260449
* https://phabricator.wikimedia.org/T260452",2020-08-14,Unknown,Unknown,Unknown,Unknown,Unknown,2020-08-14_isp-unreachable.wikitext
"<mark>What happened, in one paragraph or less. Avoid assuming deep knowledge of the systems here, and try to differentiate between proximate causes and root causes.</mark><!-- Reminder: No private information on this page! -->

'''Impact''': <mark>Who was affected and how? In one paragraph or less.  For user-facing outages, estimate: How many queries were lost? How many users were affected, or which populations (editors? readers? particular geographies?), etc.  Do not assume the reader knows what your service is or who uses it.</mark>

* <mark>Impact statement for read-only period</mark>
* <mark>Impact statement for Kartotherian</mark>
* <mark>Impact statement for search</mark>
* <mark>Impact statement for increased save times</mark>

{{TOC|align=right}}",,"<mark>Write how the issue was first detected.  Was automated monitoring first to detect it? Or a human reporting an error?</mark>

<mark>Copy the relevant alerts that fired in this section.</mark>

<mark>Did the appropriate alert(s) fire? Was the alert volume manageable? Did they point to the problem with as much accuracy as possible?</mark>

<mark>TODO: If human only, an actionable should probably be to ""add alerting"".</mark>",'''All times in UTC.''',"=== What went well? ===

* Our automation conducted the switchover cleanly, with no need for manual intervention.
* Dry-run and live-test modes enabled us to safely exercise almost every line of the scripts ahead of time, identifying sections that had gotten stale since the last test, and correct them before the scheduled exercise.
* Because each section of the cookbook was idempotent, we could safely and confidently rerun when a check failed due to a transient error.
* The critical period of the switchover—i.e. the interval between setting the wikis read-only and setting them read-write again—is now comprised of a drastically shorter set of steps and no Puppet run, due to improvements and simplifications in our architecture.","* Thumbor has dnsdisc conftool records, but they don’t appear to be used:
*: rzl@cumin1001:~$ confctl --quiet --object-type discovery select dnsdisc=thumbor get
*: {""codfw"": {""pooled"": false, ""references"": [], ""ttl"": 300}, ""tags"": ""dnsdisc=thumbor""}
*: {""eqiad"": {""pooled"": true, ""references"": [], ""ttl"": 300}, ""tags"": ""dnsdisc=thumbor""}
*: They may be a relic from when swift wasn’t active/active; we should be able to clean them up.
* [nitpick] Message “Failed to call …00-reduce-ttl-sleep” seems slightly misleading; it got called but didn’t succeed/failed?
* False-alerts for “Time elapsed since the last kafka event processed by purged on host is CRITICAL: … topic=eqiad.resource-purge” (fixed) and an unknown for “Elevated latency for eventgate-logging-external eqiad”
* Kartotherian in codfw could not cope with the load, eqiad needed repooling, solution may be more hardware (on the way)
* Remove swift from EXCLUDED_SERVICES in sre.switchdc.services and add it to the default list, as we use that DNS record after all. Addressed in {{Gerrit|626403}}
* Consider a puppet-merge lock/warning, global host MOTD, or other last-minute production-wide lockout/tagout
* Future “attach to tmux” commands should enforce a minimum terminal size requirement
* Consider re-adding the 5m sleep after reducing TTL, since the warmup script now takes less than five minutes to complete
* Consider automatically adding Grafana annotation(s) for critical operations from the cookbook(s)
* 08-update-tendril:<marostegui> parsercache (pc1,p2 and pc3) should have been changed too, but they were not, that's not important
* WRT Elasticsearch & /related/ page feature issues: dcausse@ says that, in the 2018 switchover, they took an existing capacity cache for elasticsearch and made it replicated cross-DC, because otherwise elasticsearch falls over without it.  However, the hit rate for such plummeted during this switchover [https://grafana.wikimedia.org/d/000000455/elasticsearch-percentiles?viewPanel=55&orgId=1&from=1598967571315&to=1598970596798&var-cirrus_group=codfw&var-cluster=elasticsearch&var-exported_cluster=production-search&var-smoothing=1 (graph)]
* Changeprop uses consistent connections that need to be killed during the switchover.  Adding a step to restart the Envoy TLS terminators on the old-DC jobrunners would be sufficient to force a reconnect.
* Confd on mwmaint1002 needed to be restarted manually, worth checking why
** [Riccardo] Additional hosts had confd stuck, all for the same reason AFAICT, the expired certificate. I bet is due to the Puppet CA rotation we did a while ago, the one host I checked was having confd not restated for 10 months.
** [Riccardo] I suggest to add a step to restart all confd in the preparation phase.        
* Dewiki had an extra maintenance message telling that we would add a new DC (below the generic maintenance message) <mark>TODO: include screenshot from doc</mark>
* There was a lot of confusion and alerts spam from primary db read only status. While a better setup (icinga allowing) or downtiming should had happened, a worse underlying issue was detected- confctl was not giving current results on puppetmaster1002, which was used primarily to configure alerts and its urgency on puppet (icinga). Task: [https://phabricator.wikimedia.org/T261767 T261767]
* might be useful in the future for that kind of purpose to not move all services (https://sal.toolforge.org/log/PHHkRHQBLkHzneNNOEPw ) at once. even doing them 1 minute apart would have helped here (5min apart would be nice)
* [https://phabricator.wikimedia.org/T261763 T261763] save timing increase
* We’d still like to get rid of maintenance cronjobs, so that we can eliminate the 01-stop-maintenance and 08-start-maintenance steps (the latter includes a Puppet run). The only cron left is the WDQS dispatcher; everything else has been migrated to mediawiki::periodic_job, which reads the active DC from etcd.
* DB CPU saturation dashboard exists for eqiad but not codfw. Task: [https://phabricator.wikimedia.org/T261868 T261868]
* Understand this log entry from 04-switch-mediawiki: ''2020-09-01 14:03:43,668 rzl 28797 [INFO] Sleeping 23.459 seconds to reach the 10 seconds mark''
* In 07-set-readwrite, if the siteinfo fetch times out, consider retrying on a different appserver, instead of failing immediately.
* Improvements for mwmaint switchover crons vs noc.wm webserver (Daniel) [https://phabricator.wikimedia.org/T265936 T265936]
<mark>TODO: Link to or create a Phabricator task for every step. Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] Phabricator tag to these tasks.</mark>",2020-09-01,Unknown,Monitoring,Unknown,Unknown,Unknown,2020-09-01_data-center-switchover.wikitext
"Active Wikidata Query Service instances were down (failing readiness probe). Blazegraph ""locked up"" on affected instances, rendering it unable to complete any work. As a result, queries would time out after exceeding nginx's timeout window without receiving a response from Blazegraph.

'''Impact''': The period of intermittent service disruption occurred between 20:34 and 22:27 UTC, of which about 75% of the time was spent in a state of total outage.

{{TOC|align=right}}",,"Issue was first detected when a critical alert was issued and plumbed through to IRC:

<code>[2020-09-02 20:42:31] <icinga-wm> PROBLEM - PyBal backends health check on lvs2009 is CRITICAL: PYBAL CRITICAL - CRITICAL - wdqs-heavy-queries_8888: Servers wdqs2003.codfw.wmnet, wdqs2007.codfw.wmnet, wdqs2002.codfw.wmnet are marked down but pooled: wdqs-ssl_443: Servers wdqs2003.codfw.wmnet, wdqs2007.codfw.wmnet, wdqs2002.codfw.wmnet are marked down but pooled: wdqs_80: Servers wdqs2003.codfw.wmnet, wdqs2007.codfw.wmnet, wdqs2002.codfw.wmnet are marked down but 
</code>

Alert volume was manageable and fairly clearly indicated the problem (too many nodes were down in order to maintain full availability). Alert could perhaps explicitly show the connection between ""nodes are marked down but not pooled"" and ""the aforementioned is occurring because too many nodes are down"".","Various Wikidata Query Service instances went down (failing readiness probe). Nodes that were down had errors in their blazegraph logs, and the problem would not spontaneously resolve without intervention.

Behavior was observed to be consistent with one or more “bad queries” triggering deadlocks in Blazegraph, leading to timeouts and therefore service unavailability.

The period of intermittent service disruption occurred between 20:34 and 22:27, of which about 75% of the time was spent in a state of total outage.

---

All relevant actions taken are logged in SAL [https://sal.toolforge.org/production?d=2020-09-02 here] (see topmost part of page)

The timings of when a given public-facing WDQS instance was rendered inoperable can be seen in [https://grafana.wikimedia.org/d/000000489/wikidata-query-service?viewPanel=7&orgId=1&var-cluster_name=wdqs&from=1599078710070&to=1599086198485 this graph] of RDF triple count (no data => node unable to process requests). Note wdqs1* was the passive (backup) datacenter and not being used, thus the outage corresponds to the state of the 4 wdqs2* nodes.

'''All times in UTC.'''

* 20:34 First instance has its Blazegraph rendered unresponsive (per Grafana)
* 20:42 First critical alert fires and is plumbed through to #wikimedia-operations (note that more than one instance must go down before this alert fires)
* 20:46 Blazegraph restarted across all affected nodes. This partially restores service for about 3 minutes before all nodes are unresponsive again
* 21:01 Nginx is restarted on a single instance to test if the problem is isolated to blazegraph. The restart doesn't help, so the problem is confirmed to be blazegraph.
* 21:09 It's observed that blazegraph logs contain entries indicating blazegraph is receiving malformed queries
* 21:10 Blazegraph restarted across all affected nodes. This partially restores service for about 8 minutes before all nodes are unresponsive again
(During this time we worked to identify which agents were responsible for sending malformed queries)
* 22:08 Suspected user agent responsible for malformed queries has been identified
(During this time we worked to iteratively test out modified nginx configurations to block the previously identified user agents)
* 22:26 New nginx configuration has been applied via puppet to the affected nodes, blocking the suspected user agents. nginx is automatically restarted by our puppet automation
* 22:27 Blazegraph is restarted across all affected servers, restoring full service
* 22:28 '''Monitoring system indicates full recovery of service availability'''


<!-- Reminder: No private information on this page! -->","See [https://wikitech.wikimedia.org/wiki/Incident_documentation/20200723-wdqs-outage#Conclusions previous incident's conclusions] for some general context on our service expectations.

Note that compared to the last incident, we identified the problematic queries as being specifically malformed queries (leading to MalformedQueryExceptions getting raised). In the previous incident, it was non-performant but not malformed queries that appeared to cause the deadlock.","* (Carried over from last incident) {{phab|T258754|}} Define SLOs, error budget for WDQS
* {{phab|T262002}} Improve visibility into blazegraph queries causing errors
* {{phab|T262009}} Add an entry in the WDQS Runbook on killer queries",2020-09-02,Unknown,Unknown,Unknown,Unknown,Unknown,2020-09-02_wdqs-outage.wikitext
"A stored XSS security issue was reported, affecting talk pages in MobileFrontend. It was patched in production the same day.

The issue and patch were publicly disclosed seven days later on 14 September, at [[phab:T262213|T262213]].",,,,,(None),2020-09-07,Unknown,Unknown,Unknown,Unknown,Unknown,2020-09-07_mobilefrontend-sec.wikitext
"A power cable issue in asw2-d3-eqiad during a scheduled PDU upgrade, caused hosts in racks D1, 3, 4 to be unavailable (13:34 UTC). It rebooted into a different firmware image than the rest of the switches and had to be upgraded; once this happened, all hosts were again available (15:02 UTC).

'''User impact''': Wikifeeds was not working properly; Horizon unavailability meant that creation and updating of WMCS images was broken; dns1002 was flapping, causing sporadic dns lookup failures for eqiad hosts; logins failed to stat1005, stat1006.

'''Other impact''': kafka-jumbo1006 being unavailable meant that some data was lost for webrequest and EventLogging topics; some bacula full backups had errors and failed to run and the director had to be restarted; ferm was broken on about 30 hosts that were running puppet when DNS connectivity was lost/resolution failed and had to be manually restarted
<!-- Reminder: No private information on this page! -->",,,,,"* Create icinga hostgroup per rack row / Can we make row/rack visible in icinga tactical overview of hosts down somehow? (Akosiaris)
* Investigate why asw-d3-eqiad rebooted into a different boot partition with different JunOS version, causing it to be “inactive” in the VCF - and prevent this from happening again - https://phabricator.wikimedia.org/T262290
** Run “show system snapshot media internal ” and “request system snapshot slice alternate” on all EX switches
* Wikifeeds (and as a consequence, its endpoint for restbase) basically went down until the network issues in eqiad were resolved. Figure out what the hidden dependency is there. (Akosiaris)
* Investigate why routing around a missing switch member D3 did not work, hosts in D1, D2, D4 (at least) were also impacted - https://phabricator.wikimedia.org/T256112
* Check Anycast configuration as it seemed the flapping connections caused a few issues with dns1002 - https://phabricator.wikimedia.org/T262372
* Rsyslog deliveries to cengit trallog must fail independently - https://phabricator.wikimedia.org/T226703",2020-09-08,Unknown,Unknown,Unknown,Unknown,Unknown,2020-09-08_Rack_D_hosts_outage.wikitext
"While reconfiguring all services to use our service proxy middleware to make remote procedure calls, a faulty configuration was deployed by [[User:Giuseppe Lavagetto|yours truly]] for mobileapps at 08:40. This caused mobileapps to create mobile-html content with broken css and js links for pages regenerated during the day.

The issue was reported at 16:40 and the issue was quickly reverted. Then we needed a few hours to actually clear all the caching layers (RESTBase, edge caches). All pages affected were purged by 20:20.",,,,,"* https://phabricator.wikimedia.org/T268484 Tracking task.
* https://phabricator.wikimedia.org/T262437 The bug caused by this incident (Page content service is deployed with localhost links).
* https://phabricator.wikimedia.org/T264340 Create test for api/rest_v1/page/mobile-html

* The biggest actionable is of course to always wait for validation from service owners before merging a patch - and the whole outage would've been avoided if that was done. Anything else listed here is purely a second-order actionable.
* While this deployment was the result of bad judgement, SRE need to be able to deploy a configuration change with confidence. The fact that the mobileapps spec tests all passed in staging lulled SRE into a false sense of security. The OpenAPI spec should be extended to include a test for the aforementioned URLs. '''(TODO: create task)'''
* We need staging to become a functional environment where we can test more than just a swagger spec test. Maybe linking it to restbase-dev, and making it possible to compare results of urls with production would help '''(TODO: create task)'''",2020-09-09,Unknown,Unknown,Unknown,Unknown,Unknown,2020-09-09_mobileapps_config_change.wikitext
"A number of s5 database replicas in the  Eqiad and Codfw data centres alerted about replication being stopped due to a duplicate key. This lead to watchlist and recentchanges delays.

'''Impact''': Editors of wikis hosted in the s5 section (see [https://noc.wikimedia.org/conf/highlight.php?file=dblists/s5.dblist s5.dblist]) saw stale data in recentchanges and watchlist interfaces. 
{{TOC|align=right}}",,"Automated monitoring caught the issue, SRE was paged about stopped replication in a number of s5 replicas. The alerts were appropriate, sufficient and to the point. People responded and escalated to more equipped ones to handle the incident.","'''All times below are in UTC and referring to 2020-09-25'''

* 11:42: pages for s5 replication for db2137, db2139, db2089, db2099
* 11:45 icinga is reporting about a duplicate key in enwikivoyage's ipblocks table
* 11:51: sobanski pings jcrespo
'''* 11:55 Incident opened. Alexandros Kosiaris becomes IC'''
* 11:58 jcrespo joins
* 12:13 jcrespo deletes duplicate ipb_id=16326 on db2089 and starts replication manually
* 12:16: db2089 lag recovers
* 12:17: jcrespo applies same fix to all hosts
* 12:22: All hosts lag recover
* 12:23: It is decided that all eqiad hosts exhibiting the issue, will not be touched to allow use of them as guinea pigs U during the research phase of the root case
'''* 12:25: Incident ends.'''
* 15:23 jynus: fixing enwikivoyage ipblocks inconsistency cluster-wide T263842 (to prevent the issue again on the same table). * All hosts back to normal (including eqiad). https://sal.toolforge.org/log/MFncxXQBgTbpqNOm_cm1","There are two possible explanations for this crash:

1) The drift was there and we inserted a row that touched it

2) The drift was generated somehow from MW.

The first option is hard to see, as that table was recently checked before the switchover and came up clean (https://phabricator.wikimedia.org/T260042)
There is also the fact that the row that failed had a timestamp from around the time of the crash.
The sequence of events within the transaction that failed is interesting and it definitely didn't help that we are using INSERT IGNORE here. This is a summary from the binlog transaction for the broken entry and with the affected ipb_address_unique UNIQUE:

 'REDACTED',ipb_user = 0,ipb_auto = 0

This contains the first insert that went thru (it maybe failed, but as it has the IGNORE...) with the timestamp: 20200925113933 :

 BEGIN
 INSERT /* MediaWiki\Block\DatabaseBlockStore::insertBlock  */ IGNORE INTO `ipblocks`
Then an UPDATE on that same row that broke on some replicas, with a duplicate entry, it has this timestamp: 20200925110538.

 UPDATE /* MediaWiki\Block\DatabaseBlockStore::updateBlock  */  `ipblocks` SET ipb_address = 'REDACTED',ipb_user = 0,ipb_timestamp = '20200925110538',ipb_auto = 0
That is the row that was already existing on some hosts:

 root@db2089.codfw.wmnet[enwikivoyage]> SELECT * FROM ipblocks where ipb_address = 'xxxx'\G
 *************************** 1. row ***************************
          	ipb_id: xxxx
     	ipb_address: xxx
        	ipb_user: 0
    	ipb_by_actor: xxxx
   	ipb_reason_id: xxxx
   	ipb_timestamp: 20200925110538
 The values for ipb_address_unique on the insert were:
 62.XXX.2X,0,0

And the values for that same UPDATE for that same key were exactly the same, with just the modification of the timestamp from 20200925113933 to 20200925110538.
What doesn't make sense to me is that there was an existing row with 20200925110538 timestamp (even if the timestamp isn't part of the UNIQUE).

Looking for that that same IP on the existing binlogs on an affected replica (binlogs from today till 20th Sept) and there's no trace of that IP being added there. Same on the Eqiad master, which has binlogs from 29th Aug till today, no trace of that IP being added to ipblocks, before the sequence of events. The timestamps of the first entry on eqiad and codfw master are the same.

Option 2 for this crash would imply that MW somehow introduced that inconsistency with an unsafe statement like INSERT IGNORE, however, it is difficult to know why only a few hosts failed.","* Investigate the deeper causes of the incident. [https://phabricator.wikimedia.org/T263842 T263842]. Deeper causes still unknown. {{done}}
* Automatic and continuous data checks (at least for the most important/big tables) [https://phabricator.wikimedia.org/T207253 T207253]
* Re-evaluate the use of ''INSERT IGNORE'' on ''ipblocks'' [https://phabricator.wikimedia.org/T264701 T264701]",2020-09-25,Unknown,Monitoring,Unknown,Unknown,Unknown,2020-09-25_s5_replication_lag.wikitext
"Sensitive credentials (authentication tokens) were leaked via publicly accessible, memcached servers, on a default port & without authentication.
Tokens can be used to escalate privileges and (at least) “bump quotas, create and destroy” arbitrary VMs in Cloud VPS/Toolforge. While the service runs in the “production” realm, no escalation paths to other production services and hosts are known so far.

These tokens are for access to the Keystone API, which is firewalled to the Cloud VPS and production networks, so they could not be used directly from the internet easily without changing signed cookies in Django/Horizon web UI somehow (which should require the secret key). Therefore, this exposure would theoretically benefit a malicious cloud user to the extent that they could manipulate the Keystone API from inside cloud according to the permissions of the particular captured token.

Access was blocked in eqiad at 02:21 UTC and in the development environment in codfw at 08:43 UTC.
Keystone auth tokens were manually rotated (the generally have a lifetime of 24 hours)
Investigation of potential privilege escalation within Cloud VPS (via leaked Keystone tokens) ongoing, additional mail to be sent to cloud users (see below).",,,,,"* Better understand and refine port-scanning tools in order to detect future vulnerabilities like this one. The daily network scan is in the process of being fixed to scan all ports (instead of a selection of 2000) and when that has happened we need to do a one off investigation of the findings to establish a base point so that all further runs can be studied as a diff against the base check.
* Defense-in-depth measures for cloudcontrol services. Per OpenStack recommendations, “[f]or production deployments, we recommend enabling a combination of firewalling, authentication, and encryption to secure it.”. The first of the three failed here, but it was all three that were missing and allowed this vulnerability.
* More fine-grained access control for the ferm ranges in the profile::memcached::instance class to allow to restrict access for cloudcontrol* further.
* Extend the daily network port scan to cover 64k ports instead of 2000 well-known, this requires some changes to the packet filters running on on our servers https://phabricator.wikimedia.org/T264888
* Meet with security to discuss threat model for cloud services",2020-10-06,Unknown,Unknown,Unknown,Unknown,Unknown,2020-10-06_cloud-vps.wikitext
"The [https://fr.wikipedia.org/wiki/Wikip%C3%A9dia:Accueil_principal main page of the French Wikipedia] was loading much slower than usual for some time on October 30th 2020. The page did load but took several seconds and up to 20 seconds to finish.

The incident lasted roughly 7 hours and the group of affected users were all logged in users using the main page of French Wikipedia during that time. Logged out users / general readers were served cached results and did not notice the incident. We can roughly estimate the number of affected users to about ~ 625, based on about 2500 unique editors per day.

The ones who did notice started reporting on the [https://fr.wikipedia.org/wiki/Wikip%C3%A9dia:Le_Bistro#Temps_de_chargement_de_la_page_d'accueil French Wikipedia Bistro page] and a [https://phabricator.wikimedia.org/T266865 ticket on phabricator] was created.

The FeaturedFeeds extension adds Atom feed <code><link></code> tags to the head of main pages, but internally it generates the entire feed, just to add the tags. Normally it reads from the cache so this wasn't noticed until the feed became too big to fit in memcached (<code>ITEM_TOO_BIG</code> error) and was being generated/parsed on every main page load, including <code>action=history</code> or <code>action=edit</code>.

The issue was resolved after the '''[https://noc.wikimedia.org/conf/highlight.php?file=FeaturedFeedsWMF.php wmgFeaturedFeedsOverrides]''' limit used by the [https://www.mediawiki.org/wiki/Extension:FeaturedFeeds FeaturedFeeds extension]was [https://gerrit.wikimedia.org/r/c/operations/mediawiki-config/+/637752 reduced from 60 items per feed to the default of 10 items per feed] that is used by other wikis.

This did not change rendering of the main page; the only lasting effect will be that users of [https://fr.planet.wikimedia.org fr.planet.wikimedia.org], the RSS feed aggregator for Wikimedia-related feeds, will see only 10 instead of 60 items from the fr.wikipedia featured feed.

The feed config and special case for frwiki had been added as part of {{phab|T167617}} back in 2017.

An additional mini report can be found in [[phab:T266865#6592372|T266865#6592372]].",,,"* 12:22 UTC https://phabricator.wikimedia.org/T266865 is opened
* ~ 17:40 UTC SRE starts investigating
* 17:51 SRE confirms issue on ticket and notes it is not limited to esams but affects all data centers but only the French main page and not other pages
* 18:22 UTC SRE finds “Memcached error for key ""WANCache:frwiki:featured-feeds:1:fr|#|v"" on server ""127.0.0.1:11213"": ITEM TOO BIG”
* 18:27 UTC SRE reaches out to perf-team
* 18:35 UTC SRE finds that MW config has wmgFeaturedFeedsOverrides with a limit set to “60” for frwiki while other wikis use defaults
* 18:36 UTC volunteer suggests disabling featuredfeeds extension and testing on mwdebug1001 and does so. Confirms load time is much faster without it.
* 18:39 UTC SRE uploads config change to [https://gerrit.wikimedia.org/r/c/operations/mediawiki-config/+/637752 remove the custom limit of 60 items per feed].
* 18:50 Deployment of said patch
* 18:56 French WP user confirms issue looks resolved. (https://fr.wikipedia.org/wiki/Wikip%C3%A9dia:Le_Bistro#Temps_de_chargement_de_la_page_d'accueil)
This incident also happened again on [[phab:T270631|2020-12-21]].",,"* [[gerrit:c/operations/mediawiki-config/+/637752|reduce number of feed items from 60 to default]] (done)
* {{phab|T266900}}: FeaturedFeeds should not load all feed content just to output the feed URLs on the main page (done)",2020-10-30,Unknown,Unknown,Unknown,Unknown,Unknown,2020-10-30_fr.wikipedia.org-slow-main-page.wikitext
"On Dec 2nd 2020 we Analytics team migrated Hive's netflow data set from the `wmf` database to the `event` database; and accordingly, it's HDFS data was moved from `/wmf/data/wmf/netflow` to `/wmf/data/event/netflow`. This migration was part of [https://phabricator.wikimedia.org/T231339 T231339], you can check the migration plan [https://phabricator.wikimedia.org/T231339#6652012 here]. The migration went well, but a detail was missing in the migration plan: Hive's `event` database, as well as the corresponding HDFS directory `/wmf/data/event`, have a purging job set up to delete all data older than 90 days within that database/directory. Deletion is necessary to abide to [https://meta.wikimedia.org/wiki/Data_retention_guidelines WMF's data retention guidelines]. That job runs periodically on a daily basis, at midnight UTC. If you want to prevent the purging job to delete a given data set within its premises, you have to white-list it in the [https://github.com/wikimedia/puppet/blob/production/modules/profile/manifests/analytics/refinery/job/data_purge.pp#L334 job's config] (via a regex). We did not do that, and as a result, at the end of the day (00:00 UTC) the purging job deleted all netflow data except for the last 90 days.",,,"* 2020-12-02T18:00:00 UTC  DEV and SRE perform netflow's Hive migration.
* 2020-12-03T00:00:00 UTC  Purging job silently deletes all netflow Hive data older than 90 days.
* 2020-12-03T18:00:00 UTC  DEV notices missing data while working on related task, and notifies part of the Analytics team.
* 2020-12-03T18:30:00 UTC  DEVs and SRE implement a quick fix to prevent further deletion by purging script.
* 2020-12-03T19:00:00 UTC  Part of the Analytics team discusses repercussions, possible mitigation actions, and next steps.
* 2020-12-03T20:00:00 UTC  DEV sends an email to the whole Analytics team, explaining the incident with detail.
* 2020-12-04T10:00:00 UTC  DEV notifies netflow team stakeholders of the incident, and they evaluate repercussions.
* 2020-12-17T17:30:00 UTC  The Analytics team has a post-mortem meeting to discuss actionables and learnings.",,"* Proposed mitigation: Remove --skipTrash flag from some of the data deletion scripts. Namely, the ones that delete data in a generic way for a dynamic number of data sets (like the one in this incident). If we do so, the deleted data will still live in the .Trash folder for 30 days. Usually deleted data is privacy-sensitive, so keeping it for an extra 30 days could be a problem. However, we can consider the .Trash folder as a temporary safety backup, and backups are treated differently by WMF's [https://meta.wikimedia.org/wiki/Data_retention_guidelines data retention guidelines]; they are allowed to be kept for longer than 90 days. One caveat to take into consideration is that even just 30 days of extra data can become a considerable amount of data to keep and this might in some cases put pressure on the cluster's capacity. https://phabricator.wikimedia.org/T270431
* Proposed mitigation: The deletion scripts should have a safeguard that prevents them to delete the data if its size is unexpected. You can run a deletion script telling it with a flag, how much data you expect it to delete (D). If the data to be deleted is more than D, then the script will no-op, warn and exit with non-0 code. With this feature, we could make sure that no unexpected extra data gets deleted. At some point we might receive alerts when the data size naturally grows, but that's acceptable in exchange of the extra protection. https://phabricator.wikimedia.org/T270433",2020-12-02,Unknown,Unknown,Unknown,Unknown,Unknown,2020-12-02_netflow-hive-migration.wikitext
Details restricted. See https://phabricator.wikimedia.org/T272215,,,,,* https://phabricator.wikimedia.org/T272262 - service restart script doesn't detect failure when running with poolcounter.,2021-01-16,Unknown,Unknown,Unknown,Unknown,Unknown,2021-01-16_appserver_latency.wikitext
"Today at 11:41 the icinga check for `ms-fe.svc.codfw.wmnet` timed out and thus paged (and recovered three minutes later):

<pre>
11:41 -icinga-wm:#wikimedia-operations- PROBLEM - LVS swift-https codfw port 443/tcp - Swift/Ceph media 
          storage IPv4 #page on ms-fe.svc.codfw.wmnet is CRITICAL: CRITICAL - Socket timeout after 10 
          seconds https://wikitech.wikimedia.org/wiki/LVS%23Diagnosing_problems
...
11:44 -icinga-wm:#wikimedia-operations- RECOVERY - LVS swift-https codfw port 443/tcp - Swift/Ceph media 
          storage IPv4 #page on ms-fe.svc.codfw.wmnet is OK: HTTP OK: HTTP/1.1 200 OK - 396 bytes in 
          0.140 second response time https://wikitech.wikimedia.org/wiki/LVS%23Diagnosing_problems
</pre>

Users in codfw/ulsfo/eqsin have experienced ~15min of higher latency (possibly timeouts) for hit-local and miss requests (10-25% of the site's requests, depending on the site).

[[File:Swift-codfw-11feb2021-high-latency.png.png|thumb|Screenshot from https://grafana.wikimedia.org/d/8T2XA-5Gz/frontend-ats-tls-ttfb-latency]]

Specifically hitting <tt>/monitoring/backend</tt> timed out, this in turn meant that some of the backend server(s) where the <tt>monitoring</tt> container lives were slow/unhealthy.
Case in point <tt>ms-be2033.codfw.wmnet</tt> was reported as slow from <tt>/var/log/swift/server.log</tt> on e.g. <tt>ms-fe2006.codfw.wmnet</tt>:

<pre>
Feb  1 11:44:29 ms-fe2006 proxy-server: ERROR with Object server 10.192.16.15:6000/sdk1 re: Trying to GET /v1/AUTH_mw/monitoring/backend: Timeout (10.0s) (txn: txe96767fb630b4828af04a-006017e993) (client_ip: 208.80.154.88)
</pre>

The slowness was induced by an earlier swift rebalance ({{Bug|T272837}}) and the way we do rebalances at the moment means that such operations are generally noisy/impactful to the cluster (e.g. {{Bug|T221904}}, {{Bug|T271415}}). Swift has been depooled internally from its discovery record (essentially anticipating {{Bug|T267338}}).",,,,,"* Change <code>/monitoring/backend</code> to <code>/monitoring/frontend</code> (i.e. check the frontend itself) for icinga service check and pybal's proxyfetch {{Bug|T273453}}
* Consider depooling swift's discovery records during rebalances {{Bug|T273453}}",2021-02-01,Unknown,Unknown,Unknown,Unknown,Unknown,2021-02-01_swift-codfw.wikitext
"Certificates for hostnames of Cloud VPS web proxies were able to expire, such for [https://puppet-compiler.wmflabs.org/ https://puppet-compiler.wmflabs.org].",,,,,* https://phabricator.wikimedia.org/T273959,2021-02-05,Unknown,Unknown,Unknown,Unknown,Unknown,2021-02-05_wmflabs_certs_expired.wikitext
"A [https://gerrit.wikimedia.org/r/c/operations/puppet/+/666899 patch] was merged and deployed to all hosts containing a syntax error on the <code>/etc/sudoers</code> file. This meant sudo did not work for the period of time indicated below, affecting mostly nagios execution (alerting) and creating root mail spam. As a consequence, also mail delivery got overloaded/delayed.",,,"* 08:50 [https://gerrit.wikimedia.org/r/c/operations/puppet/+/666899 666899] is merged, containing a syntax error in /etc/sudoers
* 08:51  People warn on IRC unable to sudo on db1107 due to a parse error (>>> /etc/sudoers: syntax error near line 6 <<<), and other hosts
* 08:52 100s of emails start to arrive to root@ with *** SECURITY information for <hostname>*** (sudo failures)
* 08:55 <jbond42> !log disabled puppet pending rollback of https://gerrit.wikimedia.org/r/666899
* 08:59 klausman merges [https://gerrit.wikimedia.org/r/c/operations/puppet/+/667110 667110], containing a fix, and runs puppet-merge soon after.
* 09:00 Incident opened.  jynus becomes IC.
* 09:06 Puppet reenabled
* 09:12 Start reenabling puppet fleetwide
* 09:23 Puppet run at 10%
* 09:37 Puppet run at 30%
* 09:50 Puppet run at 50%
* 10:17 Puppet run at 80%
* ~10:20ish UNKNOWN nagios alerts gone
* 10:33 puppet run finished
* 11:40: jbond took over ic
* 11:45: mx2001 queue has remain static at 4344 for 20 minutes
* 11:45: mx1001 queue reducing at between  0-3 msgs/sec
* 12:55: run `exiqgrep -i -o 7200 -y 10800 -f 'root@wikimedia.org' | xargs exim -Mrm` on mx servers
* 12:55: queue down to ~ 2000 (891 frozen) msgs on mx1001 and 500 (434 frozen) on mx2001 
* 13:02: Still receiving 450-4.2.1 from gmail for a number of recipients
* 13:30: reports of flood emails slowing down 
* 13:43: message queue excluding frozen messages on mx2001 is 0 (mx1001 ~ 800)
* 14:00 ran the following to push through the last few messages:  `for i in $(sudo exiqgrep -f nagios@lists1001.wikimedia.org -i) ; do sudo exim -M ${i} ; sleep 1 ; done `
* 14:05: unfrozen queue is still at 784 however the queue looks normal 
* 14:05: Incident resolved",,"* Add `validate_cmd` to sudoers file
** https://gerrit.wikimedia.org/r/667119
** https://gerrit.wikimedia.org/r/667120 
* Understand safe batch numbers for fleetwide puppet runs
* Prevent automatic email to get marked as spam by GMail
* Add exim queue metrics to grafana https://phabricator.wikimedia.org/T275867
* Investigate why the exim queue has many frozen messages for wiki at wikimedia.org and otrs at ticket.wikimedia.org",2021-02-26,Unknown,Unknown,Unknown,Unknown,Unknown,2021-02-26_sudo.wikitext
"On March 14 2021 the MediaWiki API were overloaded and ran out of php-fpm processes. This caused an API outage on all API servers from 17:00 to 17:26 UTC. The root cause of the outage were queries against commons that caused database s4 on server db1144 to be overloaded. Db1144 also serves queries to contributions, recentchanges, watchlist and other MediaWiki features. Task: [[phab:T277417|T277417]]

[[File:Php response time.png]][[File:Db processlist on db1144.png|alt=Processlist on db1144]]

https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?viewPanel=46&orgId=1&from=1615734448378&to=1615746774986&var-datasource=eqiad%20prometheus%2Fops&var-cluster=api_appserver&var-method=GET&var-code=200

https://grafana.wikimedia.org/d/000000273/mysql?viewPanel=37&orgId=1&var-server=db1144&var-port=13314&from=1615738747603&to=1615745074190

The queries against commons were analyzed for inefficiencies but seem to be well written and optimized SQL. See task: [[phab:T277416|T277416]]",,,,,*Improve traceability of commons queries: [[phab:T193050|T193050]] (filed in 2018),2021-03-14,Unknown,Unknown,Unknown,Unknown,Unknown,2021-03-14_MediaWiki_API.wikitext
"An upload of [[phab:T278856|65 video 4k files]] via the [[Uploading_large_files|server-side upload process]] caused high CPU/socket timeout errors on jobrunners (all jobrunner hosts are also videoscalers).  This caused an increase in job backlog and unavailability on several mw-related servers (job queue runners, etc.).  It seems that a combination of the files being 4k (and thus requiring many different downscales), long (averaging an hour in length), combined with the fact that the videos were uploads from a local server (mwmaint) with a fast connection to the rest of our infrastructure resulted in too much load being placed on the jobqueue infrastructure.

Halting the uploads and temporarily splitting the jobqueue into videoscalers and other jobrunners allowed the infrastructure to catch up.",,,,,"* <s>Document that users should use <code>--sleep</code> to pause between files when running <code>importImages.php</code> [https://wikitech.wikimedia.org/w/index.php?title=Uploading_large_files&type=revision&diff=1905816&oldid=1830856 (done)]</s>
* [[phab:T278943 | Rate limit the process to upload large files ]]
* [[phab:T278945 | Add rate limiting to the jobqueue videoscalers ]]
* [[phab:T278946 | Add alerting for Memcached timeout errors ]]
* <s>[[phab:T278948 | Update Runboook wikis for the application and LVS servers ]]</s>
* [[phab:T279100|Have some dedicated jobrunners that aren't active videoscalers]]",2021-03-30,Unknown,Unknown,Unknown,Unknown,Unknown,2021-03-30_Jobqueue_overload.wikitext
"Some mailing lists have disabled archiving, with the expectation that once current subscribers get a copy of the message then the mailing list server will no longer have a copy of the message. However, it saves any attachment permanently, crucially, this includes the HTML part of any multi-part message. These attachments are publicly accessible via a URL: <code>https://lists.wikimedia.org/pipermail/wikidata-bugs/attachments/20200901/7b1e75bd/attachment.htm</code>. It would require brute-forcing a 8 character hex string per day to figure out the URL. This functionality exists for the purpose of supporting digests, as the digests are sent as plain text and attachments are only referred to by URL. The URL is public because Mailman2 doesn't have any real web authentication support. It has been [https://bugs.launchpad.net/mailman/+bug/266317 publicly filed in the Mailman2 bug tracker] since 2006.

Old messages have been kept since the entire lifetime of our Mailman2 installation. Audits of the available Apache HTTPD access logs (30 days) found no unauthorized access.

A systemd timer was put in place to delete these archived attachments after 31 days. Some lists also disabled digests to prevent these attachment files from being saved in the first place. This issue was fully resolved with the migration to Mailman3, which does not suffer from this flaw.

'''Impact''': Any person who sent an email to a list that was not supposed to be archived could have had their message contents publicly accessible and leaked. It is impossible to say whether someone ever did brute-force these URLs given how long this vulnerability was present and how short our access logs are kept for.

{{TOC|align=right}}",,Amir discovered this issue while going through archives on lists1001 while preparing for the Mailman3 migration. It's unclear how we could have noticed this automatically or through monitoring.,"'''2021-04-03''' (Sat)

* 15:18: <Amir1> [mailman2] keeps the attachments of mailing lists that are set to not keep an archive, which is a security issue, I’ll file a bug
* 15:47: Amir opens [[phab:T279237|T279237]]
* 19:55: Reuven notifies Legal and Faidon.

'''2021-04-04''' (Sun)

* 11:56: Amir discovers that the attachments have public-facing URLs.
* 14:02: Amir deletes all lgbt@ attachments older than 2020, leaving the newer ones for investigation.

'''2021-04-05''' (Mon)

* 15:45: [https://docs.google.com/document/d/1ljqsPY3FdP3uK6NocZb0UAk3siayNYsKb3duZ5CWgN0/edit#heading=h.bp8wb4pwerhj Incident opened]. Reuven becomes IC.
* 19:30: Reuven checks Apache access logs on lists1001 and verifies no one has brute-forced the attachment URLs during the logs retention period (30 days).
* 21:32: Kunal runs purge_attachments.py manually, deleting all attachments from all no-archive lists with a datestamp older than 31 days. List of deleted files is in lists1001:/home/legoktm/attachments.txt. Also manually deleted /var/lib/mailman/archives/private/helpdesk-l/attachments/20070931 and /var/lib/mailman/archives/private/pressemeldungen/attachments/20070931 (note that September 31st is not a real date)

'''2021-04-06''' (Tues)

* 23:30: Kunal re-runs purge_attachments to clean up one more day’s worth of attachments. Log is at lists1001:/home/legoktm/attachments2.txt.

'''2021-04-13''' (Tues)

* 16:40: Kunal deploys systemd timer to automatically run purge_attachments and kicks off a manual run. It’ll now run at midnight UTC every day. Logs of deletions should be at /var/log/purge_attachments/ going forward.
* 17:00 rzl closes the incident, since the 31-day automatic purge is enough for the medium-term. The Bacula issue is still open; will continue to follow up.

'''2021-04-15'''

* Jamie configures backups to ignore attachments directory for lists that have archiving disabled: [[gerrit:679869]]

'''2021-04-20'''

* Kunal verifies that Mailman3 does not also suffer from this flaw

'''2021-05-03'''

* Amir [[phab:T280322#7054296|migrates]] lists with archiving disabled (""Group A"") over to Mailman3","===What went well?===

* We were able to quickly (within 2 days, on the first weekday) put together a script to purge old attachments.
* We were already in the progress of moving to a newer version of Mailman that didn't have this issue.","* {{Done}} Set up a timer to purge old attachments
* {{Done}} Verify Mailman3 does not have this issue
* {{Done}} Ensure digest message contents are excluded from Mailman2 and Mailman3 backups
* {{Done}} Migrate all lists with archiving disabled over to Mailman3",2021-04-03,Unknown,Monitoring,Unknown,Unknown,Unknown,2021-04-03_mailman2_attachments.wikitext
"Around 17:00 UTC, a faulty network switch caused partial failure of one rack in the [[Codfw cluster]]. As 8 individual hosts become unreachable, this led to reduced redundancy or capacity for some of the affected services. The hosts were moved to new switch ports, and were unreachabe for about 1 hour.

* Elastic search: Three elastic nodes down, mild impact to redundancy (yellow state).
* Swift media storage: Three ms-be nodes down in the standby cluster, no user impact (should catch up once reachable).
* Edge cache: One cp node unreachable for the Codfw PoP, automatically depooled by healthcheck.
* DNS: One dns node unreachable, effectively depooled automatically per lack of BGP advertising.",,,,,Tracking task for the incident: https://phabricator.wikimedia.org/T279457,2021-04-06,Unknown,Unknown,Unknown,Unknown,Unknown,2021-04-06_partial_rack_codfw.wikitext
"From 07:31 to 08:16 UTC, there was increased latency and error rates for some MediaWiki cache misses and authenticated requests. The incident predominantly affected API users and bots on commons.wikimedia.org. API request latency for some requests went up by 5 seconds, and error rates upto 25%.

The issue was found to be caused by a bot causing increased load on the commonswiki databases, and consuming API webserver resources. Later in the time range, regular users and other wikis were affected as well through cross-wiki features involving Commons.",,,,,Tracking task:  https://phabricator.wikimedia.org/T280232,2021-04-15,Unknown,Unknown,Unknown,Unknown,Unknown,2021-04-15_appserver_latency.wikitext
"From approximately 7:31 to 8:16 UTC, there has been instability serving Wiki uncached content (mostly impacted would be authenticated users and bots, as well as POST requests), affecting at first commonswiki, but later most other wikis, albeit at a lower rate.
The impact between those timestamp was increased latencies (up to 5+ seconds to return results) and high level of errors (approximately 1/4th of request lost). This was due to overload on commonswiki requests blocking resources on most api & database servers, which in turn created contention on most other wikis.",,,,,"* Uncached wiki requests partially unavailable due to excessive request rates from a bot https://phabricator.wikimedia.org/T280232
* Determine safe concurrent puppet run batches via cumin https://phabricator.wikimedia.org/T280622",2021-04-20,Unknown,Unknown,Unknown,Unknown,Unknown,2021-04-20_MediaWiki_API_slowdown.wikitext
"At 9:21, ms-be1062 had a crash in its network stack (tracked at [https://phabricator.wikimedia.org/T281107 T281107]) which caused the server to blackhole traffic. This caused a lot of requests to swift to reach timeouts, causing the consequent starvation of resources for the thumbor cluster workers, which were mostly waiting for a response from swift. This happened during a rebalance operation of the swift cluster, which we've seen in the past can impose quite some stress on the swift backend in our current configuration. Traffic was diverted to the codfw cluster for generating thumbnails at 09:26, thus minimizing impact for users. Rebooting ms-be1062 at 9:38 solved the issue instantly.",,,,,"* Understand why pybal would not depool any of the backends even if they were returning 503s even to monitoring (TODO: Create task)
* Track down the kernel bug that caused ms-be1062 to blackhole traffic. It’s also worrisome that a swift cluster would become unresponsive in such a situation.  https://phabricator.wikimedia.org/T281107",2021-04-26,Unknown,Unknown,Unknown,Unknown,Unknown,2021-04-26_thumbor.wikitext
"From 10:28 to 11:43, Wikimedia Commons was unavailable and/or slow to respond. Additionally, from 10:41 to 11:10, commons was set in unscheduled read only mode (unable to create new users, upload files or edit pages). The immediate cause was large amount of InnoDB contention for read and write queries on the primary s4 database (which serves commonswiki and testcommonswiki writes). After a restart and a forced kill, the database came back cleanly in read only mode. Global usage extension was temporarily disabled (several queries were observed from this extension blocked on the master) before the server was set back up in read write, ending the incident. 

GlobalUsage was signaled as the most probable cause, as it was sending large amount of long-running queries at the moment of the issue to the primary DB server. Because of the general InnoDB error obtained -contention on an Innodb index (on an engine where everything, including data, are indexes)- it is difficult to say if this was the primary cause, or another underlying cause caused them to block, leading the the outage. Other scaling problems are known to occur on commonswiki (e.g. large image table). Work has been done to send as much traffic as possible to the replicas.
{{TOC|align=right}}",,"* 10:33: received Alerts relating to Mysql replication on S4
 PROBLEM - MariaDB Replica IO: s4 #page on db1143 is CRITICAL: CRITICAL slave_io_state 
          Slave_IO_Running: No, Errno: 2013, Errmsg: error reconnecting to master repl@db1138.eqiad.wmnet:3306 - 
          retry-time: 60 maximum-retries: 86400 message: Lost connection to MySQL server at waiting for initial 
          communication packet, system error: 110 Connection timed out 
          https://wikitech.wikimedia.org/wiki/MariaDB/troubleshooting%23Depooling_a_replica

* 10:33: The master was quickly identified as the source of problems

On the database error log, it was seen:

 Apr 27 10:29:59 db1138 mysqld[3848]: 2021-04-27 10:29:59 139919263487744 [Note] Threadpool has  
 been blocked for 30 seconds
 Apr 27 10:30:14 db1138 mysqld[3848]: InnoDB: Warning: a long semaphore wait:
 Apr 27 10:30:14 db1138 mysqld[3848]: --Thread 139901381777152 has waited at fut0fut.ic line 51  
 for 241.00 seconds the semaphore:
 Apr 27 10:30:14 db1138 mysqld[3848]: X-lock on RW-latch at 0x7f51d3e0c898 '&block->lock'
 Apr 27 10:30:14 db1138 mysqld[3848]: a writer (thread id 139927395628800) has reserved it in mo
 de  exclusive
 Apr 27 10:30:14 db1138 mysqld[3848]: number of readers 0, waiters flag 1, lock_word: 0
 Apr 27 10:30:14 db1138 mysqld[3848]: Last time read locked in file row0sel.cc line 3075
 Apr 27 10:30:14 db1138 mysqld[3848]: Last time write locked in file buf0buf.cc line 4279
 Apr 27 10:30:14 db1138 mysqld[3848]: Holder thread 0 file not yet reserved line 0

Which ended up blocking any operation, even killing threads for a clean shutdown.","'''All times in UTC.'''

* 10:29: '''Incident starts'''
* 10:29: InnoDB Monitor starts to warn about threadpool blocked
* 10:33: received Alerts relating to Mysql replication on S4
* 10:33: DBAs and SREs show up
* 10:38: started to receive multiple alerts relating to Restbase
* 10:38: The master looks overloaded with 10k connections stuck: attempts to kill all select queries on the master
* 10:38: Incident opened. '''Jbond becomes IC.''' 
* 10:41: switched commons (s4) to read only via dbctl
* 10:42: restarts master
* 10:43: receive alerts for mysql replication again
* 10:45: high latency on mediawiki api
* 10:47: Replication status all seem to be at [https://phabricator.wikimedia.org/P15586 432521631] (this means we were in a good position to failover to a candidate master if needed).
* 10:50: master still restarting
* 10:52: During the master overload it was observed thousands of selects like the following but it is not clear whether this was the cause or the consequence: 
<syntaxhighlight lang='sql'>
SELECT /* MediaWiki\Extension\GlobalUsage\GlobalUsage::getLinksFromPage Abraham */  gil_to  FROM `globalimagelinks`    WHERE gil_wiki = 'plwiktionary' AND gil_page = 852538 
</syntaxhighlight>
* 10:55: master still/stuck restarting
* 10:55: discuss kill mysql and let the recovery do its thing
* 10:56: observed a ton of innodb contention errors:  Writer thread waiting semaphore, could this too be effect?
* 10:56: kill mysql process
* 10:57: start mysql process
* 10:57: Observed I think a lot of LinksUpdate operations caused the globalusage queries, which  overloaded master
* 10:58: InnoDB: Restoring possible half-written data pages from the doublewrite buffer
* 10:58: mysql is now doing its innodb recovery and rollback
* 10:59: innodb recovery finish (s4 master still RO)
* 10:59: Reports that page deletes on eswiki are failing (globalusage blocks deletion because commons is ro)
* 11:07: disable globalusage extension
* 11:07: nothing interesting in raid and dmesg, nothing there
* 11:10: enable Read Write on S4 database
* 11:13: add message to village pump
* 11:14: master is behaving fine
* 11:15: '''User viable issues resolved'''
* 11:45: '''incident officially closed'''","=== What went well? ===
* Response only took a few seconds after the page
* Automated monitoring detected the action
* able to recover latency issues by expediting a switch to RO
* once Mysql was killed recovery was very quick","* [https://phabricator.wikimedia.org/T281261 update icinga alert link]
* [https://phabricator.wikimedia.org/T281238 Disable GlobalUsage does selects on the master database]
* [https://phabricator.wikimedia.org/T281240 update documentation for Changeprop]
* [https://phabricator.wikimedia.org/T281242 Re-enable globalusage]
* [https://phabricator.wikimedia.org/T281249 Modify an existing tool that quickly shows the db replication status]",2021-04-27,Unknown,Unknown,Database,Unknown,Unknown,2021-04-27_Commons_wiki_primary_db.wikitext
"On 14 April, a refactor of mediawiki-BagOStuff was deployed which introduced a bug that caused revision text blobs to no longer be cached in Memcached. Over a 7-day period, this gradually increased connection and query load on ExternalStore DB servers. On 22 April, the last revision-text blobs expired from Memcached, but we remained just under the DB load limits. 

Around 13:00 UTC on 29 April, an unrelated increase in backend traffic that we are normally able to handle just fine, resulted in further unthrottled demand on ExternalStore DB. We were now over the load limits, and thus until the incident resolution around 18:00 UTC, there were numerous one-minute spikes where 1-10% of appserver requests responded with HTTP 5xx errors. 

'''Impact''': During a period of five hours (13:00-18:00 UTC) there were 18 distinct minutes during which the error rate was severely elevated above the normal <0.1%, with various minutes having had 1-10% of backend requests responded to with errors.  

{{TOC|align=right}}",,"* [https://grafana.wikimedia.org/d/000000278/mysql-aggregated?orgId=1&from=1617919200000&to=1619721000000&var-site=eqiad&var-group=core&var-shard=es5&var-shard=es4&var-shard=es3&var-shard=es2&var-shard=es1&var-role=All Grafana: MySQL load (queries, connections)]
* [https://grafana.wikimedia.org/d/lqE4lcGWz/wanobjectcache-key-group?orgId=1&from=1617919200000&to=1619721000000&var-kClass=SqlBlobStore_blob Grafana: WANObjectCache stats (cache misses)]
* [https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?orgId=1&from=1619690400000&to=1619737200000 Grafana: Application Servers (traffic, latencies, error rate)]
* [[phab:T281480|T281480: SqlBlobStore no longer caching blobs, DBConnectionError Too many connections]]","=== 14 April 2021 ===
On 14 April, the MediaWiki [[mw:MediaWiki_1.37/wmf.1|1.37-wmf.1 branch]] ([[Deployments/Train|weekly train]]) was deployed. This included a refactor of mediawiki-BagOStuff code ([[gerrit:c/mediawiki/core/+/677064|change 677064]]), which introduced a bug that led to the revision text service (""SqlBlobStore"") failing to recognise Memcached as an eligible caching layer for (new) data.

During the week that followed, database connections and query load on ExternalStoreDB gradually increased more than 1400% (or 15x), from 4000 qps to 60,000 qps. This increase happened gradually because the bug only affected the decision to cache a blob after cache-miss (existing caches remained effective, so long as they were relevant and not evicted/expired). The WANObjectCache stats show an increase in revision-text cache misses, from 1 million per minute to eventually 15 million per minute.","===What went well?===
*We had enough capacity so this incident didn't cause any visible user impact, until April 29th ofc.","*{{phabricator|T281833}} (secondary metric -database qps- monitoring improvement)
* Revert https://gerrit.wikimedia.org/r/c/operations/puppet/%2B/683682/ (not the cause)",2021-04-29,Unknown,Unknown,Caching,Unknown,Unknown,2021-04-29_db_and_memc_load.wikitext
"At 09:23 UTC, alerts indicated connectivity issues to the [[Eqsin cluster|Eqsin]] cluster in Singapore. At 09:31 UTC, @[[m:User:ERocca (WMF)|Ema]] deployed [https://gerrit.wikimedia.org/r/c/operations/dns/+/699910 a DNS change] to depool the Eqsin cluster. This diverted most of its assigned traffic to [[Ulsfo cluster|Ulsfo]], and some to [[Esams]]. At 09:35 UTC traffic started recovering, with traffic back to regular levels at 09:45 UTC. The 15-minute window is attributed to DNS caches expiring (e.g. at ISPs and on client devices). The connectivy issues were resolved later that day, and at 18:50 UTC @[[m:User:CMooney (WMF)|CMooney]] repooled the Eqsin cluster, with traffic back to regular levels in Eqsin by 19:00 UTC.

'''Impact:''' For about 35 minutes from 09:20 to 09:45 UTC, the wikis were largely unreachable from countries normally served by the Singapore DC (including India, Hong Kong, and Japan).

'''Documentation:'''
[[File:Rl traffic by dc-20210615.png.png|thumb|Traffic by DC.]]

* [https://github.com/wikimedia/operations-dns/blob/e4841851bef97124a3fd5beb09ffcc61fa49f59f/geo-maps#L7-L63 Wikimedia DNS: DC geo map]
* [https://grafana.wikimedia.org/d/000000230/navigation-timing-by-continent?orgId=1&from=1623668400000&to=1623830400000&var-metric=responseStart&var-location=Asia&var-prop=p75 Grafana: Navigation Timing by Continent 2021-06-15]
* [https://grafana.wikimedia.org/d/000000066/resourceloader?viewPanel=39&orgId=1&from=1623744000000&to=1623785400000 Grafana: Traffic volume by DC 2021-06-15]<!-- Reminder: No private information on this page! -->",,,,,"* Public tracking task: https://phabricator.wikimedia.org/T284986
* TODO [[phab:T286554|Per-country Frontend Traffic dashboards T286554]]",2021-06-15,Unknown,Unknown,Unknown,Unknown,Unknown,2021-06-15_Eqsin_network.wikitext
"Shortly after the June 2021 switch over, the backend appservers were unable to serve requests to <code>tr.wikivoyage.org</code> for about 8 minutes from 14:22 UTC to 14:30 UTC. No other wikis were affected.

'''Impact''': For 8 minutes, registered users on the Turkish Wikivoyage were consistently unable to load any pages or perform any actions. The general public may have noticed it to a lesser extent due to caching at our CDN layer, although any pages absent from the CDN cache would have also been been temporarily unavailable.",,,,,* {{done}} [[phab:T260297|T260297: db-eqiad and db-codfw sectionsByLoad can get out of sync]],2021-06-29,Unknown,Unknown,Unknown,Unknown,Unknown,2021-06-29_trwikivoyage_primary_db.wikitext
"Sometime after 22:00 UTC, a traffic surge started to fill up the capacity of a network port in the [[Ulsfo cluster]]. By 23:00 UTC the issue was mitigated.

'''Impact:''' For up to an hour, a portion of requests may have received a slower response from regions served by the San Francisco DC. This includes New Zealand, and parts of North America.

'''Documentation:'''
*[https://github.com/wikimedia/operations-dns/blob/e4841851bef97124a3fd5beb09ffcc61fa49f59f/geo-maps#L7-L63 Wikimedia DNS: DC geo map]",,,,,None.,2021-06-30,Unknown,Unknown,Unknown,Unknown,Unknown,2021-06-30_Ulsfo_traffic_surge.wikitext
"While working on [https://phabricator.wikimedia.org/T272714 updating EventGate to support Prometheus], Andrew Otto deployed the changes to eventgate-analytics in codfw (then-active DC).  This change removed the prometheus-statsd-exporter container in favor of direct Prometheus support, as added in recent versions of service-runner and service-template-node.

The deploy went fine in the idle ""staging"" and ""eqiad"" clusters, but when deploying to codfw, request latency from MediaWiki to eventgate-analytics spiked, which caused PHP worker slots to fill up, which in turn caused some MediaWiki API requests to fail.

The helm tool noticed that the eventgate-analytics deploy to codfw itself was not doing well, and auto-rolled back the deployment:

<pre>
$ kube_env eventgate-analytics codfw; helm history production
REVISION	UPDATED                 	STATUS    	CHART           	APP VERSION	DESCRIPTION
[...]
4       	Wed Jul 14 16:07:12 2021	SUPERSEDED	eventgate-0.3.1 	           	Upgrade ""production"" failed: timed out waiting for the co...
5       	Wed Jul 14 16:17:18 2021	DEPLOYED  	eventgate-0.2.14	           	Rollback to 3
</pre>

'''Impact''': For ~10 minutes, MediaWiki API clients experienced request failures.

'''Documentation''':
* [https://grafana.wikimedia.org/d/VTCkm29Wz/envoy-telemetry?orgId=1&from=1626278199112&to=1626279999112&var-datasource=codfw%20prometheus%2Fops&var-origin=appserver&var-origin_instance=All&var-destination=eventgate-analytics Grafana: Envoy telemetry]
* [https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?viewPanel=9&from=1626276391814&orgId=1&to=1626279991814&var-datasource=codfw%20prometheus%2Fops&var-cluster=api_appserver&var-method=GET&var-code=200 Grafana: Application Servers dashboard]
* [https://grafana.wikimedia.org/d/VTCkm29Wz/envoy-telemetry?viewPanel=14&orgId=1&from=1626278171118&to=1626279427383 Grafana: Envoy telemetry / Upstream latency]",,,,,"* Figure out why this happened and fix.  Based on [https://logstash.wikimedia.org/app/discover#/doc/logstash-*/logstash-syslog-2021.07.14?id=YPDMpXoBStjVNP_PTvXC this log message], it seems likely that a bug in the service-runner prometheus integration caused the nodejs worker process to die. [DONE]
** Further investigation uncovered that <code>require('prom-client')</code> within a worker causes the observed issue.  Both service-runner and node-rdkafka-prometheus require prom-client.  It was proposed to patch  node-rdkafka-prometheus to handle passing in the prom-client instance. 
** node-rdkafka-prometheus is an unmaintained project, so we have forked it to @wikimedia/node-rdkafka-propetheus and fixed the issue there.  Additionally, if [https://github.com/siimon/prom-client/issues/448 this issue in prom-client] is fixed, we probably won't need the patch we made to node-rdkafka-prometheus for this fix.",2021-07-14,Unknown,Unknown,Unknown,Unknown,Unknown,2021-07-14_eventgate-analytics_latency_spike_caused_MW_app_server_overload.wikitext
"asw-a2-codfw, the switch handling the network traffic of rack A2 on codfw became unresponsive rendering 14 hosts unreachable. Besides losing the 14 hosts hosted on rack A2, two additional load balancers lost access to codfw's row A. Services with hosts in the impacted row (such as Swift and various mw-api servers) remained available for clients due to automatic failover and load balancing to remaining hosts. While mw-api remained available for end-users and external clients, the impacted Restbase load-balancer remained pooled causing Restbase to continue to try (and fail) to reach mw-api hosts. Thus, mobileapps API and cxserver API (which rely on Restbase) returned errors to clients for some time.    

Several other services were switched over from Codfw to Eqiad: Maps Tile Service and Wikidata Query Service.   

'''Impact''': For about 1 hour the Restbase, mobileapps, and cxserver services were serving errors. Reduced capacity of high-traffic1 load balancers and MediaWiki servers in Codfw.
{{TOC|align=right}}",,"The incident was detected via automated monitoring reporting several hosts of rack A2 going down at the very same time:
* <icinga-wm> PROBLEM - Host elastic2038 is DOWN: PING CRITICAL - Packet loss = 100%
* <icinga-wm> PROBLEM - Host kafka-logging2001 is DOWN: PING CRITICAL - Packet loss = 100
* <icinga-wm> PROBLEM - Host ns1-v4 is DOWN: PING CRITICAL - Packet loss = 100%
* <icinga-wm> PROBLEM - Host authdns2001 is DOWN: PING CRITICAL - Packet loss = 100%
* <icinga-wm> PROBLEM - Host ms-be2051 is DOWN: PING CRITICAL - Packet loss = 100%
* <icinga-wm> PROBLEM - Host lvs2007 is DOWN: PING CRITICAL - Packet loss = 100%
* <icinga-wm> PROBLEM - Host thanos-fe2001 is DOWN: PING CRITICAL - Packet loss = 100%
* <icinga-wm> PROBLEM - Host elastic2055 is DOWN: PING CRITICAL - Packet loss = 100%
* <icinga-wm> PROBLEM - Host elastic2037 is DOWN: PING CRITICAL - Packet loss = 100%
* <icinga-wm> PROBLEM - Host ms-fe2005 is DOWN: PING CRITICAL - Packet loss = 100%
* <icinga-wm> PROBLEM - Host ms-be2040 is DOWN: PING CRITICAL - Packet loss = 100%","'''All times in UTC.''' '''Friday 16th'''
* 13:16 asw-a2-codfw becomes unresponsive '''OUTAGE BEGINS'''
* 13:36 authdns2001 is depooled to restore ns1.wikimedia.org
* 14:07 Maps Tile Service is moved from codfw to eqiad
* 14:11 WikiData Query Service gets pooled in eqiad
* 14:30 ports on the affected switch are marked as disabled on asw-a-codfw virtual-chassis
* 14:37 disable affected network interface in lvs2010
* 14:41 disable affected network interface in lvs2009
* 15:14 re-enable affected network interface in lvs2010
* 15:31 remote hands are used in codfw to power-cycle the affected switch without success
* 15:38 re-enable affected network interface in lvs2009
* 15:48 Decrease depool threshold for the edge caching services
* 16:29 Decrease depool threshold for MediaWiki API service
* 16:56 Error rate recovers '''OUTAGE ENDS'''
'''Monday 19th'''
* 08:15 depool text cache codfw PoP
* 17:10 defective switch gets replaced
* 17:21 authdns2001 is pooled
*18:20 restored high availability in high-traffic1 load balancer in codfw
*18:35 lvs2010 recovers row A connectivity
*18:53 lvs2009 recovers row A connectivity

* 20:29 pool text cache codfw PoP
'''Tuesday 20th'''

* 13:45 Maps Tile Service is back to being served from codfw","As a result of losing one single switch, services were affected more than expected due to several weaknesses:
* 3 load balancers including the backup one get the row A traffic from one single network switch
* Depool threshold of several services is too restrictive to continue to work as expected losing a complete row","* Mitigate unresponsive switch (Done). https://phabricator.wikimedia.org/T286787
* Load balancers should be able to handle a NIC failing to be configured. https://phabricator.wikimedia.org/T286924
*Audit eqiad & codfw LVS network links. https://phabricator.wikimedia.org/T286881
*Avoid using the same switch to get traffic from a row on the primary and secondary load balancers. https://phabricator.wikimedia.org/T286879
*Fix Pybal IPVS diff check https://phabricator.wikimedia.org/T286913
* Fix depool threshold for text & upload edge caching services https://gerrit.wikimedia.org/r/c/operations/puppet/+/705381
*Fix depool threshold for mw api service https://gerrit.wikimedia.org/r/c/operations/puppet/+/708072",2021-07-16,Unknown,Monitoring,Logging,Unknown,Unknown,2021-07-16_asw-a2-codfw_network.wikitext
"[[File:Errors during 2021-07-26 DPL outage.png|thumb|Spikes in 503 errors returned to users]]
[[File:Overall traffic during 2021-07-26 DPL outage.png|thumb|Corresponding graphs of traffic during the outage, showing approximately 15% of requests receiving errors.]]
Following a large bot import to the Russian Wikinews, expanding the size of that project to 13 million pages, slow queries originating from ruwikinews's usage of the [[mw:Extension:DynamicPageList (Wikimedia)|DynamicPageList extension]]  (also known as ""[[git:mediawiki/extensions/intersection|intersection]]"") overloaded the s3 cluster of databases, causing php-fpm processes to hang/stall, eventually taking down all wikis with it. The outage was resolved by disabling the DynamicPageList extension on ruwikinews and aggressively killing queries on s3 replicas. Normally, DPL's database queries roughly scale to the size of the smallest category being intersected. This would be bad enough, as ruwikinews has categories that are orders of magnitude higher than other wikis with this extension. However, in this case MariaDB chose a query plan that involved scanning the entire categorylinks table. The query in question seen during the outage took more than 3 minutes to finish on an idle replica.

The DPL query was using a sort by page_id, descending (instead of the more common sort by c1.cl_timestamp that is DPL's default sort method). According to the [[phab:P16896|EXPLAIN]], MariaDB decided to optimize this by using the PRIMARY key on (cl_from, cl_to) [Remember, cl_from is a foreign key to the page_id of the page in the category, and cl_to is the name of the category in question]. This would have been a good query plan if most of the rows of the В_мире category matched the query (or even just 18 of them), especially since they all had high cl_from since they were all newly created. However, since no rows matched the query, this resulted in a full table scan of the categorylinks table (44,435,648 rows). The other possible query plan, using the (cl_to, cl_timestamp) index to only look at the 180,231 rows for the В_мире category and then filesorting the results, would probably have been more efficient in this case due to no rows matching the query. Potentially, disabling the sort by page_id feature of DPL, and instead only allowing the sort by c1.cl_timestamp (c1 in this case was [[n:ru:Category:Опубликовано|Опубликовано]]) would likely result in more efficient queries, relatively speaking. For context the reason the default method of sorting is sort by the first category's timestamp, is to allow the first category to be a ""published"" category, and allow showing results in order of when they were published. It would be even more efficient (And reduce likelihood of filesorts) if the sort was by the timestamp of the smallest category, although that may not be the behaviour desired by users.

It appears the triggering event was the creation of [[n:ru:Category:В мире]] which was linked on about 180,000 pages that also had this DPL query on them due to a template. This page creation triggered a large number of parse jobs (so links could change from red to blue) all making the same, very slow DPL query, which began to overload the s3 DB, which snowballed. The caching mitigation introduced during the previous incident did not work properly as the query often did not complete prior to a timeout involved, preventing it from being cached. This was true even before the incident when the DB replicas were not under extreme load.

Overall were 30 minutes of high latencies, failing to respond, or fatal errors, affecting wikis due to unavailable PHP-FPM workers. Based on traffic graphs the outage impacted to approximately a 15% of all incoming HTTP requests for wikis, those being either lost, suffering high latencies or 5XX error codes. The main impact was uncached requests, suffering a 0% availability during several moments of the outage, on all wikis.

'''Impact''': For 30 minutes, 15% of requests from contributors on all wikis were responding either slowly, with an error, or not at all. There were also brief moments during which no readers could load recently modified or uncached pages.
{{TOC|align=right}}",,"Icinga sent two pages at 10:33 for <code>Not enough idle PHP-FPM workers for Mediawiki</code> on the appserver and api_appserver clusters.

The first user report on IRC appears to have been at 10:35 in #wikimedia-sre: <code><RhinosF1> Meta is down</code>.

Because this was a full outage, every host was individually alerting and so were services that depend upon MediaWiki. Each appserver triggered two alerts, like:
<pre>
<icinga-wm> PROBLEM - Apache HTTP on mw2316 is CRITICAL: CRITICAL - Socket timeout after 10 seconds https://wikitech.wikimedia.org/wiki/Application_servers
<icinga-wm> PROBLEM - PHP7 rendering on mw2316 is CRITICAL: CRITICAL - Socket timeout after 10 seconds https://wikitech.wikimedia.org/wiki/Application_servers/Runbook%23PHP7_rendering
</pre> 

icinga-wm sent 145 messages to #wikimedia-operations between 10:34 and 10:36 before being kicked off the Libera Chat network for flooding. That IRC channel was unusable and discussion was moved to #wikimedia-sre and then #mediawiki_security.","'''Prologue'''
* 2020-09-07 and 2020-09-08: Following rapid bot imports (~100k pages in 1 day), DynamicPageList queries from ruwikinews caused problems on s3, though it did not lead to a sitewide outage. A summary of that incident is available at [[Phab:T262240#6449531|T262240#6449531]] (TODO: create proper incident report).
* 2021-07-12 through 2021-07-26: [[n:ru:User:NewsBots|NewsBots]] imports/creates 3,182,955 pages to the Russian Wikinews.
* 2021-07-24 through 2021-07-26 - ruwikinews starts logging ""Async refresh failed for ruwikinews:DPLQuery:4ea73c61c64e57c48f89a3da9caee058cbc5888bc11d2597bcfa957dd542f4e4"". This will spike during the incident. The hash corresponds to the DPL query used in the infobox included on pages in [[n:ru:Категория:В_мире]]. This happens when the query takes longer than 60 seconds, and indicates that the caching solution from the previous incident meant to reduce risk of cache stampedes is no longer working.
* 2021-07-26: In the lead up to the incident, DPL queries are often taking up to 20 seconds to complete

'''Main outage, all times in UTC.'''
* 10:29: [[n:ru:Категория:В_мире]] is created by a logged out user [https://ru.wikinews.org/w/index.php?title=%D0%A1%D0%BB%D1%83%D0%B6%D0%B5%D0%B1%D0%BD%D0%B0%D1%8F:%D0%96%D1%83%D1%80%D0%BD%D0%B0%D0%BB%D1%8B&logid=19137921]. This category is linked to on 179,385 pages, triggering a re-parse job for each page. Most of these pages have an infobox containing the problematic DPL query in question. During the incident much of the log entries have the request ID ''b0324fd-a93e-4aba-ab50-e58be6f2c38d'' from this edit.
* 10:30: Database overload starts '''OUTAGE BEGINS'''
* 10:30: Lots of the Parsoid jobs triggered by the edit to the category are reporting as failed.
* 10:33: Page fire for both appserver and api_appserver clusters: <code>Not enough idle PHP-FPM workers for Mediawiki</code>
* 10:34: Significant IRC alert spam ensues, comms move to #wikimedia-sre and #mediawiki_security
* 10:35: ""upstream connect error or disconnect/reset before headers. reset reason: overflow"" on enwiki
* 10:38: Manuel depools db2149, that seems the most affected DB ([https://sal.toolforge.org/log/QRJl4noB1jz_IcWuURo0 SAL entry])
* 10:39: [[Phab:T287362|T287362]] filed by users unable to access arwiki,
* 10:40: after a brief apparent recovery the load shifts to another DB
* 10:42: Slow query identified as coming from <code>DynamicPageListHooks::processQuery</code>
* 10:42: Link to previous incident from 2020-09 established ([[Phab:T262240|T262240]]), people involved in that ticket pinged on IRC
* 10:46-10:49: Manuel slowly repools db2149
* 10:48: Recommendation made to disable DynamicPageList on ruwikinews instead of increasing cache TTL
* 10:50: Incident opened ([https://docs.google.com/document/d/15uw05hinO3ADNbbxKMPfpzxrTGkDBWifWY14-xu6lKU/edit?usp=sharing private Google Doc]).
[[File:MariaDB traffic during 2021-07-26 DPL outage.png|thumb|MariaDB traffic during the outage showing a dramatic increase in rows read and number of open connections.]]
* 10:51: Jaime sets the query killer on S3 replicas to 10 seconds for the MediaWiki user ([https://sal.toolforge.org/log/6mtx4noB8Fs0LHO5PaBK SAL entry])
* 10:55: Amir disables DPL on ruwikinews ([https://sal.toolforge.org/log/7Rp04noBa_6PSCT9NDtq SAL entry])
* 10:56: Icinga recoveries starts to be fired
* 10:59: Database throughput back to normal levels '''OUTAGE ENDS'''
* 11:01: Last Icinga recovery","=== What went well? ===
* Automated monitoring detected the incident before humans did
* Once the problematic query was identified, it was immediately linked to the previous incident
* Once the extension was disabled + query killer running on s3, everything came back up","High-level discussion and brainstorming is happening in [[phab:T287380|T287380: Decide on the future of DPL]]. Out of that, some specific actionables have been identified: 

* {{done}} {{Gerrit|708390}} Send queries to ""vslow"" database group
* {{Done}} [[phab:T287916|T287916]] Disable DPL on wikis that aren't using it
* {{stalled}} [[phab:T263220|T263220]] Limit concurrency of DPL queries
*[[phab:T287983|T287983]] Raw ""upstream connect error or disconnect/reset before headers. reset reason: overflow"" error message shown to users during outage
*{{In progress}} [[phab:T288180|T288180]] Investigate [[mw:Extension:GoogleNewsSitemap|Extension:GoogleNewsSitemap]] query performance
**[[phab:T288227|T288227]] Add concurrency limiting to GoogleNewsSitemap using PoolCounter
* Maybe it would be cool if job runners slow down if a DB overload is detected
*Consider removing ordermethod=created as an unnecessary footgun
* ...",2021-07-26,Unknown,Unknown,Mediawiki,Unknown,Unknown,2021-07-26_ruwikinews_DynamicPageList.wikitext
"At 02:05 UTC, the parse2007 server in Codfw started to spontaneously respond with fatal error, possibly due to a php-opcache corruption. At 11:20 UTC, the server was restarted per the (now common) procedure in response to such corruptions after which the errors immediately stopped. Other parse servers and MW servers were not affected.  

'''Impact''': For 9 hours, 10% of submissions to Parsoid to parse or save wiki pages were failing on all wikis.

{{TOC|align=right}}",,Human reporting an error.,"[[File:2021-09-01 Parsoid fatals.png|thumb|Fatal errors in Logstash.]]
[[File:2021-09-01 Parsoid POST 5xx.png|thumb|Failing POST requests in Grafana.]]

'''All times in UTC.'''

*02:05: '''Outage starts''' (Retroactively) Logs indicate that <code>parse2007: Cannot declare class XWikimediaDebug in XWikimediaDebug.php</code> started at this time. This is affecting about 10% of POST requests
*03:54: mmodell, as [[Deployments/Train|train conductor]] this week, notices the spike and creates [[phab:T290120|T290120]]. 
*12:00 Krinkle finds the report during routine triage, notices that it is still on-going at high frequency, still unattended, and investigates.
*12:10: The issue is investigated and understood to be a likely opcache corruption, given that the source code in question has not changed recently and is not known to be defined or referenced in an unusual manner. The usual procedure for opcache corruptions is to restart php-fpm.
*12:19: Effie restarts php-fpm on parse2007
*12:20: '''Outage ends'''","===What went well?===

* A simliar issue was found a year earlier at [[phab:T245183#6212267|T245183#6212267]] (and remembered today), which let us jump to the opcache suspicion and thus restart the php-fpm service as mitigation.
* Logstash dashboards.
* Grafana dashboards.","* <s>Let deployers ssh to parse hosts. [[phab:T290144|T290144]]</s>
* TODO: Re-evaluate alerting strategy around ""mediawiki-exceptions"". We have a breakdown by cluster (web,api,job,parsoid). Do we need a breakdown by HTTP verb? (E.g.  ""read"" GET/HEAD/OPTIONS vs write ""POST/DELETE"" or some such).
* TODO: I was unable to find stats on error rates of api.php requests in Grafana. HTTP-level app server stats are insufficient since api errors are HTTP 200. The Graphite metrics for [https://grafana-rw.wikimedia.org/d/000000559/api-requests-breakdown API req breakdown] don't measure errors currently. The Logstash data for api-appservers errors is also insufficient since properly handled errors wouldn't be exceptions and wouldn't be logged there as such (e.g. when action=visualeditoredit finds Restbase/Parsoid respond with http 500, it responds to the client with an error. Where do we measure this?)

<mark>TODO: Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] Phabricator tag to these tasks.</mark>",2021-09-01,Unknown,Unknown,Unknown,Unknown,Unknown,2021-09-01_partial_parsoid_outage.wikitext
"An increase in load on a database server resulted in many queries being much slower to respond. This in turn meant backend traffic occupies appserver php-fpm workers for much longer, and a proportion of those requests will fail entirely due to unavailable workers. The failed requests got an error page with the message ""''upstream connect error or disconnect/reset before headers. reset reason: overflow''"".

'''Impact''': For 37 minutes, backends were slow (taking several seconds to respond) and 2% of requests failed entirely. This affected logged-in users, most bots/API queries, and some page views from unregistered users for pages that were recently edited or otherwise expired from the CDN cache.

'''Documentation''':

* Public task about incident. [[phab:T290373|T290373]]
*[https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?orgId=1&from=1630721700000&to=1630725300000&var-datasource=codfw%20prometheus%2Fops&var-cluster=appserver&var-method=GET&var-code=200 Grafana: Application RED dashboard]
<gallery mode=""nolines"" widths=""200"">
File:4-Sep-2021-http-status.png|HTTP error rates.
File:4-Sep-2021-latency.png|Latency buckets.
File:4-Sep-2021-latency-quantile.png|Latency quantile estimates.
</gallery>",,,,,* [[phab:T277416|T277416]] (restricted),2021-09-04,Unknown,Unknown,Unknown,Unknown,Unknown,2021-09-04_appserver_latency.wikitext
"A MediaWiki API outage happened two days earlier on 2021-09-04. It was correlated back then to a rise in HTTP 503s returned by the Wikifeeds service for the next 2 days. The issue was particularly subtle since only some requests ended up in HTTP 503, so the service health checks failed intermittently for a brief amount of time every now and then during the weekend, only noticed the next Monday. A rolling restart of the Wikifeeds Kubernetes pods reportedly restored the service to a healthy status. 

While the possibility that the Mediawiki API outage is related remains, it was discovered that end-user traffic to the Wikifeeds service more than quadrupled (4x) at peak, violating the traffic [[Wikifeeds#Service level indicators/objectives (SLIs/SLOs)|Wikifeeds SLO]] leaving the service in a degraded state and returning many 503 and 504s. The tls-proxy container was throttled at times during the peaks of the incident, causing the [https://grafana.wikimedia.org/d/lxZAdAdMk/wikifeeds?viewPanel=102&orgId=1&from=1630708200000&to=1631010300000 timeouts].  

'''Impact:''' For 3 days, the Wikifeeds API failed about 1% of its requests (e.g. 5 of every 500 per second).  {{TOC|align=right}}",,"The detection of the issue happened two days after it started, thanks to a service-checker icinga alert:<syntaxhighlight lang=""irc"">
17:32 +<icinga-wm> PROBLEM - wikifeeds codfw on wikifeeds.svc.codfw.wmnet is CRITICAL: /{domain}/v1/page/featured/{year}/{month}/{day} 
                   (retrieve title of the featured article for April 29, 2016) is CRITICAL: Test retrieve title of the featured article for 
                   April 29, 2016 returned the unexpected status 504 (expecting: 200): 
                   /{domain}/v1/media/image/featured/{year}/{month}/{day} (retrieve featured image data for April 29, 2016) is CRITICAL: 
                   Test retrieve featu
17:32 +<icinga-wm> e data for April 29, 2016 returned the unexpected status 504 (expecting: 200): 
                   /{domain}/v1/page/most-read/{year}/{month}/{day} (retrieve the most-read articles for January 1, 2016 (with 
                   aggregated=true)) is CRITICAL: Test retrieve the most-read articles for January 1, 2016 (with aggregated=true) returned 
                   the unexpected status 504 (expecting: 200) https://wikitech.wikimedia.org/wiki/Wikifeeds
</syntaxhighlight>This alert fired and self recovered every now and then during the weekend, where attention to IRC reported errors is lower, and it got noticed by one SRE by chance while looking at the #wikimedia-operations IRC channel. A lot of time was spent trying to figure out how the service worked, how to reproduce the problem and what could be the root cause of it.","'''All times in UTC.'''

* 2021-09-04T02:40 - '''OUTAGE BEGINS'''
* 2021-09-06T17:32 - One SRE starts to investigate the problem after noticing an icinga alert about service-checker failures on #wikimedia-operations (two more SREs will join during the subsequent hour).
*2021-09-06T18:30 - https://phabricator.wikimedia.org/T290445 is created with a list of possible suspects to look into.
*2021-09-06T20:00 - The three SREs in Europe working on the problem (US holiday) decided to reconvene the next morning, the impact for the service seemed not worth a page.
*2021-09-07T07:08 - One Software engineer notices a recurrent stacktrace listed in the logs, and files a patch for it (turned out to be a red herring, more details [[phab:T290445#7335379|T290445#7335379]]). 
* 2021-09-07T07:13 - A connection is made between the MediaWiki API outage timings and the rise of HTTPS 503s. As consequence, a roll restart of all the Wikifeeds Kubernetes pods is executed - https://sal.toolforge.org/log/Zg0av3sB1jz_IcWuiMhu -  '''OUTAGE ENDS'''
*2021-09-28T13:26 - After an incident review across multiple days, an engineer figures out that the errors were the result of increased end-user traffic generated by the Wikipedia App. The envoy based tls-proxy was also mildly [https://grafana.wikimedia.org/d/lxZAdAdMk/wikifeeds?viewPanel=102&orgId=1&from=1630708200000&to=1631010300000 throttled] during the incident, causing the timeouts. Log and metrics investigation did not reveal any other sign of envoy based tls-proxy misbehaving.

Metrics related to the above time window: https://grafana.wikimedia.org/d/lxZAdAdMk/wikifeeds?orgId=1&from=1630597675079&to=1630985454607

Wikifeeds tls-proxy (envoy) logs related to the above time window: https://logstash.wikimedia.org/goto/22555b41f9b7bbd2829081a041d76bb2","The main pain point was surely to identify what systems are involved in handling a request for the Wikifeeds API, and how to reproduce one error case. The documentation on Wikitech is good but generic, and there were some important details that not all SREs involved had clear in mind (one above all, the fact that a Wikifeeds request involves Restbase, Wikifeeds on Kubernetes, and possibly again Restbase to fetch some auxiliary data). Due to the unlucky circumstances (weekend plus US holiday) there were fewer SREs available to work on the problem, and it was not clear if it was worth a page or not. This situation will probably get better when we'll introduce SLOs and more formal on-call process.","* Add (more) documentation to the Wikifeeds Wikitech page to describe what is the relationship between Restbase, Wikifeeds and its tls-proxy container. [[phab:T291912|T291912]]
*Increase the capacity for wikifeeds [[phab:T291914|T291914]]
*Originally, it was deemed as a worthy actionable to find a way to reproduce the Envoy inconsistent state, or check if there is already literature/links/etc.. about it. However, since this was related to the greatly increased end-user traffic levels and since the incident review and investigation did not manage to reveal signs of envoy having an inconsistent state, this should not happen",2021-09-06,Unknown,Unknown,Unknown,Unknown,Unknown,2021-09-06_Wikifeeds.wikitext
"[[File:Cp3065-no-threads.png|thumb|Varnish on cp3065 running out of work threads]]
[[File:Cp3053-ats-be-connections.png|thumb|Thousands of connections established on ats-be@cp3053]]
On 2021-09-12 starting at 18:13 UTC, the cache upload cluster at esams (upload-lb.esams.wikimedia.org) was affected by an outage that was the result of a large image being hotlinked by what appears to be multiple Romanian news organization judging from HTTP referer values. The sudden surge in traffic was sent to a single ATS backend instance, [https://grafana.wikimedia.org/d/6uhkG6OZk/ats-instance-drilldown?viewPanel=66&orgId=1&from=1631468644595&to=1631474529640&var-site=esams%20prometheus%2Fops&var-instance=cp3053&var-layer=backend saturating the 10G NIC of the host between 18:06 and 18:08] and later triggering an anomalous behavior on 5 Varnish frontend instances in esams out of 8. The behavior consisted in the establishment of [https://grafana.wikimedia.org/d/wiU3SdEWk/cache-host-drilldown?viewPanel=100&orgId=1&from=1631468644595&to=1631474529640&var-site=esams%20prometheus%2Fops&var-instance=cp3055 thousands of connections] from all affected Varnish instances to a single cache backend. 

At the same time, all instances showing this behavior [https://grafana.wikimedia.org/d/wiU3SdEWk/cache-host-drilldown?viewPanel=99&orgId=1&from=1631467575412&to=1631474057976&var-site=esams%20prometheus%2Fops&var-instance=cp3055 quickly reached the maximum configured number of work threads] (24K given the configuration settings: thread_pools=2, thread_pool_max=12000). Without available work threads, the instances could not serve responses anymore. The error condition was resolved by a rolling restart of all Varnish instances in the upload cluster at esams. The incident was closed at 18:43 UTC. 

Between 18:13 and 18:37, a high number of web requests against the upload cache cluster at Esams [https://grafana.wikimedia.org/d/000000479/frontend-traffic?viewPanel=12&orgId=1&from=1631467575412&to=1631474057976&var-site=esams&var-cache_type=upload&var-status_type=5 resulted in server errors], up to about 15K req/s at peak. The amount of requests received from Romanian IPs against the upload cache cluster in Esams doubled, and the amount of data sent as response to requests from Romanian IPs increased about 50 times.","For 20 minutes, images and other media files were unavailable for many clients in countries routed to Esams (e.g. Europe), with up to 15,000 failed requests at peak. This affected all wikis, with gaps in articles where an image should be.","The SRE team was notified about the issue by a ""ATS TLS has reduced HTTP availability"" page, as well as IRC and email notifications.","'''All times UTC'''
* 18:13: Notification of the problem by alerts: ""alert1001/ATS TLS has reduced HTTP availability #page is CRITICAL"" and ""Possible DDoS to upload-lb.esams.wikimedia.org 91.198.174.208"". Discussion on #mediawiki_security channel started.
* 18:13 <+icinga-wm> PROBLEM - ATS TLS has reduced HTTP availability #page on alert1001 is CRITICAL: cluster=cache_upload layer=tls https://wikitech.wikimedia.org/wiki/Cache_TLS_termination https://grafana.wikimedia.org/dashboard/db/frontend-traffic?panelId=13&fullscreen&refresh=1m&orgId=1
* 18:13 <+icinga-wm> PROBLEM - Varnish HTTP upload-frontend - port 3127 on cp3061 is CRITICAL: HTTP CRITICAL - No data received from host https://wikitech.wikimedia.org/wiki/Varnish
* 18:13 <+icinga-wm> PROBLEM - Varnish HTTP upload-frontend - port 3123 on cp3057 is CRITICAL: CRITICAL - Socket timeout after 10 seconds https://wikitech.wikimedia.org/wiki/Varnish
* 18:13 <+icinga-wm> PROBLEM - Varnish HTTP upload-frontend - port 3120 on cp3057 is CRITICAL: CRITICAL - Socket timeout after 10 seconds https://wikitech.wikimedia.org/wiki/Varnish
* 18:18: First report that this was possibly due to Android user-agents fetching a single image.
* 18:22: vgutierrez records ""cp3057 ats-tls is unable to connect with varnish-frontend"".
* 18:24: Hotlinked image identified, including the Android app from the referer.
* 18:25: vgutierrez suggests he is going to restart varnish-fe on cp3057 to try to see if this resolves the problem.
* 18:26 < vgutierrez> !log restart varnish on cp3057
* 18:27 <+icinga-wm> RECOVERY - Varnish HTTP upload-frontend - port 3127 on cp3057 is OK: HTTP OK: HTTP/1.1 200 OK - 472 bytes in 0.162 second response time https://wikitech.wikimedia.org/wiki/Varnish
* 18:27 <+icinga-wm> RECOVERY - Varnish HTTP upload-frontend - port 3125 on cp3057 is OK: HTTP OK: HTTP/1.1 200 OK - 472 bytes in 0.165 second response time https://wikitech.wikimedia.org/wiki/Varnish
* 18:31: vgutierrez reports that restarting worked and he will do a rolling restart of varnish-fe on upload@esams.
* 18:33 < vgutierrez> !log restart varnish-fe on cp3061, cp3063 and cp3065
* 18:33: Recovery observed; 502 start dropping.
* 18:43: Incident resolved, error rates down, traffic back to normal values.","=== What went well? ===
* Incident detection
* The image got cached at the ATS backend layer, protecting both the Swift origin servers and the esams-codfw network link from excessive traffic
* Some Varnish frontends eventually cached the image thanks to the [https://phabricator.wikimedia.org/T275809#6984682 exp caching policy]
* Three nodes were not affected (cp3051, cp3053, cp3059)
* There were many (12) SREs around when the incident happened","* {{Status|TODO}} Prioritize work on single backend CDN nodes https://phabricator.wikimedia.org/T288106
* {{Status|TODO}} Investigate issue causing Varnish to establish thousand of connections to its origins / max out work threads / mmap counts (possibly different issues)
* {{Status|TODO}} Revisit maximum number of varnish-fe connections to origins (currently 50K) 

{{#ifeq:{{SUBPAGENAME}}|Report Template||
[[Category:Incident documentation]]
}}",2021-09-12,Unknown,Unknown,Unknown,Unknown,Unknown,2021-09-12_Esams_upload.wikitext
"Applying a change to our cirrus elasticsearch systemd unit triggered puppet to automatically restart the associated elasticsearch services. Sufficient number of hosts (6) were having puppet-agent ran simultaneously to drop the cluster into yellow and then red status; the large bandwidth/resources required to reschedule and assign shards when the respective elasticsearch services came back up on each host likely contributed to general cluster instability leading to a cascading failure scenario.

Elasticsearch itself did its job in coming back and immediately recovering shards, but given the volume of hosts restarted, it took about 40 minutes for the impact to users to become largely invisible, and 2 hours and 5 minutes for a full recovery of the backend (i.e. green cluster status).

* 16:18 Puppet change touching cirrus elasticsearch systemd units merged: https://gerrit.wikimedia.org/r/c/operations/puppet/+/720667
* 16:23 operator runs puppet manually on <code>elastic2052</code>, confirmed elasticsearch service came back up properly
* 16:25 operator runs puppet on rest of fleet, 6 hosts at a time: <code>sudo cumin -b 6 'P{elastic*}' 'sudo run-puppet-agent'</code>
* 16:29 <code>PROBLEM - PyBal backends health check on lvs1015 is CRITICAL: PYBAL CRITICAL - CRITICAL - search-psi-https_9643: Servers elastic1059.eqiad.wmnet, elastic1049.eqiad.wmnet, elastic1044.eqiad.wmnet, elastic1048.eqiad.wmnet, elastic1052.eqiad.wmnet, elastic1047.eqiad.wmnet, elastic1067.eqiad.wmnet, elastic1035.eqiad.wmnet, elastic1045.eqiad.wmnet are marked down but pooled https://wikitech.wikimedia.org/wiki/PyBal</code>
* 16:31 <code>ElasticSearch health check for shards on 9243 on search.svc.codfw.wmnet is CRITICAL.</code> At the same time, <code>production-search-codfw</code> has dropped into red cluster status
* 17:09 <code>enwiki</code> searches begin to work again (presumably due to recovery of the associated <code>enwiki</code> shards). User-visible impact decreases.
* 18:xx codfw enters green cluster status (full recovery)
* 18:xx eqiad enters green cluster status (full recovery)
* 18:30 MediaWiki api_appserver latency returns to normal levels
* 18:34 PoolCounter stops rejecting requests, user-visible impact ends.
[[File:2021-09-13 appserver latency due to cirrussearch outage.png|thumb|The api_appserver cluster saw higher latency during the outage.]]

'''Impact''': For about 2 hours (from 16:29 until 18:34) search requests on en.wikipedia.org (and likely other wikis) failed with ""''An error has occurred while searching: Search is currently too busy. Please try again later.''"". Search suggestions (from API opensearch) were absent or delayed. During the incident, the api_appserver cluster saw higher average latency overall due to the proportion of search queries.

'''Documentation''':
* Impact to shards during incident: https://grafana.wikimedia.org/d/000000455/elasticsearch-percentiles?viewPanel=64&orgId=1&from=1631549942000&to=1631558700000&var-cirrus_group=codfw&var-cluster=elasticsearch&var-exported_cluster=production-search&var-smoothing=1
*Impact on API opensearch queries: https://grafana.wikimedia.org/d/000000559/api-requests-breakdown?orgId=1&var-metric=&var-module=opensearch&from=1631521577658&to=1631588860150
* Impact on per-host CPU load averages during incident: https://grafana.wikimedia.org/d/000000455/elasticsearch-percentiles?viewPanel=63&orgId=1&from=1631549942000&to=1631558700000&var-cirrus_group=codfw&var-cluster=elasticsearch&var-exported_cluster=production-search&var-smoothing=1
* Impact to PoolCounter rejections (proxy for user impact): https://grafana.wikimedia.org/d/qrOStmdGk/elasticsearch-pool-counters?orgId=1&from=1631549942000&to=1631558700000
* [https://wm-bot.wmcloud.org/logs/%23wikimedia-operations/20210913.txt #wikimedia-operations log]",,,,,"* {{phab|T290902}} Iron out procedure to roll out cirrus/elasticsearch changes involving [implicitly via puppet or explicitly] service restarts
<!-- * <mark>To do #2 (TODO: Better understand under what conditions cirrussearch will issue the ""Search is currently too busy"" error...in this incident, even when in yellow cluster status, searches still failed)</mark> -->

<!--TODO: Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] Phabricator tag to these tasks.-->",2021-09-13,Unknown,Unknown,Unknown,Unknown,Unknown,2021-09-13_cirrussearch_restart.wikitext
"Slow database queries resulted in php-fpm worker exhaustion.

'''Impact''': For about 10 minutes, backends were slow or unavailable for all wikis. This affected logged-in users, most bots/API queries, and some page views from unregistered users (pages that were recently edited or otherwise expired from the CDN cache). 

'''Documentation''':

* Public incident task: [[phab:T291311|T291311]] 
* Similar to [[Incident documentation/2021-09-04 appserver latency]] and [[Incident documentation/2021-04-15 appserver latency]].",,,,,* [[phab:T284419|T284419]] (restricted),2021-09-18,Unknown,Unknown,Unknown,Unknown,Unknown,2021-09-18_appserver_latency.wikitext
"Increased db load for enwiki (s1) resulted in slower responses, which in turn resulted in overall php-fpm worker limits being reached and thus affecting requests for all wikis. For requests above the limit, the error was ""''upstream connect error or disconnect/reset before headers. reset reason: overflow''"".

'''Impact''': For about 15 minutes, backend appservers were slower or unable to respond for all wikis. This mainly affected logged-in users and most bot/API queries. Some page views from unregistered users were affected, for pages that were recently edited or otherwise expired from the CDN cache.  

'''Documentation''':

* Public incident task: [[phab:T291767|T291767]]

* Similar to [[Incident documentation/2021-09-04 appserver latency]] and [[Incident documentation/2021-09-18 appserver latency]].",,,,,"* [[phab:T291767|T291767]] (restricted)
* [[phab:T251885|T251885]] (restricted)",2021-09-26,Unknown,Unknown,Unknown,Unknown,Unknown,2021-09-26_appserver_latency.wikitext
"Network in the kubernetes cluster was briefly disrupted due to a rolling restart of calico pods. For 2 minutes, all service in kubernetes were unreachable. That manifested in elevated errors rates and latencies in mediawiki among other things as mediawiki relies on many of those service, e.g. sessionstore.

The incident was the result of an operator forcing a rolling restart of all pods in all namespaces (in order to pickup a docker configuration change), a process that has been done multiple times in the past and never caused issues. The process works by deleting the old pods one by one. It is without much risk mainly because:

* Each delete depools first the pod, draining it of traffic
* The pod is asked to stop first and then is killed and deleted
* In the meantime a new pod has been spun up, probably in a different node, has run the initialization process and is ready to receive traffic
* The initialization process of almost all our services is very fast.

However this time around, the process fell on a race condition when it entered the '''kube-system''' namespace, which contains the components that are responsible for the networking of kubernetes pods. All calico node pods were successfully restarted, however before they had enough time to initialize and perform their BGP peering with the Core routers (cr*-eqiad.wikimedia.org), a component they rely on called [https://docs.projectcalico.org/reference/typha/overview calico typha] was also restarted. Unfortunately we run only 1 replica of the typha workload. The new replica was scheduled in a new node that did not have the image yet on the OCI imagefs. Enough time elapsed while the image was being fetched and the new pod started by the kubelet that the graceful BGP timers on the core routers expired, forcing the core routers to withdraw from their routing tables the IPv4/IPv6 prefixes for the pod networks, leading to them becoming unreachable. The typha pod started up eventually, calico node pods managed to connect to it, fetched their configuration and BGP peered with the core routers re-announcing their pod IPv4/IPv6 prefixes. Routing was restored and the outage ended. No operator action was necessary to fix the issue, the platform auto-healed.

On the plus side, due to this (crude admittedly) form of chaos engineering, we found out a change in our kubernetes clusters and a need to amend our process.

'''Impact''': For 2 minutes fetches to mediawiki resulted in higher error rates, up to 8.51% for POSTs and elevated latencies for GETs (p99 at 3.82s). Some 1500 edits failed (although there is duplication there, those are not unique edits). Those seem to have happened later, when the services were restored. Kafka logging lag increased temporarily.

'''Documentation''':

* [https://grafana.wikimedia.org/d/000000208/edit-count?viewPanel=13&orgId=1&from=1632911315744&to=1632913750652 Save failures graphs]
* [https://grafana.wikimedia.org/d/000001590/sessionstore?orgId=1&var-dc=thanos&var-site=eqiad&var-service=sessionstore&var-prometheus=k8s&var-container_name=kask-production&from=1632912485692&to=1632912811172 Sessionstore graphs]
* [https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?orgId=1&from=1632911211615&to=1632913728120 Appservers graphs]",,,,,"* TODO: Document the rolling pod restart procedure and skip kube-system namespace
*<s>Investigate whether running >1 replicas of calico-typha is feasible and prudent. [[phab:T292077|T292077]]</s>",2021-09-29,Unknown,Unknown,Unknown,Unknown,Unknown,2021-09-29_eqiad-kubernetes.wikitext
,,,,,"*[[phab:T292792|2021-10-07 network provider issues causing all Wikimedia sites to be unreachable for many users]]
**[[gerrit:c/operations/puppet/+/727594|patch to make NEL alert paging]]
*Request RFO from provider",2021-10-08,Unknown,Unknown,Unknown,Unknown,Unknown,2021-10-08_network_provider.wikitext
,,,"* 19:38 Icinga reports issues with 1 interface going down on cr3-ulsfo
* 19:39 Icinga reports 2 interfaces down on cr2-eqord and 1 down on cr2-codfw
* 19:50 Telia mails us saying “we Suspected Cable fault in St Louis and your circuits are affected” and lists multiple circuits (turned out to be unrelated, red herring in the investigation)
* 20:06 '''a cable is patched in Eqiad ([[Phab:T293726#7452237|T293726#7452237]]) - incident & user impact begin here'''
* 20:09 <AntiComposite> reports of timeouts from a few users on Discord
* 20:10 <dontpanic> oh, I'm not the only one
* 20:14 Icinga starts reporting RIPE Atlas probes starting to be down
* 20:15 <mutante> XioNoX: users report issues right after a cable was patched in Eqiad but things work for me
* 20:15 <mutante> jclark-ctr: are you working with someone on that cable thing?
* 20:15 '''<+icinga-wm> PROBLEM - Too high an incoming rate of browser-reported Network Error Logging events #page on alert1001 is CRITICAL: type=tcp.timed_out https://wikitech.wikimedia.org/wiki/Network_monitoring%23NEL_alerts https://logstash.wikimedia.org/goto/5c8f4ca1413eda33128e5c5a35da7e28'''
* 20:24 Incident opened, Kunal becomes IC
* ... mostly looking into Telia transport links being down (red herring)
* 20:28 <bblack> the transport circuit, the interface appears to be down, so that's good at that level
* 20:32 <cdanis> saturation on transport links shouldn't be causing these flavors of NELs nor the RIPE Atlas alert
* 20:35 <bblack> https://librenms.wikimedia.org/device/device=2/tab=port/port=11600/ <bblack> equinix peering in eqiad, there's a dropoff in traffic, probably from telia fiber cut impacting other peers there?
* 20:39 < bblack> I think we've got some peers over that exchange which we're still advertising in one or both directions with, but are affected by telia somehow and the peering traffic is borked. 
* 20:41 < cdanis> it's a return path issue for sure
* 20:41 < XioNoX> !log disable sessions to equinix eqiad IXP ([https://sal.toolforge.org/log/GIK8qXwB8Fs0LHO5kUJR SAL entry]) '''user impact temporarily ends'''
* 20:41 <+icinga-wm> RECOVERY - BGP status on cr2-eqiad is OK: BGP OK - up: 74, down: 0, shutdown: 0 https://wikitech.wikimedia.org/wiki/Network_monitoring%23BGP_status
* 20:42 <greg-g> it's back <greg-g> my mtr is happy now
* 20:42 multiple users start to report things are working now and they can reach Gerrit
* 20:45 <greg-g> can't connect to the eqiad lb again '''change auto-rolls-back on the router; user impact begins again'''
* 20:45 <+icinga-wm> RECOVERY - Too high an incoming rate of browser-reported Network Error Logging events #page on alert1001 is OK: All metrics within thresholds. https://wikitech.wikimedia.org/wiki/Network_monitoring%23NEL_alerts https://logstash.wikimedia.org/goto/5c8f4ca1413eda33128e5c5a35da7e28
* 20:46 <cdanis> XioNoX: you did commit confirmed but didn't confirm <cdanis> it auto rolled back
* 20:46 Brandon depools eqiad in DNS ([https://sal.toolforge.org/log/bCTDqXwB1jz_IcWuZmcx SAL entry])
* 20:47 <XioNoX> cdanis: er, yeah commiting for real  '''user impact ends again and stays fixed'''
* 20:48 users report recoveries, again
* 20:49 <bblack> it takes several minutes for most to see a real impact from the dns-level depool, so any immediate recoveries are probably from re-committing the exchange fix
* 20:50 <mutante> are we sure this has nothing to do with it?  ""eqiad: patch 2nd Equinix IXP - https://phabricator.wikimedia.org/T293726 (Jclark-ctr) Cable has been run shows link.""
* 20:52 <XioNoX> yeah, that's most likely it
* 20:52 <cdanis> bug post is at 20:06 <cdanis> the NEL reports start at 20:06 exactly
* 20:54 <XioNoX> I disabled the interface on cr1, going to re-enabled the active on on cr2 ([https://sal.toolforge.org/log/ZILIqXwB8Fs0LHO5o1Ci SAL entry])
* 20:57 '''Cause identified and fixed, incident over'''
* 20:57 <bblack> !log re-pooling eqiad in DNS ([https://sal.toolforge.org/log/VSTLqXwB1jz_IcWutHH7 SAL entry])",,"* [[phab:T293726|T293726]]: Investigate & document why adding a new patch cable to the Equinix IX ports caused return path issues
* [[phab:T294166|T294166]]: NEL alert included #page in IRC but did not page through VictorOps (and then it did page through VO, at 21:07)
** This was because our outbound path to any internet destinations best reached via the Equinix IX were broken from (half of?) eqiad for the duration of the incident.
**Icinga also missed reporting when ""BGP status on cr2-eqiad"" went down, only reporting its recovery, our tentative theory is that it was also affected by the networking issues.
**Working around this is difficult but we should think about what to do.
* Remind people that using Klaxon when they're relatively confident something is wrong is preferred to waiting for automated monitoring to page.
** It's possible that Klaxon would also not work for the same users -- Klaxon is hosted in eqiad on alert1001.wikimedia.org, with the codfw installation being a passive backup host.
**It would have required someone on IRC to relay the issue through Klaxon, which had happened, just lack of actually using the tool.
*Have an incident replay, looking at both the technical aspects as well as procedural.
*[[phab:T295672|T295672]]: Use next-hop-self for iBGP sessions",2021-10-22,Unknown,Unknown,Unknown,Unknown,Unknown,2021-10-22_eqiad_return_path_timeouts.wikitext
,,,,,"* [[phab:T294490|T294490]]: db1112 being down did not trigger any alert that paged until the host was brought back up (we get paged for replication lag but not for host down, Marostegui said for DB hosts we should start paging on HOST down which we normally don't do. This would require a puppet change.)",2021-10-25,Unknown,Unknown,Unknown,Unknown,Unknown,2021-10-25_s3_db_recentchanges_replica.wikitext
,,"Some Grafana dashboards backed by Graphite showed partial data (starting Oct 11 or Oct 21) for a subset of metrics, as reported by  Lucas Werkmeister in https://phabricator.wikimedia.org/T294355","'''All times in UTC.'''

* Oct 11 11:45 reimage of graphite2003 https://phabricator.wikimedia.org/T247963#7416306
* Oct 18 11:30 backfill of graphite 2003 https://phabricator.wikimedia.org/T247963#7435382
* Oct 19 failover from graphite1004 to graphite2003
* Oct 21 10:24 reimage of graphite1004 and data backfill from graphite2003 https://phabricator.wikimedia.org/T247963#7447084
* Oct 25 failover from graphite2003 to graphite1004 https://phabricator.wikimedia.org/T247963#7455052
* Oct 26 16:50 report of missing metric data in https://phabricator.wikimedia.org/T294355","The <tt>whisper-sync</tt> backfill process is not as reliable as previously thought, no visible errors were logged and/or detected.","* Understand the feasibility (and need) to back up a small subset of important metrics https://phabricator.wikimedia.org/T294355#7464552
* Revise the backfill procedure to be more robust in the face of similar failures in the future (e.g. run a full rsync first, then backfill only the gap) https://phabricator.wikimedia.org/T296295
* Perform validation post-sync / post-backfill to check the number of datapoints across all metric files is roughly in sync between hosts https://phabricator.wikimedia.org/T296295
*Continue (and speed up) the Graphite retirement plan https://phabricator.wikimedia.org/T228380",2021-10-29,Unknown,Unknown,Unknown,Unknown,Unknown,2021-10-29_graphite.wikitext
,,,,,"* Incident tracking task, [[phab:T294853|T294853]]

* Improve automated testing and monitoring of cloud networking, [[phab:T294955|T294955]]
* Set up static route for cr-codfw, [[phab:T295288|T295288]]
* Avoid keepalived flaps when rebooting servers, [[phab:T294956|T294956]]",2021-11-02,Unknown,Unknown,Unknown,Unknown,Unknown,2021-11-02_Cloud_VPS_networking.wikitext
"{{Incident scorecard
| task = T299965
| paged-num = 
| responders-num = 
| coordinators = 
| start = 
| end = 
| impact = For 9 months, editors were entirely unable to upload large files (e.g. on Wikimedia Commons). The UploadWizard feature, and other upload tools, displayed generic error messages about this failure, typically after a timeout was reached. Editors reportedly gave up, or sacrificed image, video, and document quality (which provides a poorer user experience to readers), or resorted to manual ""server-side upload"" requests (which demanded additional sysadmin time during these months).
}}
Since the upgrade of MediaWiki appservers to Debian Buster early in 2021, large file uploads (anecdotally anything over 300MB) have been failing because of timeouts when uploading the file to Swift cross-datacenter (all uploads are sent to both primary [[Clusters|datacenters]]). The cause was determined to be the libcurl upgrade enabling HTTP/2 by default, which is generally slower at these kinds of transfers than HTTP/1.1 or HTTP/1.0 is (see [https://blog.cloudflare.com/delivering-http-2-upload-speed-improvements/ this Cloudflare blog post] for a brief explainer). Forcing internal requests from MediaWiki PHP to Swift to use HTTP/1 immediately resolved the issue. We have since also disabled HTTP/2 more generally in the internal Nginx proxy we use for TLS termination to Swift. The nginx puppet code we used here was originally written for [[MediaWiki at WMF|public traffic TLS]] termination (this now uses ATS), which explains why it had HTTP/2 enabled.{{TOC|align=right}}",,"The issue was first detected as a train blocker because the timeouts were causing database query errors on the MediaWiki side. Further manual investigation discovered the timeouts in MediaWiki/Swift communication.

In this specific case, alerting would not have helped given that the issue was noticed almost immediately, there was just no one actively looking into it for a significant amount of time.

For the general case, having metrics about throughput of uploads to Swift for large files and alerting (non-paging) based on that average might be useful (see actionables).","*Feb 11: [[phab:T274589|T274589: No atomic section is open (got LocalFile::lockingTransaction) filed]] 
*Feb 25: [[phab:T275752|T275752: Jobrunner on Buster occasional timeout on codfw file upload]] filed
*March 1: Link between file timeouts and buster migration identified (or at least theorized)
*mid-March: video2commons is unblocked by YouTube, and people attempt to upload large files at a much higher rate than before
*Late March-early April: One jobrunner is temporarily reimaged back to stretch and it appears to not show the same timeout symptoms.
*April 13: [https://commons.wikimedia.org/w/index.php?title=Commons%3AMaximum_file_size&type=revision&diff=552419451&oldid=552148334 Help:Maximum file size on Commons edited] to say that files over 100MB need a server-side upload (discussion ensues on the talk page)
*mid-April: Remaining servers reimaged to buster, including that one jobrunner.
*Oct. 11: [[phab:T275752#7418759|priority raised to high]], with this being the most likely cause of failing uploads 
*... slow progress in figuring out a reproducible test case
*Oct. 27: [https://lists.wikimedia.org/hyperkitty/list/wikimedia-l@lists.wikimedia.org/message/2UARCU7VH27P7URI7YVS5QQRXHAVNRBQ/ <nowiki>[Wikimedia-l] Upload for large files is broken</nowiki>]
*Oct. 28: A reproducible test case shows that from the same host, the CLI curl command works fine, while PHP via libcurl does not
**libcurl is transferring data using very small requests, causing lots of round-trips which add up for cross-datacenter requests
**[[phab:T275752#7467700|It's noticed]] that CLI curl is using HTTP/1, while PHP is using HTTP/2. Forcing PHP to use HTTP/1 fixes the issue.
**It is identified that:
***libcurl 7.62.0 enabled HTTP/2 multiplexing when available, which arrived as part of the Buster upgrade (7.52.1 to 7.64.0)
***A prerequisite of above was to also [https://curl.se/mail/lib-2018-07/0027.html switch by default to HTTP/2] when available.
**Patches to force SwiftFileBackend's MultiHttpClient to use HTTP/1 are submitted 
*Oct. 29: Patches are deployed, large file uploads being working again, at a much faster/expected speed
*Nov. 2: HTTP/2 disabled on nginx instances that provide TLS termination in front of Swift
*Nov. 8: HTTP/2 disabled on remaining nginx instances that use the tlsproxy::localssl Puppet role.","===What went well?===
*Once the underlying issue was identified (use of HTTP/2), we were able to prepare patches right away and deploy them in less than a day.","'''The biggest issue identified here is needing a maintainer/owner for backend file upload/management related components.'''

* There is a general need for better metrics collection and logging of the individual actions in SwiftFileBackend.

* [[phab:T283045|T283045: LocalFile::lock() rearchitecting]] - don't hold a database transaction open for the duration of the upload
* [[phab:T295008|T295008: MediaWiki uploads files to Swift in eqiad and codfw in serial, not parallel]]
*[[phab:T295482|T295482: Track throughput of large file uploads to Swift]]
* [[gerrit:735433|Log headers in MultiHttpClient]]",2021-11-04,Unknown,Timeouts,Mediawiki,Unknown,Unknown,2021-11-04_large_file_upload_timeouts.wikitext
,,"The issue was first detected by users of Chinese Wikipedia.  There was no automated monitoring.

On investigation, there '''do not appear to be any parser tests or other test cases which exercise language conversion on the table of contents''' ([[phab:T295187]]) or which verify the correct operation of the WikidataPageBanner extension ([[phab:T295003]]).

Rollback removed the table of contents from many articles on all wikis; this was also not detected by any monitoring.

Rollback ''did'' cause <u>spurious</u> (unrelated to the table of contents issue) alerts, as discussed above: [[phab:T295079]].","===September 15===
* Work begins on table of contents patch [[gerrit:721115]] (initial author: [[user:Jdlrobson]], with [[user:cscott]] reviewing, but quickly becomes a joint effort)","This incident exposed weaknesses in test coverage of the Table of Contents, and in the way that Parser Cache content interacts with our deployment and versioning systems.  Content stored in RESTBase has the potential for similar issues, as discussed below, but has slightly better purging and versioning systems to allow prevention and/or mitigation of version mismatch issues such as these.

In addition, procedural weaknesses were exposed in flagging potentially ""risky"" patches, and in the forum used for rollback discussions.  A related issue is that, due to time zone skew, detecting and reacting to failures in Chinese Wikipedia (deployed late UTC time on Thursday) can easily push timelines past 5pm local time on a Friday for engineers involved in the response.  The community involved in the smaller group 1 projects, like Chinese Wikivoyage, would in theory have noticed both ToC problems a full day earlier, but those community members did not successfully relay the issue to WMF staff.  It may be advisable to move Chinese Wikipedia from group 2 to group 1 in order to accelerate detection and response to issues.","Technical changes:

* Add test cases which exercise language conversion on the table of contents [[phab:T299973|T299973]]
* Add test cases for WikidataPageBanner extension, esp for the table-of-contents replacement code (selenium tests?) [[phab:T299974|T299974]]
* Create and document on wiki a test and deploy plan for changes to Parser Cache contents: (<mark>TODO: Phab ticket  [[phab:project/view/4758/|#Sustainability (Incident Followup)]] )</mark>
** Patch which adds ""future compatibility"" (to handle ""future"" parser cache contents) should land first and roll out with the train, *before* any changes to parser cache contents are made.  This should have test cases verifying correct behavior during roll back (that is, correct behavior if ""parser cache contents from the future"" are encountered).  (These test cases were absent in this incident, since this split was not done.)
** A subsequent patch which *changes* parser cache contents to the ""future"" form can roll out in a separate train; this will make it more likely that this train can be safely rolled back if issues are found.  This should have test cases verifying correct behavior during roll forward (that is, correct behavior if ""parser cache contents from the past"" are encountered).  (These test cases were present in this incident, which is a ""what went well"".)
** Documentation for ParserOutput::getText() in particular should reference this plan to provide guidance around how best to make backwards compatible changes; it should note that such changes should be always be flagged as potentially risky to the SREs and have communicated a rollback plan.
** Make this plan relevant to DiscussionTools and other users of parser cache contents as well.
* Create and document on wiki a test and deploy plan for changes to RESTBase contents, including documenting [[user:ppchelko]]'s ""five lines of code"" to allow for purge during rollback.  (<mark>TODO: Phab ticket  [[phab:project/view/4758/|#Sustainability (Incident Followup)]] )</mark>
* Allow earlier visibility for issues involving language variants by some means, such as by moving zhwiki or srwiki from group2 to group1.  (<mark>TODO: Phab ticket  [[phab:project/view/4758/|#Sustainability (Incident Followup)]])</mark>
* Enable pig latin variant on English Wikipedia beta cluster ([[phab:T299975|T299975]]). This has two benefits:
** It allows easier testing of variant-related issues in beta, for example to validate a patch on master before cherry-picking it to production.
** It possibly also facilitates earlier visibility of issues involving language variants, although effectiveness requires someone to activity monitor the status of this wiki.

Process changes  (<mark>TODO: Phab ticket (??) Add   [[phab:project/view/4758/|#Sustainability (Incident Followup)]] )</mark>

* Revise communication protocols based on the current expectations of communication medium of WMF employees. For example, if product engineers are required to be available on IRC, that should be communicated broadly. If that's not a requirement, we should perhaps use email/Phabricator as primary communication",2021-11-05,Unknown,Monitoring,Wikidata,Unknown,Unknown,2021-11-05_TOC_language_converter.wikitext
"The metadata is aimed at helping provide a quick snapshot of context around what happened during the incident.
{| class=""wikitable""
|'''Incident ID''' 
|2021-11-10 cirrussearch commonsfile outage
|'''UTC Start Timestamp:'''
|YYYY-MM-DD hh:mm:ss
|-
|'''Incident Task'''
|https://phabricator.wikimedia.org/T299967 
|'''UTC End Timestamp'''
|YYYY-MM-DD hh:mm:ss
|-
|'''People Paged'''
|<amount of people> 
|'''Responder Count'''
| <amount of people>
|-
|'''Coordinator(s)'''
|Names - Emails
|'''Relevant Metrics / SLO(s) affected'''
|Relevant metrics
% error budget 
|-
|'''Impact:'''
| colspan=""3"" |For about 2.5 hours (14:00-16:32 UTC), the Search results page was unavailable on many wikis (except for English Wikipedia). On Wikimedia Commons the search suggestions feature was unresponsive as well. 
|}On 10 November, as part of verifying a bug report, a developer submitted a high volume of search queries against the active production Cirrus cluster (eqiad cirrussearch) via a tunnel from their local mw-vagrant environment. <code>vagrant provision</code> was (probably) later run without the tunnel being properly closed first, which resulted in (for reasons not yet understood) the deletion and recreation of the <code>commonswiki_file_1623767607</code> index.

As a direct consequence, any Elasticsearch queries that targetted media files from commonswiki encountered a hard failure.

During the incident, all media searches on Wikimedia Commons failed. Wikipedia projects were impacted as well,<ref>[https://logstash.wikimedia.org/goto/73a9d7e35f409c0d122888d42df94761 Log events of all affected requests] ('''note''': requires [[Logstash]] access)</ref> through the ""cross-wiki"" feature of the sidebar on Search results pages. This cross-wiki feature is enabled on most wikis by default, though notably not on English Wikipedia where the community disabled search integration to Commons.

Note that the search suggestions feature, as present on all article pages was not affected (except on Wikimedia Commons itself). The search suggestions field is how how most searches are performed on Wikipedia, and was not impacted. Rather, it impacted the dedicated Search results page (""Special:Search"", which consistently failed to return results on wikis where the rendering of that page includes a sidebar with results from Wikimedia Commons.",,,"* '''15:21''' First ticket filed by impacted user https://phabricator.wikimedia.org/T295478
* '''15:28''' Additional, largely duplicate ticket filed by user https://phabricator.wikimedia.org/T295480
* '''15:32''' <code><Dylsss> Searching for files on Commons is currently impossible, I believe this is quite critical given the whole point of Commons is being a file repository </code>
* '''15:52''' Initial attempt to shift cirrussearch traffic to codfw (did not work due to missing a required line in patch) (https://sal.toolforge.org/log/05mNCn0B1jz_IcWuO9iw)
* '''16:32''' Search team operator successfully moves all cirrussearch traffic to codfw, resolving user impact (https://sal.toolforge.org/log/8p2xCn0Ba_6PSCT9sorW)
* '''???''' Index successfully restored, and traffic is returned to eqiad",,"* Future one-off debugging of the sort that triggered this incident, when it requires production data, should be done on <code>cloudelastic</code>, which is an up-to-date read-only Elasticsearch cluster. If production data is needed but <= 1 week stale data is acceptable, <code>relforge</code> should be used instead.",2021-11-10,Unknown,Unknown,Unknown,Unresponsive,Unknown,2021-11-10_cirrussearch_commonsfile_outage.wikitext
,,,,,"* Icinga check for ipv6 host reachability, [[phab:T163996|T163996]]",2021-11-18,Unknown,Unknown,Unknown,Unknown,Unknown,2021-11-18_codfw_ipv6_network.wikitext
"{| class=""wikitable""
|'''Incident ID'''
|2021-11-23 Core Network Routing
|'''UTC Start Timestamp:'''
|YYYY-MM-DD hh:mm:ss
|-
|'''Incident Task'''
|[[phab:T299969|T299969]]
| '''UTC End Timestamp'''
|YYYY-MM-DD hh:mm:ss
|-
|'''People Paged''' 
| <amount of people> 
|'''Responder Count'''
|<amount of people> 
|-
|'''Coordinator(s)'''
|Names - Emails
|'''Relevant Metrics / SLO(s) affected'''
|Relevant metrics
% error budget
|-
|'''Summary:'''
| colspan=""3"" |For about 12 minutes, Eqiad was unable to reach hosts in other data centers (e.g. cache PoPs) via public IP addresses due to a BGP routing error. There was no impact on end-user traffic, and internal traffic generally uses local IP subnets that are routed with OSPF instead of BGP.
|}At approx 09:37 on Tues 23-11-2021 a change was made on cr1-eqiad and cr2-eqiad, to influence route selection in BGP.  The specific change was to remove a BGP setting which causes the BGP ""MED"" attribute to be set to the OSPF cost to reach the next-hop of the BGP route, as part of [[phab:T295672|T295672]]. This caused a change in how routes to certain remote sites were evaluated by the core routers there. At a high-level the change meant that the correct, externally-learnt routes for remote BGP destinations, suddenly looked less preferable than reflections of these routes devices in eqiad were announcing to each other.

For instance, cr2-eqiad was previously sending traffic for [[Eqsin cluster|Eqsin]] BGP routes via [[Codfw cluster|Codfw]]. But following the change BGP decided it was better to send this traffic to its local peer cr1-eqiad. Unfortunately, cr1-eqiad was a mirror image of this, and decided the best thing was to send such traffic to cr2-eqiad. A ""routing loop"" thus came into being, with the traffic never flowing out externally, but instead bouncing between the two [[Eqiad cluster|Eqiad]] routers. Alerts fired at 09:39, the first being due to Icinga in Eqiad failing to reach the public text-lb address in Eqsin (Singapore).  At 09:42 the configuration change was reverted on both devices by Cathal.  Unfortunately that did not immediately resolve the issue, due to the particular way these routes update on a policy change. After some further troubleshooting a forced reset was done to the BGP session between the eqiad routers at 09:51, which resolved the issue.

The impact was noticed by Icinga via its health checks against edge caching clusters like Eqsin. However, the Icinga alert was likely also the only impact as we generally don't explicitly target remote data centers over public IP. The exception to that is [[WMCS]] instances, but that runs within Eqiad and DNS generally resolves public endpoints to the same DC, not a remote one.

'''Impact''': For about 12 minutes, Eqiad was unable to reach hosts in other data centers (e.g. cache PoPs) via public IP addresses due to a BGP routing error. There was no impact on end-user traffic, and internal traffic generally uses local IP subnets that are routed with OSPF instead of BGP.",,,,,"*Ticket [[phab:T295672|T295672]] has been updated with further detail on what happened and a proposal to adjust our BGP sessions to address both the ""second IXP port"" issue and prevent this happening again in future. The changes were backed out so situation is back to what it was before the incident.",2021-11-23,Unknown,Unknown,Unknown,Unknown,Unknown,2021-11-23_Core_Network_Routing.wikitext
,,,,,"*automate maintenance and proper de-depooling of Kubernetes services using a cookbook [[phab:T277677|T277677]] and [[phab:T260663|T260663]]
*reduce snowflake services which need special treatment and make most/all of them active-active (for example [[phab:T288685|T288685]])
*optional: create a lvs/pybal/k8s service dashboard to see which service is pooled in which DC (will create a task)
*[[phab:T296699|T296699: Pool eventgate-main in both datacenters (active/active)]]",2021-11-25,Unknown,Unknown,Unknown,Unknown,Unknown,2021-11-25_eventgate-main_outage.wikitext
"The metadata is aimed at helping provide a quick snapshot of context around what happened during the incident.
{| class=""wikitable""
| '''Incident ID'''
|2021-12-03 mx
|'''UTC Start Timestamp:'''
|YYYY-MM-DD hh:mm:ss
|-
|'''Incident Task'''
| [[phab:T297127|T297127]]
|'''UTC End Timestamp'''
|YYYY-MM-DD hh:mm:ss
|-
| '''People Paged'''
|<amount of people>
|'''Responder Count'''
|<amount of people>
|-
| '''Coordinator(s)'''
|Names - Emails
|'''Relevant Metrics / SLO(s) affected'''
|Relevant metrics

% error budget
|-
|'''Impact:'''
| colspan=""3"" |For about 24 hours, a portion of outgoing email from wikimedia.org was delayed in delivery. This affected staff Gmail, and Znuny/OTRS/Phabricator notifications. No mail was lost, it was eventually delivered. 
|}
On November 24th the Linux kernel on one of our mail servers was upgraded. [https://sal.toolforge.org/log/qkFVUX0B8Fs0LHO5688M]

Then on November 30th changes were made to prioritization of mail servers used for wiki mail which sent more traffic than before to this server. [https://gerrit.wikimedia.org/r/c/operations/puppet/+/742757]

Due to a kernel bug in the iptables conntrack module packets from our mail server towards Google mail servers started to be dropped. [https://www.spinics.net/lists/stable/msg509296.html]

Outgoing mail became stuck in the outgoing queue. [https://grafana.wikimedia.org/d/000000451/mail?orgId=1&from=1638051489230&to=1638576494254]

After a while the number of mails in the queue started to trigger monitoring alerts [https://icinga.wikimedia.org/cgi-bin/icinga/extinfo.cgi?type=2&host=mx2001&service=exim+queue], [https://icinga.wikimedia.org/cgi-bin/icinga/notifications.cgi?host=mx2001&ts_start=1638748800&ts_end=1638820953&limit=0&type=0&order=new2old&timeperiod=lastweek&start_time=2021-12-06+00%3A00%3A00&end_time=2021-12-06+20%3A02%3A33] and internal users started to report to ITS about issues with mail.[https://phabricator.wikimedia.org/T297017]

SRE started to investigate and identified first the timeouts as a cause and then after some debugging that the cause for that was the firewall suddenly dropping packets. [https://phabricator.wikimedia.org/T297017#7547326]

A manual fix was applied to add extra firewall rules to allow these dropped packets. (ip6tables -I INPUT -s 2620:0:861:102:10:64:16:8 -j ACCEPT)

After this the outgoing mail stuck in the queue started to be sent out but it took some time to catch up. [https://grafana.wikimedia.org/d/000000451/mail?viewPanel=3&orgId=1&from=1638571161474&to=1638579596286]

At this point it was still unclear what the further root cause of the firewall change was but mail was being sent out again normally. SRE informed ITS about the ongoing process.

After further debugging, finding other bug reports and going through server admin logs it became clear that the change correlated nicely to the latest reboot which was done for a kernel upgrade.

Finding this and after the queue was fully processed SRE proceeded to deactivate the affected mail server and rebooted it in order to downgrade it to the previous kernel version.[https://sal.toolforge.org/log/wA7vgn0B1jz_IcWuoPxQ]

While doing this another unrelated issue appeared, the ganeti VM that is the mail server did not come back from the reboot.[https://sal.toolforge.org/log/zXEBg30B8Fs0LHO5knFE]

It failed to get an IP on its network interface and was online but could not be reached via SSH and had no networking at all. [https://tenor.com/view/bryan-cranston-replace-the-bulb-malcolm-in-the-middle-kitchen-hal-gif-11562300]

After a little while SRE identified this as a known issue with device renumbering on ganeti VMs and was able to get the server back up by editing network configuration manually via root console followed by another reboot.

At this point the server was now back up with the previous kernel version (5.10.0-8) and tests were made to see if the issue was gone. It was confirmed mail could be sent again with that kernel!

Some changes that had been reverted in Gerrit were re-reverted and a decision was made to keep the mailserver deactivated over the weekend.

Incident was closed, of course minus the follow-ups.{{TOC|align=right}}",,"It was detected by Icinga at: ""(2021-12-02 10:24:02 - Icinga notices the mail queue size grew over the alerting threshold of 2000 (is: 4013) and notifies IRC about it).""

This could be seen on IRC and on the Icinga web UI but did not send direct SMS or emails to specific people.

A follow-up will be to change this and make it page SRE ([[phab:T297144]]).

In parallel it was noticed by wikimedia.org staff who reported to ITS who created [[phab:T297017]].

[https://icinga.wikimedia.org/cgi-bin/icinga/notifications.cgi?host=mx2001&ts_start=1638748800&ts_end=1638820953&limit=0&type=0&order=new2old&timeperiod=lastweek&start_time=2021-12-06+00%3A00%3A00&end_time=2021-12-06+20%3A02%3A33 Icinga - Alert Notifications for mx2001]

[https://icinga.wikimedia.org/cgi-bin/icinga/extinfo.cgi?type=2&host=mx2001&service=exim+queue Icinga - exim queue size check on mx2001]","*2021-11-24 mail server mx2001 gets rebooted for a maintenance kernel upgrade
*(a bug in kernel 5.10.0-9 starts to affect firewall rules on the server but we don't know this yet because it has very low no traffic))
*2021-11-30 as part of other work, disabling LDAP callouts, more traffic than before is shifted to mx2001 [https://gerrit.wikimedia.org/r/c/operations/puppet/+/742757]
*2021-12-02 10:24:02 - Icinga notices the mail queue size grew over the alerting threshold of 2000 (is: 4013) and notifies IRC about it
*2021-12-02 11:56:26 - icinga notifies a second time (but only to IRC) that mail queue is now at  4042
* 2021-12-03 16:48 - ITS creates ticket T297017 because “Several staff members are getting intermittent Google Mail bounce backs.”
*2021-12-03 21:11 SRE investigates, identifies issue is timeout to Google servers. aspmx.l.google.com [142.250.114.27] DT=10m: SMTP timeout after end of data
*2021-12-03 22:10 issue is escalated to an incident
* 2021-12-03 23:11 SRE finds dropped packets, applies manual ip6tables command, confirms it fixes it, reverts manual fix, shortly after we disable puppet and the queue starts to go down (slowly)
*2021-12-03 ~ 23:18 We find out firewall rule is missing from RELATED,ESTABLISHED, shortly after that the issue appears to be in conntrack
*2021-12-03 ~ 23:28  we keep watching the queue size go lower, start to draft a response to ITS, keep debugging to identify what caused the change and becomes more and more clear it matches with the recent kernel upgrade. We try to guess how long it might take until the queue is fully sent.
*2021-12-04 ~ 00:22 we are discussing how to continue once the queue is down to 0. Decide to keep exim deactivated, reload ferm to close the holes we opened up manually
*2021-12-04 ~ 00:26 - OTRS starts alerting with systemd alerts, clamav randomly died after trying to scan some Windows malware. We get distracted and fix that too by restarting it.
*2021-12-04 ~ 00:42 - mail queue finished catching up, mail had been sent out, we stopped exim
*2021-12-04 ~ 00:46 - we decide to test rebooting the server with the previous kernel to confirm the kernel bug theory
*2021-12-04 ~ 00:55 - server does not come back from reboot, does not get IP on interface, is affected by ganeti device renumbering bug
*2021-12-04 - 01:10 - we manage to bring the server back up after connecting via ganeti console and root password, manually editing /etc/network/interfaces and replacing ens5 with ens13, rebooting again, and this time with the previous kernel version
*2021-12-04 - 01:11 - we are able to confirm sending out mail works with this kernel
*2021-12-04 - 01:14 - server is reachable via SSH again but spamd did not start, ferm did not start and a stale if-up, we fix those as well, iptables rules are in place again
*2021-12–04 - 01:26 - we decided to keep exim disabled over the weekend, masked the exim service again
*2021-12-04 - 01:32 - incident is declared to be over, of course pending follow-ups
*2021-12-07 - The long-term fix was to revert bullseye hosts from 5.10.70 to a previous kernel version [https://phabricator.wikimedia.org/T297180]
*2021-12-08 - mx2001 is back in service [https://phabricator.wikimedia.org/T297128]","*Monitoring exists but notifications need to be worked on.
* More specifically test sending out mail after reboots/upgrades on mail servers.","*[[phab:T297128]] - Bringing mx2001 back into service
*[[phab:T297144]] - large MX queues should page

<references />",2021-12-03,Unknown,Unknown,Unknown,Unknown,Unknown,2021-12-03_mx.wikitext
"{{Incident scorecard
| task = No task
| paged-num = 20+
| responders-num = 6+
| coordinators = 0
| start = 2022-01-31 16:05
| end = 2022-01-31 16:08
| impact = For 3 minutes, clients served by the ulsfo POP were not able to contribute or display un-cached pages. 8000 errors per minute (HTTP 5xx).
}}

A firewall change was pushed to ulsfo routers, which caused [[ulsfo]] to lose connectivity to the other POPs and core sites for 3min.",,,"* 16:03 - apply [[gerrit:c/operations/homer/public/+/748098/3|configuration change]] on cr3-ulsfo
* 16:05 - apply [[gerrit:c/operations/homer/public/+/748098/3|configuration change]] on cr4-ulsfo - '''outage starts'''
* 16:06 - Icinga notifies about connectivity issues to ulsfo - '''paging'''
* 16:08 - change rolled back - '''outage ends'''

'''Documentation''': 

* https://grafana.wikimedia.org/d/000000503/varnish-http-errors?from=1643645069528&to=1643645324222

'''Root cause:'''

The change incorrectly restricts [[:en:Bidirectional_Forwarding_Detection|BFD]] to BGP peers:<syntaxhighlight lang=""diff"">
+      term allow_bfd {
+          from {
+              source-prefix-list {
+                  bgp-sessions;
+              }
+              protocol udp;
+              port 3784-3785;
+          }
+          then accept;
+      }
</syntaxhighlight>While BFD is also used by OSPF sessions, which caused them to be tear down.

One surprising point is that the issue didn't show up in the verification commands (<code>show ospf interfaces</code>, <code>show ospf neighbors</code>), all neighbors are present.<syntaxhighlight lang=text>
show ospf interface   

Interface           State   Area            DR ID           BDR ID          Nbrs

ae0.2               PtToPt  0.0.0.0         0.0.0.0         0.0.0.0            1

et-0/0/1.401        PtToPt  0.0.0.0         0.0.0.0         0.0.0.0            1

xe-0/1/1.0          PtToPt  0.0.0.0         0.0.0.0         0.0.0.0            1

show ospf neighbor    

Address          Interface              State     ID               Pri  Dead

198.35.26.197    ae0.2                  Full      198.35.26.193    128    35

198.35.26.199    et-0/0/1.401           Full      198.35.26.194    128    33

198.35.26.209    xe-0/1/1.0             Full      208.80.154.198   128    34
</syntaxhighlight>While it was effectively down:

<code>rpd[16292]: RPD_OSPF_NBRDOWN: OSPF neighbor 198.35.26.209 (realm ospf-v2 xe-0/1/1.0 area 0.0.0.0) state changed from Full to Down due to InActiveTimer (event reason: BFD session timed out and neighbor was declared dead)</code>",,,2022-02-01,Unknown,Unknown,Unknown,Unknown,Unknown,2022-02-01_ulsfo_network.wikitext
"{{Incident scorecard
| task = T301147
| paged-num = 0
| responders-num = 0
| coordinators = 0
| start = 2022-02-06 23:00:00
| end = 2022-02-07 06:20:00
| metrics = WDQS Updater Lag, Wikidata MaxLag
| impact = For about 7 hours, WDQS updates failed to be processed. As a consequence, bots and tools were unable to edit Wikidata during this time.
}}
The streaming updater stopped to function properly because a k8s node misbehaved. More details at [[Incidents/2022-02-22 wdqs updater codfw]].

'''Documentation''':
*https://phabricator.wikimedia.org/T301147
For 7 hours (<code>2022-02-06T23:00:00</code> to <code>2022-02-07T06:20:00</code>) the streaming updater in <code>eqiad</code> stopped working properly preventing edits to flow to all the wdqs machines in eqiad.

The lag started to rise in eqiad and caused edits to be throttled during this period:

Investigations:

* the streaming updater for WCQS went down from <code>2022-02-06T16:32:00</code> to <code>2022-02-06T23:00:00</code>
* the streaming updater for WDQS went down from <code>2022-02-06T23:00:00</code> to <code>2022-02-07T06:20:00</code>
* the number of total task slots went down to 20 from 24 (4tasks == 1pod) between <code>2022-02-06T16:32:00</code> and <code>2022-02-07T06:20:00</code> causing resource starvation and preventing both jobs from running at the same time (<code>flink_jobmanager_taskSlotsTotal{kubernetes_namespace=""rdf-streaming-updater""}</code>)
* kubernetes1014 (T301099) seemed to have showed problems during this same period (<code>2022-02-06T16:32:00</code> to <code>2022-02-07T06:20:00</code>)
* the deployment used by the updater used one POD (<code>1db45eb6-2405-4aa3-bec1-71fcdbbe4f9a</code>) from kubernetes1014
* the flink session cluster was able to regain its 24 slots after <code>1db45eb6-2405-4aa3-bec1-71fcdbbe4f9a</code> came back (at <code>2022-02-07T08:07:00</code>), then this POD disappeared again in favor of another one and the service successfully restarted.
* during the whole incident k8s metrics & flink metrics seem to disagree:
** flink says that it lost 4 task managers (1 POD)
** k8s always reports at least 6 PODS (<code>count(container_memory_usage_bytes{namespace=""rdf-streaming-updater"", container=""flink-session-cluster-main-taskmanager""})</code>)

Questions (answered):

* why do flink and k8s metrics disagree (active PODs vs number of task manager)?
** Flink could not contact the container running on kubernetes1014 and thus freed it's resources (task slots), k8s attempted to kill the container as well but did not fully reclaim the resources (PODs) allocated to it
* why a new POD was not created after kubernetes1014 went down (making <code>1db45eb6-2405-4aa3-bec1-71fcdbbe4f9a</code> unavailable to the deployment)?
** From the k8s point of view kubernetes1014 was flapping between the ready and not ready state and preferred to reboot containers there

What could we have done better:

* we could have route wdqs traffic to codfw during the outage and avoid throttling edits

Action items:

* T305068: Alert if the number of flink tasks slots go below what we expect
* T293063: adapt/create runbooks for the streaming updater and take this incident into account (esp. we should have had reacted to the alert and routed all wdqs traffic to codfw)
* To be discussed with service ops:
** Investigate and address the reasons why after a node failure k8s did not fulfill its promise of making sure that the rdf-streaming-updater deployment have 6 working replicas
** If the above is not possible could we mitigate this problem by over-allocating resources (increase the number of replicas) to the deployment to increase the chances of proper recovery if this situation happens again?
* T277876: to possibly improve the resiliency of the k8s nodes",,,,,"*[[phab:T305068]]<mark>: alert when flink does not have the capacity it expects</mark>
*[[phab:T293063]]<mark>:adapt/create runbooks/cookbooks for the wdqs streaming updater</mark>",2022-02-06,Unknown,Unknown,Unknown,Unknown,Unknown,2022-02-06_wdqs_updater.wikitext
"{{Incident scorecard
| task = -
| paged-num = 0
| responders-num = 3
| coordinators = -
| start = 2022-02-22 08:00
| end = 2022-02-22 16:47
| impact = For 12 hours, incoming emails to a specific new VRTS queue were not processed with senders receiving a bounce with an SMTP 550 Error. It is estimated no ""useful"" emails were lost.
}}
<!-- Reminder: No private information on this page! -->

A stuck vrts aliases generating process on mx2001 resulted in rejects for dcw@wikimedia.org, a new VRTS queue.

On 2022-02-02 an SRE with long-time knowledge about VRTS received an email to their individual work address from a known VRTS admin, stating that a newly created VRTS queue ""dcw@wikimedia.org"" returned errors to some users that tried to use it (but not always, e.g. manual testing worked fine). The errors were of type SMTP 550 Error and looked as follows:
 208.80.153.45 does not like recipient.
 Remote host said: 550 Previous (cached) callout verification failure
A few hours later (by 2022-02-22 13:29), an investigation independently verified that email would not always be reliably sent to this VRTS email queue and the issue was escalated to a couple of other knowledgeable SREs. Given the incoming path and the fact that the only failing email address was a relatively new one not yet in widespread use, the incident was implicitly triaged as low priority. By 14:35 UTC it was verified again, adding more data points and a first theory formulated that our Google work email system was at fault as emails from other MTAs were sent out successfully but sending from wikimedia.org domains failed. However, by 16:47 UTC, it became clear that the <code>generate_otrs_aliases.service</code> systemd timer job was stuck and was not updating VRTS mailing lists/queues on mx2001 while it was running fine on mx1001 (that discrepancy explains why it was sometimes reproducible). After restart of the systemd timer job, the issue was fixed and the fix communicated to the VRTS admin.",,,,,"*Figure out why generate_otrs_aliases.service was stuck.
*Alert on a stuck generate_otrs_aliases.service.

<mark>TODO: Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFIRE (Pending Review & Scorecard)]] Phabricator tag to these tasks.</mark>",2022-02-22,Unknown,Unknown,Unknown,Unknown,Unknown,2022-02-22_vrts.wikitext
"{{Incident scorecard
| task = T302340
| paged-num = 0
| responders-num = 3
| coordinators = Ryan Kemper
| start = 2022-02-22 17:47:00
| end = 2022-02-22 19:27:00
| metrics = updateQueryServiceLag ([https://grafana.wikimedia.org/d/000000489/wikidata-query-service?viewPanel=8&orgId=1&var-cluster_name=wdqs&from=1645548333076&to=1645559701497 Grafana])
| impact = For about two hours, WDQS updates failed to be processed. As a consequence, bots and tools were unable to edit Wikidata during this time.
}}

WDQS updaters stopped processing updates in Codfw due to a failure with Flink in Codfw.

The [[mw:Manual:Maxlag_parameter|API maxlag feature]], is configured on Wikidata.org to incorporate WDQS lag. The updateQueryServiceLag service exists to transfer this datapoint from Prometheus to MW. Because bots generally opt-in to be friendly and enable the ""maxlag"" parameter, and because the metric was configured to consider both Eqiad and Codfw, their edits were rejected for two hours.",,,"2022-02-22:

'''17:30''' Search dev deploys a version upgrade (0.3.103) of the flink application to codfw for wdqs

'''17:31''' The flink application is unable to restore from the savepoint

'''17:51''' Search dev does not find any solution to unblock the situation and asks for a depool of wdqs@codfw (users no longer see stale results when hitting wdqs@codfw)

'''17:52''' (traffic switched to eqiad) <code><gehel> !log depooling WDQS codfw (internal + public) - issues with deployment of new updater version on codfw</code>

'''19:00''' wikidata maxlag alert is triggered eventhough codfw is depooled (known limitation: [[phab:T238751]])

'''19:20''' wdqs@codfw is removed from the wikidata maxlag calculation (bots can resume editing)

'''19:20''' Search dev rolls WDQS codfw flink state back to a previously saved [https://nightlies.apache.org/flink/flink-docs-release-1.14/docs/dev/datastream/fault-tolerance/checkpointing/ checkpoint ], restoring the processing of updates in WDQS. Within a few minutes lag catches up and the user impact resolves.

'''19:25''' <code><ryankemper> !log T302330 `ryankemper@cumin1001:~$ sudo -E cumin '*mwmaint*' 'run-puppet-agent'` (getting https://gerrit.wikimedia.org/r/c/operations/puppet/+/764875 out)</code>

'''19:27''' <code>(RdfStreamingUpdaterFlinkJobUnstable) resolved: WDQS_Streaming_Updater in codfw (k8s) is unstable - https://wikitech.wikimedia.org/wiki/Wikidata_Query_Service/Streaming_Updater  - https://alerts.wikimedia.org</code>

'''20:00''' WCQS version 0.3.104 is deployed, which includes a fix for WCQS failures https://gerrit.wikimedia.org/r/c/wikidata/query/rdf/+/764864. (Note: https://gerrit.wikimedia.org/r/c/wikidata/query/rdf/+/764864 addressed some WCQS failures but was not the primary cause of the WDQS failures)

2022-02-23

'''14:00''' investigation of the root cause shows that flink can no longer start properly in k8s, the app was restarted in yarn

'''18:00''' the flink app is still unable to run from k8s@codfw

2022-02-24

'''10:00''' Search devs link the root cause to a poor implementation of the swift client protocol and decides to switch to a S3 client, the app will remain running in YARN while we move away from this swift client.

2022-03-08 

'''10:00''' The flink app is able to start on k8s@codfw thanks to the switch to the S3 client protocol",,"*https://phabricator.wikimedia.org/T238751 (pre-existing ticket) would have prevented the period in which Wikidata edits could not get through despite the affected hosts having already been depooled


<mark>TODO: Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFIRE (Pending Review & Scorecard)]] Phabricator tag to these tasks.</mark>",2022-02-22,Unknown,Unknown,Unknown,Unknown,Unknown,2022-02-22_wdqs_updater_codfw.wikitext
"{{Incident scorecard
| task = 
| paged-num = 
| responders-num = 4
| coordinators = jhathaway
| start = 2022-03-01 22:35:25
| end = 2022-03-01 22:55:00
| impact = For 20 minutes, clients normally routed to Ulsfo were unable to reach any of our projects. This includes New Zealand, parts of Canada, and the United States west coast.
}}
Multiple of our redundant network providers for the San Francisco datacenter simultaneously experienced connectivity loss. After 20 minutes, clients were rerouted to other datacenters.

'''Documentation''':
*https://gerrit.wikimedia.org/r/c/operations/dns/+/767250/
*https://www.wikimediastatus.net/incidents/2rp6n2cpym3m
*https://phabricator.wikimedia.org/P21629#102776",,,,,"* [[phab:T303219|T303219]] Integrate DNS depools with Etcd and automate/remove the need for writing a Git commit.

* Can we increase fiber redundancy?",2022-03-01,Unknown,Unknown,Unknown,Unknown,Unknown,2022-03-01_ulsfo_network.wikitext
"{{Incident scorecard
| task =T303036 
| paged-num =25 
| responders-num =10 
| coordinators =jcrespo 
| start =2022-03-04 09:18:00 
| end =2022-03-04 10:47:53 
|metrics=Varnish uptime, general site availability|impact=For 1.5h, wikis were largely unreachable from Europe (via Esams) with shorter and more limited impact across the globe via other data centers as well.}}
A particular banner was deployed via CentralNotice that was both enabled for all users and with 100% sampling rate for its event instrumentation.

This caused instabilities at the outer [[Global traffic routing|traffic layer]]. The large amount of incoming traffic for event beacons, each of which had to be handed off to a backend service (eventgate-analytics-external), resulted in connections piling up and Varnish was unable to handle it and other traffic as a result, thus causing wikis to be unreachable in the affected regions. Initially [[Esams data center|Esams datacenter]] clients (mostly Europe, Africa and Middle East), with some temporary issues on other datacenters (Eqiad) as well when we initially attempted to reroute traffic to there.
[[File:Varnish traffic 2022-03-04 8-12AM.png|thumb|Varnish traffic 08:00-12:00]]
[[File:Navtiming pageviews 2022-03-04.png|thumb|Impacted pageviews by continent.]]
[[File:Traffic 2xx requests 2022-03-04.png|thumb|HTTP 2xx responses.]]
'''Documentation''':
*[https://grafana.wikimedia.org/d/000000093/varnish-traffic?orgId=1&from=1646382579944&to=1646393415075 Varnish traffic 08:00-12:00 (Grafana)]
*[https://grafana.wikimedia.org/d/000000479/frontend-traffic?orgId=1&from=1646382611263&to=1646393361590&var-site=All&var-cache_type=text&var-status_type=2 Frontend traffic 2xx responses (Grafana)]
*[https://grafana.wikimedia.org/d/000000230/navigation-timing-by-continent?orgId=1&from=1646366160339&to=1646404835341 navtiming pageview sampling (Grafana)]
*https://www.wikimediastatus.net/incidents/rhn1l6k33ynz
*[https://docs.google.com/document/d/1xYYzFlJcAP9pckqBWyiXUbs7HN5iThg85lkjv_RUh_o/edit Restricted documented]",,,,,"* {{bug|T303155}} Avoid flood of CN banner analytics
* {{bug|T303326}} Set a maximum for configurable sample rate of CentralNotice events that use EventGate",2022-03-04,Unknown,Unknown,Unknown,Unknown,Unknown,2022-03-04_esams_availability_banner_sampling.wikitext
"{{Incident scorecard
| task = 
| paged-num = sre
| responders-num = 3
| coordinators = 0
| start = 2022-03-06 13:50
| end = 2022-03-06 17:24
| impact = For 1.5 hour, some requests to the public Wikidata Query Service API were sporadically blocked.
}}
The service got overloaded and started to block client traffic, including Pybal which ultimately triggered the page.  Icinga sent some non-paging alerts at 13:50 but paging alerts didn't get sent until 16:52.",,,"* 13:50 Icinga alerts about <code>CRITICAL - CRITICAL - wdqs-heavy-queries_8888</code> - '''none paging'''
* 16:52 Icinga notifies about Wikidata Query Service wdqs eqiad - '''outage starts, paging'''
* 16:57 Icinga notifies about Wikidata Query Service wdqs eqiad - '''paging'''
* 17:02 Icinga notifies about Wikidata Query Service wdqs eqiad - '''paging'''
* 17:05 joe restarts <code>wdqs1006</code>
* 17:07 Icinga notifies about Wikidata Query Service wdqs eqiad - '''paging'''
* 17:07 load on <code>wdqs1006</code> increases to 120
* 17:15 jbond tries to reach search team, contacted gehel
* 17:17 gehel comes online
* 17:21 jbond restart wdqs services (at gehels request) clusterwide (<code>wdqs::public</code>)
* 17:24 RECOVERY messages in -operations  - '''outage ends'''

'''Documentation''':<syntaxhighlight lang=text>
02:15:05.481 [qtp2137211482-702147] INFO  o.w.q.r.b.t.ThrottlingFilter - A request is being banned. req.requestURI=/bigdata/namespace/wdq/sparql, req.xForwardedFor=10.64.1.19, req.queryString=query=%20ASK%7B%20%3Fx%20%3Fy%20%3Fz%20%7D, req.method=GET, req.remoteHost=localhost, req.requestURL=http://localhost/bigdata/namespace/wdq/sparql, req.userAgent=Twisted PageGetter

</syntaxhighlight>

* Fix required restarting the wdqs services on the cluster<syntaxhighlight lang=""bash"">
sudo cumin -b 1 -s 1 O:wdqs::public 'systemctl restart wdqs-blazegraph wdqs-categories wdqs-updater'

</syntaxhighlight>",,"* To make that service stable is to re-architect and replace Blazegraph.  The Search team will discuss this and arrange follow up actions
* In the meantime, https://phabricator.wikimedia.org/T293862 might help to improve the reliability of Blazegraph.
* Investigate if earlier alerts should page https://phabricator.wikimedia.org/T303134",2022-03-06,Unknown,Unknown,Unknown,Unknown,Unknown,2022-03-06_wdqs-categories.wikitext
,,,,,"*[[phab:T303499|Incident tracking task]]
*[[phab:T303498|Investigate if stopping mysql with buffer_pool dump between 10.4 versions is safe]]",2022-03-10,Unknown,Unknown,Unknown,Unknown,Unknown,2022-03-10_MediaWiki_availability.wikitext
"{{Incident scorecard
| task = 
| paged-num = N/A
| responders-num = 8
| coordinators = Alexandros
| start = 2022-03-27 14:36
| end = 2022-03-28 12:39
| impact = For about 4 hours, in three segments of 1-2 hours each over two days, there were higher levels of failed or slow MediaWiki API requests.
}}

A template changes in itwiki triggered translusion updates to many pages. Changeprop (with retries) issued thousands of requests to the API cluster to reparse the transcluding pages, including page summaries, which are done by Mobileapps.{{TOC|align=right}}",,"The consequences of the issues, that is not having enough PHP-FPM workers available was detected in a timely manner from icinga multiple times

* 14:36: PROBLEM - Not enough idle PHP-FPM workers for Mediawiki api_appserver at eqiad #page
* 15:33: PROBLEM - Some MediaWiki servers are running out of idle PHP-FPM workers in api_appserver at eqiad
* 19:09 Pages again: PROBLEM - Not enough idle PHP-FPM workers for Mediawiki api_appserver at eqiad #page on alert1001 is CRITICAL: 0.2933 lt 0.3 

Unfortunately it took a considerable amount of time to pin down the root cause.",'''All times in UTC.''',"===What went well?===

* Multiple people responded
* Automated monitoring detected the incident
* Graphs and dashboard showcased the issue quickly","*<s>Mobileapps is often throttled in codfw [[phab:T305482|T305482]]</s>
*<s>Limit changeprop transclusion concurrency. https://gerrit.wikimedia.org/r/c/operations/deployment-charts/+/774462</s>",2022-03-27,Unknown,Unknown,Mediawiki,Unknown,Unknown,2022-03-27_api.wikitext
"{{Incident scorecard
| task = 
| paged-num = sre
| responders-num = 1
| coordinators = 0
| start = 2022-03-27 13:51
| end = 2022-03-27 14:24
| metrics = per-host latency
| impact = For 30 minutes, all WDQS queries failed due to an internal deadlock.
}}WDQS in Codfw entered a state of deadlock that persisted until service restarts were performed. Note that WDQS eqiad was depooled at the time of the incident, reducing total capacity.{{TOC|align=right}}",,,,,"* To make that service stable is to re-architect and replace Blazegraph.  The Search team will discuss this and arrange follow up actions
* In the meantime, https://phabricator.wikimedia.org/T293862 might help to improve the reliability of Blazegraph.
* As the service is fairly fragile, but recovers quickly after a restart, simple auto-remediation such as scheduled service restarts might be appropriate.
* Investigate if earlier alerts should page https://phabricator.wikimedia.org/T303134
* As discussed [[phab:T242453|here]], the command-line utility ''jstack'' can detect deadlocks, and is installed on all wdqs hosts. Perhaps we can use it to monitor for these deadlocks.
* Update [[Wikidata Query Service/Runbook#Blazegraph%20deadlock|https://wikitech.wikimedia.org/wiki/Wikidata_Query_Service/Runbook#Blazegraph_deadlock]] with the exact verbiage from the alerts and examples of what Grafana looks like during these outages.",2022-03-27,Unknown,Unknown,Unknown,Unknown,Unknown,2022-03-27_wdqs_outage.wikitext
"'''Impact:'''

For approximately 5 minutes, Wikipedia and other Wikimedia sites were slow or inaccessible for many users, mostly in Europe/Africa/Asia.

'''Documentation''':
*https://www.wikimediastatus.net/incidents/ft72m2rcs8tg",,,,,,2022-03-29,Unknown,Unknown,Unknown,Unknown,Unknown,2022-03-29_network.wikitext
"{{Incident scorecard
| task = T305119
| paged-num = 17
| responders-num = 2
| coordinators = n/a
| start = 2022-03-31 05:18
| end = 2022-03-31 05:40
| metrics = No relevant SLOs exist. The API Gateway SLO was unaffected. If the planned app server SLO existed, we would have consumed a small portion of its error budget.
| impact = For 22 minutes, API server and app server availability were slightly decreased (~0.1% errors, all for s7-hosted wikis such as Spanish Wikipedia), and the latency of API servers was elevated as well.
}}

After a code change [https://gerrit.wikimedia.org/r/c/mediawiki/core/+/773915] [https://gerrit.wikimedia.org/r/c/mediawiki/extensions/CentralAuth/+/773913] rolled out in this week's train, the GlobalUsersPager class (part of CentralAuth) produced expensive DB queries that exhausted resources on [[S7|s7 database]] replicas.

Backpressure from the databases tied up PHP-FPM workers on the API servers, triggering a paging alert for worker saturation. The slow queries were identified and manually killed on the database, which resolved the incident.

Because the alert fired and the queries were killed before available workers were fully exhausted, the impact was limited to s7. Full worker saturation would have resulted in a complete API outage.

Because only two engineers responded to the page and the response only took half an hour, we decided not to designate an incident coordinator, start a status doc, and so on. We didn't need those tools to organize the response, and they would have taken time away from solving the problem.

'''Documentation''':
* [https://phabricator.wikimedia.org/T305119 Phabricator task detailing the slow query]
* [https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?orgId=1&from=1648702800000&to=1648706400000&var-datasource=eqiad%20prometheus%2Fops&var-cluster=api_appserver&var-method=GET&var-code=200 API server RED dashboard showing elevated latency and errors; php-fpm workers peaking around 75% saturation; and s7 database errors]
* [https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?orgId=1&from=1648702800000&to=1648706400000&var-datasource=eqiad%20prometheus%2Fops&var-cluster=appserver&var-method=GET&var-code=200 Same dashboard for the app servers showing measurable but lesser impact]",,,,,"* Revert the patches generating the slow queries - done [https://sal.toolforge.org/log/J7Yi4H8Ba_6PSCT9nWWR] [https://sal.toolforge.org/log/Wigp4H8B8Fs0LHO5M3s9]
* Later (2022-04-06) it was discovered the query killer was using the old 'wikiuser' name, which prevented it from acting. Fixed in [https://gerrit.wikimedia.org/r/c/operations/software/+/777760], deploying soon.",2022-03-31,Unknown,Unknown,Unknown,Unknown,Unknown,2022-03-31_api_errors.wikitext
"{{Incident scorecard
| task = T305532
| paged-num = 
| responders-num = 
| coordinators = Jaime C
| start = 08:20
| end = 08:50
| impact = For 30 minutes, wikis were slow or unreachable for a portion of clients to the Esams data center. This is one of two DCs serving Europe, Middle-East, and Africa.
}}

This was due to [https://www.ams-ix.net/ams/news/outage-at-the-ams-ix-platform-in-amsterdam an issue] at the [[w:Amsterdam Internet Exchange|Amsterdam Internet Exchange]] (AMS-IX).

'''Documentation''':

* https://www.wikimediastatus.net/incidents/jnqvz8gljzhy

*[https://docs.google.com/document/d/1FfWF7LVyDpWvtIcv3G-Z4xaPZvOR3Rn05NZegywfVaY/edit#heading=h.vg6rb6x2eccy Restricted document]",,,,,,2022-04-06,Unknown,Unknown,Unknown,Unknown,Unknown,2022-04-06_esams_network.wikitext
"{{Incident scorecard
| task = 
| paged-num = 18
| responders-num = 5
| coordinators = marostegui
| start = 05:13
| end = 07:27
| metrics = None.
| impact = None.
}}Since the previous evening (April 25), the Telia link from Eqord to Eqiad was down due to a fiber cut. At 05:00 the next morning, a Telia maintenance began that took down our remaining transports from Eqord, to Codfw and Ulsfo. As a result, we were entirely unable to reach the Eqord networking equipment. There was no end-user impact since Eqord is a network-only location with end-user traffic for [[Codfw data center|Codfw]] and [[Ulsfo data center|Ulsfo]] naturally going there directly instead of via Eqord.


<code>cathal@nbgw:~$ ping -c 3 208.115.136.238 PING 208.115.136.238 (208.115.136.238) 56(84) bytes of data.  --- 208.115.136.238 ping statistics ---  3 packets transmitted, 0 received, 100% packet loss, time 2055ms</code>

Telia circuit failure logs:
 Apr 25 17:32:42 cr2-eqord fpc0 MQSS(0): CHMAC0: Detected Ethernet MAC Remote Fault Delta Event for Port 5 (xe-0/1/5) 
 Apr 26 05:11:57 cr2-eqord fpc0 MQSS(0): CHMAC0: Detected Ethernet MAC Remote Fault Delta Event for Port 3 (xe-0/1/3)
 Apr 26 05:11:57 cr2-eqord fpc0 MQSS(0): CHMAC0: Detected Ethernet MAC Local Fault Delta Event for Port 0 (xe-0/1/0)
 Apr 26 07:15:19 cr2-eqord fpc0 MQSS(0): CHMAC0: Cleared Ethernet MAC Remote Fault Delta Event for Port 3 (xe-0/1/3)
 Apr 26 07:15:47 cr2-eqord fpc0 MQSS(0): CHMAC0: Cleared Ethernet MAC Local Fault Delta Event for Port 0 (xe-0/1/0)
'''Documentation''':

* [[Network design]]
* [https://docs.google.com/document/d/13-kHFdSw33P6NJzS95c24zOHaJAKvVSl8DFF3o6VCmY/edit Restricted document]",,,,,* Grant Cathal authorization to be able to create remote hands cases (DONE),2022-04-26,Unknown,Unknown,Unknown,Unknown,Unknown,2022-04-26_cr2-eqord_down.wikitext
"{{Incident scorecard
| task = T307382
| paged-num = 15
| responders-num = 2
| coordinators = dzahn, cwhite
| start = 04:48
| end = 06:38
| metrics = [[SLO/etcd main cluster|etcd main cluster]]
| impact = For 2 hours, Conftool could not sync Etcd configuration data between our core data centers. This means the DCs started getting out of sync, puppet-merge was inoperational, and wikitech/labweb hosts expeerienced failed systemd timers. No noticable impact on public services.
}}
<!-- Reminder: No private information on this page! -->

The TLS certificate for etcd.eqiad.wmnet expired. Nginx servers on conf* hosts use this certificate, and thus [[conftool]]-data could not sync between conf hosts anymore. During this time, <code>puppet-merge</code> returned sync errors. [[Labweb1001|labweb]] (wikitech) hosts alerted because of failed timers/jobs.

We got paged by monitoring of ""Etcd replication lag"". We had to renew the certificate but it wasn't a simple renew, because additionally some certificates had already converted to a new way or creating and managing them while others had not. Our two core [[data centers]] were in different states. Only Eqiad was affected by lag and sync errors. After figuring this out, we eventually created a new certificate for etcd.eqiad using <code>cergen</code>, copied the private key and certs in place and reconfigured servers in Eqiad to use it. After this, all alerts recovered.

'''Documentation''':
*https://grafana.wikimedia.org/d/Ku6V7QYGz/etcd3?orgId=1&from=1651361014023&to=1651428319517
*https://logstash.wikimedia.org/goto/1e1994e64e8c23ef570fb19f562bf08b",,,,,"<mark>Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.</mark>

*[[phab:T307382|T307382]] (Modernize etcd tlsproxy certificate management)
*[[phab:T307383|T307383]] <mark>(Certificate expiration monitoring)</mark>
* https://gerrit.wikimedia.org/r/q/topic:etcd-certs (5 Gerrit changes)

<mark>TODO: Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFIRE (Pending Review & Scorecard)]] Phabricator tag to these tasks.</mark>",2022-05-01,Unknown,Unknown,Unknown,Unknown,Unknown,2022-05-01_etcd.wikitext
"{{Incident scorecard
| task = T307349
| paged-num = 0
| responders-num = 4
| coordinators = Jaime Crespo
| start = 2022-05-02 11:13
| end = 2022-05-02 15:21
| metrics = No relevant SLOs exist
| impact = For 4 hours, MediaWiki and other services could not be updated or deployed due to data loss on the active deployment server.
}}
<!-- Reminder: No private information on this page! -->

Parts of <code>/srv/deployment</code> were lost on active [[deployment server]] (deploy1002) due to the wrong command, <code>rm -rf</code>, being executed. This halted deployments for some time, until we were able to restore the directory from a backup and we checked it for correctness.

'''Documentation''':
*[[phab:T307349|T307349]]
*[[Bacula|Bacula backups]]",,,,,* https://phabricator.wikimedia.org/T309162,2022-05-02,Unknown,Unknown,Unknown,Data loss,Unknown,2022-05-02_deployment.wikitext
"{{Incident scorecard
| task = T307647
| paged-num = 14
| responders-num = 4
| coordinators = Due to the low amount of people responding there was no IC
| start = 05:36
| end = 05:55
| impact = For 20 minutes, all wikis were unreachable for logged-in users and non-cached pages.
}}

A schema change ([[phab:T307501]]) made mariadb's optimizer change its query plan and made a very frequent query to globalblocks table on centralauth database (s7) to take 5 seconds instead of less than a second.

{{TOC|align=right}}",,"* Alerts (IRC and pages)<syntaxhighlight lang=irc>
[05:39:18]  <+jinxer-wm> (ProbeDown) firing: (10) Service appservers-https:443 has failed probes (http_appservers-https_ip4) - https://wikitech.wikimedia.org/wiki/Network_monitoring#ProbeDown - https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=http - https://alerts.wikimedia.org/?q=alertname%3DProbeDown
[05:39:18]  <+jinxer-wm> (ProbeDown) firing: (20) Service appservers-https:443 has failed probes (http_appservers-https_ip4) #page - https://wikitech.wikimedia.org/wiki/Network_monitoring#ProbeDown - https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=http - https://alerts.wikimedia.org/?q=alertname%3DProbeDown
[05:40:04]  <+icinga-wm> PROBLEM - High average POST latency for mw requests on appserver in eqiad on alert1001 is CRITICAL: cluster=appserver code=200 handler=proxy:unix:/run/php/fpm-www.sock https://wikitech.wikimedia.org/wiki/Monitoring/Missing_notes_link https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=9&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=eqiad+prometheus/ops&var-cluster=appserver&var-method=POST
[05:40:10]  <+icinga-wm> PROBLEM - Not enough idle PHP-FPM workers for Mediawiki api_appserver at eqiad #page on alert1001 is CRITICAL: 0.07182 lt 0.3 https://bit .ly/wmf-fpmsat https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=54&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=eqiad+prometheus/ops&var-cluster=api_appserver
[05:40:14]  <+icinga-wm> PROBLEM - High average GET latency for mw requests on api_appserver in eqiad on alert1001 is CRITICAL: cluster=api_appserver code=200 handler=proxy:unix:/run/php/fpm-www.sock https://wikitech.wikimedia.org/wiki/Monitoring/Missing_notes_link https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=9&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=eqiad+prometheus/ops&var-cluster=api_appserver&var-method=GET</syntaxhighlight>
* User reported it on IRC","'''All times in UTC.'''

*05:35 Schema change deployed
*05:36 '''Outage starts here''' (gradual increase on latency, decrease on regular app server traffic)
*05:39 <jinxer-wm> (ProbeDown) firing: (10) Service appservers-https:443 has failed probes (http_appservers-https_ip4) - [[Network monitoring#ProbeDown|https://wikitech.wikimedia.org/wiki/Network_monitoring#ProbeDown]] - [https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=http&#x20;-&#x20;https://alerts.wikimedia.org/?q=alertname%3DProbeDown <nowiki>https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=http</nowiki> - <nowiki>https://alerts.wikimedia.org/?q=alertname%3DProbeDown</nowiki>]
*05:39 <legoktm> I can't load enwp
*05:39 <legoktm> or it's very slow
*05:40 <icinga-wm> PROBLEM - High average POST latency for mw requests on appserver in eqiad on alert1001 is CRITICAL: cluster=appserver code=200 handler=proxy:unix:/run/php/fpm-www.sock https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=9&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=eqiad+prometheus/ops&var-cluster=appserver&var-method=POST
*05:40 marostegui notices db1127 to be unavailable and he depools it
*05:41 _joe_ and marostegui start debugging, confirming all servers are stuck at [0x00007fbcc9e1ee20] query() /srv/mediawiki/php-1.39.0-wmf.9/includes/libs/rdbms/database/DatabaseMysqli.php:49, confirming it is a database issue
*05:44 CDN start to spike on 5XX errors
*05:45 Ongoing schema changes are stopped by Amir. Issue is narrowed down to s7 section queries.
*05:45 marostegui mentions https://gerrit.wikimedia.org/r/c/mediawiki/extensions/GlobalBlocking/+/785376/1/sql/mysql/patch-globalblocks-timestamps.sql#7  suggesting it as the cause. The query suspected that is causing it is: SELECT /* MediaWiki\Extension\GlobalBlocking\GlobalBlocking::getGlobalBlockingBlock  */  gb_id,gb_address,gb_by,gb_by_wiki,gb_reason,gb_timestamp,gb_anon_only,gb_expiry,gb_range_start,gb_range_end  FROM `globalblocks`	WHERE (gb_range_start  LIKE '5B85%' ESCAPE '`' ) AND (gb_range_start <= '5B85B2D2') AND (gb_range_end >= '5B85B2D2') AND (gb_expiry > '20220505054805');
*05:46 The status page is updated by _joe_ https://www.wikimediastatus.net/incidents/xzmd6vwvvgmx
*05:48 <icinga-wm> RECOVERY - PHP7 rendering on mw1329 is OK: HTTP OK: HTTP/1.1 302 Found - 650 bytes in 7.305 second response time [[Application servers/Runbook#PHP7%20rendering|https://wikitech.wikimedia.org/wiki/Application_servers/Runbook%23PHP7_rendering]]
*05:50 Schema change is rolled back by Marostegui. T307501#7905692
*05:51 Median App server latency & CDN errors starts going down
*05:52 <legoktm> wiki is working again for me
*05:55 App server latency and CDN 5XX errors go back to normal levels - '''OUTAGE ENDS HERE'''","===What went well?===
*The root cause was quickly identified and the revert was easy and fast to apply.",*Investigate the mariadb optimizer behaviour for this specific table: https://phabricator.wikimedia.org/T307501,2022-05-05,Unknown,Monitoring,Mediawiki,Unknown,Unknown,2022-05-05_Wikimedia_full_site_outage.wikitext
"{{Incident scorecard
| task = T309691
| paged-num = 26
| responders-num = 6
| coordinators = 
| start = 2022-05-09 07:44:00
| end = 2022-05-09 07:51:00
| impact = For 5 minutes, all web traffic routed to Codfw received error responses. This affected central USA and South America (local time after midnight).
}}
<!-- Reminder: No private information on this page! -->

The <code>confctl</code> command to depool a server was accidentally run with an invalid selection parameter (<code>'''host'''=mw1415</code> instead of <code>'''name'''=mw1415</code>, details at [[phab:T308100|T308100]]). There exists no ""host"" parameter, and Confctl did not validate it, but silently ignore it. The result was that the depool command was interpreted as applying to all hosts, of all services, in all data centers. The command was cancelled partway through the first DC it iterated on (Codfw).

Confctl-managed services were set as inactive for most of the [[Codfw data center]]. This caused all end-user traffic that was at the time being routed to codfw (Central US, South America - at a low traffic moment) to respond with errors. While appservers in codfw were at the moment ""passive"" (not receiving end-user traffic), other services that are active were affected (CDN edge cache, Swift media files, Elasticsearch, [[WDQS]]…).

The most visible effect, during the duration of the incident, was approximately 1.4k HTTP requests per second to not be served to text edges and 800 HTTP requests per second to fail to be served from upload edges. The trigger for the issue was a gap in tooling that allowed running a command with invalid input.

{{TOC|align=right}}",,"The issue was detected by both the monitoring, with expected alerts firing, and the engineer executing the change.

Example alerts:

07:46:18: <jinxer-wm> (ProbeDown) firing: (27) Service appservers-https:443 has failed probes (http_appservers-https_ip4) - <nowiki>https://wikitech.wikimedia.org/wiki/Network_monitoring#ProbeDown</nowiki> - <nowiki>https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=http</nowiki> - <nowiki>https://alerts.wikimedia.org/?q=alertname%3DProbeDown</nowiki>

07:46:19: <jinxer-wm> (ProbeDown) firing: (29) Service appservers-https:443 has failed probes (http_appservers-https_ip4) #page - <nowiki>https://wikitech.wikimedia.org/wiki/Network_monitoring#ProbeDown</nowiki> - <nowiki>https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=http</nowiki> - <nowiki>https://alerts.wikimedia.org/?q=alertname%3DProbeDown</nowiki>","'''All times in UTC.'''

*07:44 confctl command with invalid parameters is executed '''OUTAGE BEGINS'''
*07:44 Engineer executing the change realizes the change is running against more servers than expected and cancels the execution mid-way
*07:46 Monitoring system detects the app servers unavailability, 15 pages are sent
*07:46 Engineer executing the change notifies others via IRC
*07:50 confctl command to repool all codfw servers is executed '''OUTAGE ENDS''' 
[[File:2022-05-09_confctl_5xx.png|none|thumb|confctl 5xx errors]]
[[File:2022-05-09_confctl_error_graph.png|none|thumb|wikimediastatus.net]]","When provided with invalid input, confctl executes the command against all hosts, it should fail instead.",* [https://phabricator.wikimedia.org/T308100 T308100: Invalid confctl selector should either error out or select nothing],2022-05-09,Unknown,Monitoring,Unknown,Unknown,Unknown,2022-05-09_confctl.wikitext
"{{Incident scorecard
| task = T307873
| paged-num = 0
| responders-num = 5
| coordinators = Keith Herron
| start = 2022-05-04 01:28
| end = 2022-05-09 16:40
| metrics = No relevant SLOs exist, nor are there published metrics to quantify the impact.
| impact = During five days, about 14,000 incoming emails from Gmail users to wikimedia.org were rejected and returned to sender.
}}
<!-- Reminder: No private information on this page! -->

Starting on Sat 4 May 2022, incoming emails from Google Mail servers began being rejected with the log message ""503 BDAT command used when CHUNKING not advertised"". These errors were not noticed by us until five days later on Thu 9 May 2022. After some investigation, it was determined that disabling chunking support in Exim would mitigate the errors. During the time span of the incident about 14,000 emails were rejected with an SMTP 503 error code, the senders are naturally notified by their email provider about undelivered mail.

{{TOC|align=right}}",,"The issue was first detected by users sending emails, https://phabricator.wikimedia.org/T307873. Though, the messages were rejected by Exim with a 503 error code. We do graph the number of bounced messages, but our alerting did not pick up these bounces, https://grafana.wikimedia.org/d/000000451/mail?orgId=1&from=1651536000000&to=1652227199000","'''All times in UTC.'''

'''2022-05-04'''

*01:28 '''Exim begins rejecting some email with 503s (Impact Begins)'''
'''2022-05-08'''
*17:07 (bcampbell) Opens a ticket saying that multiple users have reported their emails being bounced with errors, https://phabricator.wikimedia.org/T307873
'''2022-05-09'''
*8:25 (jbond) replies to ticket and begins investigation 
*13:55 (herron) '''Incident declared Herron becomes IC'''
*14:00 (jhathaway) rolls back recently upgraded kernels, no effect
*16:03 request to ITS to ask Google if anything has changed on their end
*16:40 (jhathaway) '''chunking disabled in Exim, which successfully mitigates the incident, (Impact Ends)'''

<!-- Reminder: No private information on this page! -->==Detection==
The issue was first detected by users sending emails, https://phabricator.wikimedia.org/T307873. Though, the messages were rejected by Exim with a 503 error code. We do graph the number of bounced messages, but our alerting did not pick up these bounces, https://grafana.wikimedia.org/d/000000451/mail?orgId=1&from=1651536000000&to=1652227199000",Email monitoring of bounced messages does not account for all bounces.,"* Improve monitoring, https://phabricator.wikimedia.org/T309237",2022-05-09,Unknown,Unknown,Unknown,Unknown,Unknown,2022-05-09_exim-bdat-errors.wikitext
"{{Incident scorecard
| task =T308380 
| paged-num =26 
| responders-num =10 
| coordinators =jwodstrcil 
| start =2022-05-20 09:35:00 
| end =2022-05-20 09:35:00 
|impact=Two occurrences of impact on uncached traffic (high latency, unavailability) related to application server worker thread exhaustion caused by slow database response.}}

On 2022-05-14 at 8:18 UTC there was a 3 minute impact on uncached traffic (high latency, unavailability) related to application server worker thread exhaustion caused by slow database response. There was no clear root cause at the time. The incident occurred again on the same database host on 2022-05-20 at 09:35 UTC, this time lasting for 5 minutes. After further investigation the likely root cause is a MariaDB 10.6 performance regression under load, further researched in https://phabricator.wikimedia.org/T311106.

'''Documentation''':
*[https://grafana.wikimedia.org/d/000000438/mediawiki-exceptions-alerts?orgId=1&var-datasource=eqiad%20prometheus%2Fops&viewPanel=18&from=1653039084000&to=1653040837000 MediaWiki Exceptions]",,,,,*[[phab:T311106|Investigate mariadb 10.6 performance regression during spikes/high load]],2022-05-20,Unknown,Unknown,Unknown,Unknown,Unknown,2022-05-20_Database_slow.wikitext
"{{Incident scorecard
| task = T308940
| paged-num = 5
| responders-num = 7
| coordinators = dzahn
| start = 2022-05-21 19:01:00
| end = 2022-05-21 19:03:00
| impact = For 2 minutes, all wikis and services served by our CDN were unavailable to all users.
}}

A flood of API traffic from an AWS instance caused caching servers to be overloaded. Services behind our caching layer were up, but not reachable during this time.",,,,,"*[[phab:T308952|T308952 - get a legend for haproxy ""anomalous session termination states""]]
*[[phab:T308940|T308940 - follow-up on user reported ticket with  public incident report]]
*[[phab:T308941|T308941 - semi related: Klaxon redirects to http]]",2022-05-21,Unknown,Unknown,Unknown,Unknown,Unknown,2022-05-21_varnish_cache_busting.wikitext
"{{Incident scorecard
| task = 
| paged-num = 27
| responders-num = 5
| coordinators = Manuel Arostegui
| start = 11:40
| end = 12:16
| impact = A very small amount of 502 HTTP errors for users (predominantly logged-in users). Plus some 140 IRC alerts for a subset of hosts running Apache
}}
* A Puppet change seem to have caused an apache restart https://gerrit.wikimedia.org/r/c/operations/puppet/+/798615/ that didn’t work as it made it listen 443 regardless whether mod_ssl is enabled
** Alerts: connect to address 10.X.X.X and port 80: Connection refused | CRITICAL - degraded: The following units failed: apache2.service
** Puppet was quickly disabled to prevent a site-wide outage (apache failing everywhere affected)
** Initial triage was done via cumin: ''sudo cumin -m async 'mw1396*' 'sed -i"""" ""s/Listen 443//"" /etc/apache2/ports.conf '  'systemctl start apache2 '''  as the patch revert didn’t work.
** The final fix: https://gerrit.wikimedia.org/r/c/operations/puppet/+/798631/
* {{TOC|align=right}}",,"* The first detection was by the engineer who merged the patch.
* Later IRC alerts and pager.
** ''<+jinxer-wm> (ProbeDown) firing: Service kibana7:443 has failed probes (http_kibana7_ip4) #page - [[Network monitoring#ProbeDown|https://wikitech.wikimedia.org/wiki/Network_monitoring#ProbeDown]] - <nowiki>https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=http</nowiki> - <nowiki>https://alerts.wikimedia.org/?q=alertname%3DProbeDown</nowiki>'' ''<+icinga-wm> PROBLEM - Apache HTTP on parse2001 is CRITICAL: connect to address 10.192.0.182 and port 80: Connection refused [[Application servers|https://wikitech.wikimedia.org/wiki/Application_servers]]''  ''<+icinga-wm> PROBLEM - Apache HTTP on mw1320 is CRITICAL: connect to address 10.64.32.41 and port 80: Connection refused [[Application servers|https://wikitech.wikimedia.org/wiki/Application_servers]]''  ''<+icinga-wm> PROBLEM - Apache HTTP on mw1361 is CRITICAL: connect to address 10.64.48.203 and port 80: Connection refused [[Application servers|https://wikitech.wikimedia.org/wiki/Application_servers]]''","'''All times in UTC.'''

* 11:40 <nowiki>https://gerrit.wikimedia.org/r/c/operations/puppet/+/798615/</nowiki> gets merged

* 11:45 INCIDENT STARTS ​​<+jinxer-wm> (ProbeDown) firing: Service kibana7:443 has failed probes (http_kibana7_ip4) #page - [[Network monitoring#ProbeDown|https://wikitech.wikimedia.org/wiki/Network_monitoring#ProbeDown]] - https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=http - https://alerts.wikimedia.org/?q=alertname%3DProbeDown

* 11:45 John realises the patch broke apaches across mw hosts
* 11:45 puppet gets disabled on mw hosts, preventing a wider outage
* 11:46 The patch gets reverted and merged: <nowiki>https://gerrit.wikimedia.org/r/c/operations/puppet/+/797222</nowiki>
* 11:47 A manual puppet run is forced on eqiad
* 11:47 Multiple alerts arrive to IRC
* 11:49 the revert doesn’t fix things and a manual cumin run is needed
* 11:50  Incident opened.  <Manuel Arostegui> becomes IC.
* [11:50:46] <_joe_> jbond: it seems it tries to listen on port 443
* [11:51:16] <taavi> jbond: your patch makes apache2 listen on 443 regardless whether mod_ssl is enabled
* 11:56: The following command is issued across the fleet:  sudo cumin -m async 'mw1396*' 'sed -i"""" ""s/Listen 443//"" /etc/apache2/ports.conf '  'systemctl start apache2 ' 
* 11:57: First recoveries arrive
* 11:58 MW app servers fixed here
* 12:05 The following patch is merged https://gerrit.wikimedia.org/r/c/operations/puppet/+/798631/ to get the proper fix in place for other non-mw servers relying on existing default debian configuration
* 12:07 <_joe_> I am running puppet on people1003 (to test patch) - <_joe_> the change does the right thing there
* 12:16 Other non-mw apaches fixed here:  < jbond> sudo cumin C:httpd 'systemctl status apache2.service &>/dev/null' is all good","During the Incident Review ritual, it was pointed out that if we had a way to deploy those changes in a controlled environment (e.g. canary) we could have been saved from this one. It was also noted that PCC did not catch this one as puppet complied this fine, it's just the resulting Apache configuration was unconditionally also listening on port 443.","==Scorecard==


{| class=""wikitable""
|+[[Incident Scorecard|Incident Engagement™  ScoreCard]]
!
!Question
!Answer
(yes/no)
!Notes
|-
! rowspan=""5"" |People
|Were the people responding to this incident sufficiently different than the previous five incidents?
|No
|At least 4 out of 5 are usual responders (Marostegui, jbond, _joe_, jynus)
|-
|Were the people who responded prepared enough to respond effectively
|Yes
|
|-
|Were fewer than five people paged?
|No
|
|-
|Were pages routed to the correct sub-team(s)?
|No
|N/A
|-
|Were pages routed to online (business hours) engineers?  ''Answer “no” if engineers were paged after business hours.''
|Yes
|
|-
! rowspan=""5"" |Process
|Was the incident status section actively updated during the incident?
|Yes
|
|-
|Was the public status page updated?
|No
|
|-
|Is there a phabricator task for the incident?
|No
|
|-
|Are the documented action items assigned?
|No
|
|-
|Is this incident sufficiently different from earlier incidents so as not to be a repeat occurrence?
|No
|
|-
! rowspan=""5"" |Tooling
|To the best of your knowledge was the open task queue free of any tasks that would have prevented this incident? ''Answer “no” if there are''
''open tasks that would prevent this incident or make mitigation easier if implemented.''
|No
|
|-
|Were the people responding able to communicate effectively during the incident with the existing tooling?
|Yes
|
|-
|Did existing monitoring notify the initial responders?
|Yes
|
|-
|Were all engineering tools required available and in service?
|No
|At least Kibana and piwik/matomo where down
|-
|Was there a runbook for all known issues present?
|No
|
|-
! colspan=""2"" align=""right"" |Total score (c'''ount of all “yes” answers above)'''
|5
|
|}",2022-05-24,Unknown,Monitoring,Unknown,Unknown,Unknown,2022-05-24_Failed_Apache_restart.wikitext
"{{Incident scorecard
| task = T309178
| paged-num = 26
| responders-num = 8
| coordinators = Jbond
| start = 20:08
| end = 20:14
| impact = For 6 minutes, a portion of logged-in users and non-cached pages experienced a slower response or an error. This was due to increased load on one of the databases.
}}

An increase in POST requests to de.wikipedia.org caused an increase in load on one of the DB servers resulting in an increase in 503 responses and increased response time

{{TOC|align=right}}",,"Error was detected by alert manager monitoring

<syntaxhighlight lang=""irc"">
20:08 <+jinxer-wm> (ProbeDown) firing: (8) Service text-https:443 has failed probes (http_text-https_ip4) #page - https://wikitech.wikimedia.org/wiki/Network_monitoring#ProbeDown -
https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=http - https://alerts.wikimedia.org/?q=alertname%3DProbeDown
20:08 <+jinxer-wm> (FrontendUnavailable) firing: HAProxy (cache_text) has reduced HTTP availability #page - TODO - https://grafana.wikimedia.org/d/000000479/frontend-traffic?viewPanel=13 - https://alerts.wikimedia.org/?q=alertname%3DFrontendUnavailable
20:09 <+jinxer-wm> (FrontendUnavailable) firing: varnish-text has reduced HTTP availability #page - https://wikitech.wikimedia.org/wiki/Varnish#Diagnosing_Varnish_alerts - https://grafana.wikimedia.org/d/000000479/frontend-traffic?viewPanel=3 - https://alerts.wikimedia.org/?q=alertname%3DFrontendUnavailable
</syntaxhighlight>","'''All times in UTC.'''

*20:04 '''OUTAGE BEGINS'''
*20:04 Received page ""Service text-https:443 has failed probes""
*20:08 '''rzl starts investigation'''
*20:08 Received page ""(FrontendUnavailable) firing: HAProxy (cache_text) has reduced HTTP availability""
*20:08 rzl asked cjming to halt deploying
*20:09 Recived page ""(FrontendUnavailable) firing: varnish-text has reduced HTTP availability""
*20:09 '''jbond takes IC''' 
*20:10 < rzl> looks like a spike of DB queries to s5 that saturated php-fpm workers, seems like it's already cleared
*20:11 Received recovery ""RECOVERY - High average GET latency for mw requests on appserver""
*20:11 < cwhite> Lots of POST to https://de.wikipedia.org
*20:12 < rzl> [https://grafana.wikimedia.org/d/000000278/mysql-aggregated?orgId=1&var-site=eqiad&var-group=core&var-shard=s5&var-role=All&from=1653416788376&to=1653427535487 s5 did see a traffic spike but recovered], still digging
*20:13 Received recovery ""resolved: (8) Service text-https:443 has failed probes""
*20:13 Received recovery ""resolved: HAProxy (cache_text) has reduced HTTP availability""
*20:14 Received recovery ""resolved: varnish-text has reduced HTTP availability""
*20:14 '''OUTAGE ENDS'''
*20:14 < cwhite> [https://logstash.wikimedia.org/goto/f23d960df9200156ea11b85fc727d58c 2217 unique ips (according to logstash)]
*20:18 < bblack> identified traffic as ""a bunch of dewiki root URLs""
*20:22 < _joe_> php slowlogs mostly showed query() or curl_exec()
*20:30 < _joe_> someone was calling radompage repeatedly?
*20:31 <rzl> [https://grafana-rw.wikimedia.org/explore?left=%5B%221653422697708%22,%221653423304355%22,%22eqiad%20prometheus%2Fops%22,%7B%22expr%22:%22sum%20by%20(instance)%20(irate(apache_accesses_total%7Bcluster%3D%5C%22appserver%5C%22,%20job%3D%5C%22apache%5C%22,%20instance%3D~%5C%22mw1.*%5C%22%7D%5B2m%5D))%22,%22format%22:%22time_series%22,%22intervalFactor%22:2,%22refId%22:%22A%22,%22target%22:%22%22,%22datasource%22:%7B%22type%22:%22prometheus%22,%22uid%22:%22000000006%22%7D,%22interval%22:%22%22,%22exemplar%22:false%7D,%7B%22expr%22:%22%22,%22format%22:%22time_series%22,%22intervalFactor%22:2,%22refId%22:%22B%22,%22datasource%22:%7B%22type%22:%22prometheus%22,%22uid%22:%22000000006%22%7D,%22interval%22:%22%22,%22instant%22:true,%22range%22:true,%22exemplar%22:false%7D%5D&orgId=1 looks like it was all appservers pretty equally]
*20:40 Discuss remediation strategy
*20:48 '''Incident officially closed'''
*20:51 < rzl> gave cjming all clear to continue with deploy
*21:29 '''requestctl rule put in place'''",Understanding of legitimate backed traffic would enable us to better sanitize bad traffic at the front end,"*[https://phabricator.wikimedia.org/T309147 T309147]  any POST that doesn't go to /w/*.php  or /wiki/.* should become a 301 to the same url
*[https://phabricator.wikimedia.org/T309186 T309186] Created sampled log of post data
*[https://phabricator.wikimedia.org/T310009 T310009] Make it easier to create a new requestctl object",2022-05-25,Unknown,Monitoring,Unknown,Unknown,Unknown,2022-05-25_de.wikipedia.org.wikitext
"{{Incident scorecard
| task = T309286
| paged-num = 26
| responders-num = 4
| coordinators = 
| start = 2022-05-26 09:38:00
| end = 2022-05-26 09:50:00
| impact = For 12 minutes, internal services hosted on the m1 database (e.g. Etherpad) were unavailable or at reduced capacity.
}}
For approximately 12 minutes some internal services (e.g. [[Bacula]] and [[Etherpad]]) were not available or operated at reduced capacity. This was caused by a faulty memory stick leading to a reboot of db1128, which was at the time the primary host of the [[MariaDB#Miscellaneous|m1 database section]].

'''Documentation''':
*[[phab:P28584|Full list of potentially affected services]]",,,,,"*[[phab:T309296|Failover m1 primary db from db1128 to db1164]]
*[[phab:T309291|db1128 faulty memory]]",2022-05-26,Unknown,Unknown,Unknown,Unknown,Unknown,2022-05-26_Database_hardware_failure.wikitext
"{{Incident scorecard
| task = T309649
| paged-num = 3
| responders-num = 4
| coordinators = Andrew Otto
| start = 2022-05-31 17:09:10
| end = ~2022-05-31 18:00:00
| impact = A component of our Hadoop HDFS distributed file system called the NameNode failed, on both the primary and standby server. This caused all HDFS writes and reads to fail.

There was no public-facing impact to this incident, so the users involved are those in the various WMF data teams who use tools such as Hive, Spark, Jupyter, Superset etc.

In addition to these interactive users, several scheduled ingestion pipelines that read from or write to HDFS were also broken for the duration of the incident. 

After recovery of the namenodes, regular ingestion from Kafka was resumed from the point where it stopped. Therefore there was no lasting impact nor data loss as a result of this incident.
}}
<!-- Reminder: No private information on this page! -->
* At Tue May 31 17:09:10 UTC 2022 analytics-alerts@wikimedia.org received an email alert: ""At least one Hadoop HDFS NameNode is active is CRITICAL""
* Otto, Btullis, Joal, and Mforns jumped in hangout to troubleshoot.
* /var/lib/hadoop/journal on all 5 journalnodes was full
* Otto and Btullis stopped namenodes and journalnodes
* Btullis increases <code>/var/lib/hadoop/journal</code> on journalnodes from 10 GB to 30 GB
* Btullis starts journalnodes, then master namenode.
* Otto forces HDFS to stay in safe mode.
* Wait for master namenode to apply edits from journalnodes.
* Start standby namenode
* Wait for standby namenode to apply edits
* Otto allows HDFS to leave safe mode.

'''Documentation''':

Cause analysis:

On Sunday evening May 22, <code>/srv</code> filled up on an-master1002. an-master1002 takes daily fs image snapshots, and saves them in <code>/srv/backup/hadoop/namenode</code>, keeping the last 20.  Over time, as the number of HDFS blocks has increased, so has the size of these backup images.  

We received an alert email for a failure of the <code>hadoop-namenode-backup-fetchimage</code> that takes these backups with the subject ""''an-master1002/Check unit status of hadoop-namenode-backup-fetchimage is CRITICAL''"".

24 hours later, this backup job succeeded, even if no new image backup was taken, and we got a RECOVERY status email for this job.  Otto was on ops week, and only working half days this week.  Otto most likely saw the RECOVERY email and ignored the alert.

On Tuesday May 31, <code>/var/lib/hadoop/journal</code> on all journalnodes completely filled, and NameNodes crashed as they were not able to get ACKs from the journalnodes that their writes had been saved.

We believe that after <code>/srv/backup/hadoop/namenode</code> filled up on May 22, the standby NameNode was no longer able to save its image to <code>/srv/hadoop/name/current</code>.  Because no new image was saved, the hadoop-namenode-backup-fetchimage did not detect that a new image was present, it did not try to take a new backup.  The hadoop-namenode-backup-prune kept purning backup files older than 20 days, freeing up space on the <code>/srv</code> partition.  

However, because the standby NameNode was not able to save its FS images snapshots, JournalNodes were not able to clear up historical edits files, which caused ''them'' to fill up their journal partitions.

After the NameNodes were recovered and out of safe mode, writes could proceed.  All ingestion is handled either via Kafka or periodic jobs, and these can resume from where they left off.  No lasting impact.",,,,,"The following ticket contains all actionable items. https://phabricator.wikimedia.org/T309649

These are:

* Make old journalnode edits files are cleaned properly now that namenodes are back online and saving fs image snapshots.
* Reduce <code>profile::hadoop::backup::namenode::fsimage_retention_days</code>, 20 is too many
* Create an alert for the freshness of the standby namenode's FSImage dump in <code>/srv/hadoop/name/current</code>
* Make sure journalnodes alert sooner about disk journalnode partition
* Check that bacula backups of fs image snapshots are available and usable
* Check that the alerting for disk space is correct on an-master hosts - since we seem not to have been alerted to <code>/srv/</code> becoming full on an-master1002

All have now been completed.",2022-05-31,Unknown,Unknown,Unknown,Data loss,Unknown,2022-05-31_Analytics_Hadoop_failure.wikitext
"{{Incident scorecard
| task = T309648
| paged-num = 0
| responders-num = 3
| coordinators = Brian King
| start = 2022-05-31 13:00 UTC
| end = 2022-07-12 14:00 UTC
| metrics = To the best of my knowledge, no SLOs affected
| impact = For 41 days, Cloudelastic was missing search results about files from commons.wikimedia.org.
}}

During a reimage operation, the cloudelastic Elasticsearch cluster lost a shard and went into red status, indicating data loss.

Until the data was restored, search results were incomplete on Cloudelastic. Restoration from production snapshots, using the previously understood and documented process, failed consistently, requiring a different approach to be devised which is why restoration was delayed by a month. Restoration was completed on 12 July. 

'''Documentation''':
*https://phabricator.wikimedia.org/T309648#8072778",,,,,"*Restore data to cloudelastic
*Document cloudelastic cluster (what is its purpose, who are the stakeholders, etc)
*Document restore process
*Review monitoring for cloudelastic
*Inform stakeholders of the current situation

<mark>TODO: Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFIRE (Pending Review & Scorecard)]] Phabricator tag to these tasks.</mark>",2022-06-01,Unknown,Unknown,Unknown,Data loss,Unknown,2022-06-01_Lost_index_in_cloudelastic.wikitext
"{{Incident scorecard
| task = 
| paged-num = 20
| responders-num = 4
| coordinators = Filippo
| start = 2022-06-10 14:57
| end = 2022-06-10 15:00
| impact = For 3 minutes wiki traffic was disrupted in multiple regions at the CDN layer, affecting both cached and logged-in responses with HTTP 5xx error responses.
}}

Lots of HTTP 429 from Varnish (due to ongoing rate-limiting) caused overload at the [[HAProxy]] level and general service disruption.

'''Documentation''':
*HTTP 503s reported on phab https://phabricator.wikimedia.org/T310368
*https://www.wikimediastatus.net/incidents/5k90l09x2p6k",,,,,"*(Re) evaluate effectiveness / usefulness of varnish/haproxy traffic drop alerts https://phabricator.wikimedia.org/T310608
*Mitigate/fix overload situations between varnish and haproxy https://phabricator.wikimedia.org/T310609",2022-06-10,Unknown,Unknown,Unknown,Service disruption,Unknown,2022-06-10_overload_varnish_haproxy.wikitext
"{{Incident scorecard
| task =T310431 
| paged-num = 
| responders-num = 
| coordinators = 
| start =2022-06-12 08:14 UTC 
| end =2022-06-12 08:44 UTC 
|impact=For about 30 minutes, backends were intermittently slow or partially unresponsive. This affected a portion of logged-in clients and uncached page views.}}
'''Documentation''':
*https://www.wikimediastatus.net/incidents/nb6v1zxp86ns",,,,,,2022-06-12,Unknown,Unknown,Unknown,Unresponsive,Unknown,2022-06-12_appserver_latency.wikitext
"{{Incident scorecard
| task = T310796
| paged-num = 0
| responders-num = 8
| coordinators = moritz
| start = 13:00
| end = 15:00
| impact = For about 2 hours, a current production database password was publicly known. No user-facing impact, and no data was compromised. While the incident broke an important security boundary, other boundaries (specifically, firewalls) prevented data compromise.
}}

While troubleshooting database issue on labtestwikitech, I (Andrew) dumped some internal data structures to stdout while debugging in PHP via <code>print</code> statements. These data structures contained database credentials. It took a bit for me to remember that because of how PHP works, <code>print</code> statements (also) write the response to web clients.

The standard practice for ad-hoc debugging is <code>wfDebugLog()</code>, see also [[Debugging in production#Ad-hoc%20log%20messages|Debugging in production]].

As soon as Sam Reed noticed the leakage there was a quick response and the password was rotated. Due to incomplete automation, rotating the password took quite some time (maybe 60-90 minutes with several SREs participating).

Because of firewalls and host-selective database grants, the leaked passwords are only useful from production hosts (10.64.0.0 and 10.192.0.0) so data integrity was not compromised.",,,,,"*document the repool script for dbctl in wikitech
*reconsider labtestwikitech. Decommission, or standardize to some degree so it isn't managed as an afterthought
**https://phabricator.wikimedia.org/T310795
*Improve automation:
**upgrade & document password-rotation script
**productionize repool script
*Consider improved pw redaction:
**The data shown by var_dump(), print_r() etc. has theoretically been configurable via the __debugInfo() magic method since PHP 5.4 (RFC). T277618 proposed using this mechanism (originally to reduce the size of the output, rather than to redact sensitive information), but found PHP bug 80894, which is only fixed in PHP 7.4 or later. Once WMF is on PHP 7.4 (T271736), we should consider using __debugInfo() to remove the password from the debug output of database objects. (And $wgDBpassword from globals / config?)
**In PHP 8.2, the \SensitiveParameter attribute can be used to redact function parameters from stack traces (RFC), though that’s less relevant for us since (I think?) we never show stack traces with values anyways (only value types).",2022-06-16,Unknown,Unknown,Unknown,Unknown,Unknown,2022-06-16_MariaDB_password_leak.wikitext
"{{Incident scorecard
| task = T309957
| paged-num = 0
| responders-num = 4
| coordinators = XioNoX
| start = 2022-06-21 14:32:00
| end = 2022-06-21 14:43:00
| impact = For 11 minutes, one of the Codfw server racks lost network connectivity. Among the affected servers was an LVS host. Another LVS host in Codfw automatically took over its load balancing responsibility for wiki traffic. During the transition, there was a brief increase in latency for regions served by Codfw (Mexico, and parts of US/Canada).
}}

[[File:Screenshot 20220621 185358.png|thumb|right|Codfw app servers suffered increased latency during the incident. The latencies affected only internal monitoring (health checks) because Codfw was not serving application traffic at this time.|320x320px]]During regular maintenance, there was a (scheduled) loss of power redundancy on the codfw-A1 server rack around 14:32:00 UTC.

While the servers in this rack did not lose power (given a redundant power supply), they ''did'' fully lose network connectivity and thus effectively went down. This happened because the second power cable for the ASW network switch was not plugged all the way in, resulting in an unscheduled full loss of the switch for that rack, and hence the rack's network connectivity.

Happily, higher-level service redundancy worked as expected:
* regarding [[LVS]], lvs2010 automatically took over from lvs2007, for CDN traffic to [[Codfw data center|Codfw]]. There was a very temporary increase on response latency for on-the-fly Codfw requests until traffic stabilized.
* ns1 [[DNS]] server was automatically moved to Eqiad, should not have any user impact.
* Most A2 servers alerted about loss of power redundancy, but having 2 power supplies they didn't go down.
* App servers could have been affected more, latency-wise while they were automatically depooled, but they were not serving production traffic at this time as [[Eqiad data center|Eqiad]] is the primary DC.

After the secondary power cord was properly connected, connectivity recovered with no issues. Maintenance finished at 15:01.",,,,,,2022-06-21,Unknown,Unknown,Unknown,Unknown,Unknown,2022-06-21_asw-a2-codfw_accidental_power_cycle.wikitext
"{{Incident scorecard
| task = T309957
| paged-num = 0
| responders-num = 7+
| coordinators = -
| start = 2022-06-30 15:23:00
| end = 2022-06-30 15:41:00
| impact = For approximately 18 minutes, servers in the A4-codfw rack lost network connectivity. Little to no external impact as affected services were either inactive in Codfw or had local redundancy.
}}

Network connectivity for the A4 codfw server rack went down due to full power loss of its switch. This caused lots of alert spam, but otherwise it had very little to no impact on users due to services not being pooled on codfw or redundancy working as intended.

This was very similar incident to [[Incidents/2022-06-21 asw-a2-codfw accidental power cycle]] (bump wrong cable on switch side again). See that page for more details.

After the secondary power cord was properly connected, connectivity recovered with no known issues. Power maintenance on that rack finished at 15:50.",,,,,,2022-06-30,Unknown,Unknown,Unknown,Unknown,Unknown,2022-06-30_asw-a4-codfw_accidental_power_cycle.wikitext
"{{Incident scorecard
| task = T310557
| paged-num = 26 (twice)
| responders-num = 10
| coordinators = lsobanski
| start = 2022-07-03 11:17:00
| end = 2022-07-03 11:33:00
| impact = For 16 minutes, edits and previews for pages with Score musical notes were too slow or unavailable.
}}
[[File:Shellbox Grafana 2022-07-03.png|thumb|Shellbox Grafana dashboard]]
An increase in Score requests (musical note rendering) from Parsoid overwhelmed the Shellbox service. This was mitigated by allocating more k8s pods to Shellbox. The overload took place from 11:17 to 11:33.

From the Grafana dashboard, we see that a majority of requests took over 5,000ms (5s) instead of less the usual 10ms (0.01s), and for two minutes 11:25-11:27 requests actually failed with HTTP 503 instead.

From the Logstash dashboard, we measure approximately 35,000 failed requests during this time. Of which 99.9% were from Parsoid, for de.wikipedia.org requests that render musical notes through the Score extension. 26K received HTTP 503, and 9K received HTTP 504. The remaining 0.1% were edits or pageviews calling Shellbox for syntax highlighting. 

See [[Incidents/2022-07-11 Shellbox and parsoid saturation|2022-07-11 Shellbox and parsoid saturation]] for re-occurrence and follow up action items.
[[File:Logstash shellbox 2022-07-03.png|thumb|Shellbox service errors in Logstash]]
'''Documentation''':
*[https://grafana.wikimedia.org/d/RKogW1m7z/shellbox?orgId=1&var-dc=eqiad%20prometheus%2Fk8s&var-service=shellbox&var-namespace=shellbox&var-release=main&from=1656843600000&to=1656852600000 Shellbox requests dashboard]",,,,,"*[[phab:T310557|T310557: Improving Shellbox resource management]]
*[[phab:T312319|T312319: Reduce Lilypond shellouts from VisualEditor]]",2022-07-03,Unknown,Unknown,Unknown,Unknown,Unknown,2022-07-03_shellbox_request_spike.wikitext
"{{Incident scorecard
| task =T312722 
| paged-num = 
| responders-num =3 
| coordinators = 
| start = 
| end = 
|impact=For several days, Thumbor p75 service response times gradually regressed by several seconds.}}

Due to a uptream bug introduced in a firejail update, Thumbor constantly restarting itself. This lead to increased error rates, and also increased delays from the HAProxy in front of it.

{{TOC|align=right}}",,"<mark>Write how the issue was first detected.  Was automated monitoring first to detect it? Or a human reporting an error?</mark>

<mark>Copy the relevant alerts that fired in this section.</mark>

<mark>Did the appropriate alert(s) fire? Was the alert volume manageable? Did they point to the problem with as much accuracy as possible?</mark>

<mark>TODO: If human only, an actionable should probably be to ""add alerting"".</mark>",,,"*…

<mark>Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.</mark>

<mark>Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFIRE (Pending Review & Scorecard)]]  Phabricator tag to these tasks.</mark>",2022-07-10,Unknown,Monitoring,Unknown,Unknown,Unknown,2022-07-10_thumbor.wikitext
"{{Incident scorecard
| task = 
| paged-num = 26
| responders-num = 5
| coordinators = Brandon Black, Arnold Okoth
| start = 2022-07-11 19:30:00
| end = 2022-07-11 19:36:00
| metrics = No relevant SLOs exist
| impact = For 5 minutes, the MediaWiki API cluster in eqiad responded with higher latencies or errors to clients.
}}
<!-- Reminder: No private information on this page! -->

There was an increase in requests to the API cluster that resulted in reduction of availabe PHP workers and a database host (db1132) running out of available connections. This database host runs MariaDB 10.6 which is known to be sensitive to high load. This resulted in an increase in latency and errors returned to clients. The spike auto-recovered and it's not clear what the exact root cause was. 

'''Documentation''':
*[https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red?from=1657566000000&orgId=1&to=1657573200000&var-cluster=api_appserver&var-site=eqiad&var-method=GET&var-code=200&var-php_version=proxy:unix:%2Frun%2Fphp%2Ffpm-www.sock.* Appservers RED dashboard]",,,,,*https://phabricator.wikimedia.org/T311106 - investigate mariadb 10.6 performance regression during spikes/high loads.,2022-07-11,Unknown,Unknown,Unknown,Unknown,Unknown,2022-07-11_FrontendUnavailable_cache_text.wikitext
"{{Incident scorecard
| task = T312319
| paged-num = 2
| responders-num = 5
| coordinators = Simon
| start = 2022-07-11 13:14:00
| end = 2022-07-11 14:26:00
| impact = For 13 minutes, mobileapps service was serving HTTP 503 errors to clients.
}}

The reason appears to be background parsing associated with VisualEditor. The MWExtensionDialog as used in Score has the default 0.25s debounce preview, meaning we're shelling out to Lilypond through Shellbox every quarter-second while the user is typing -- regardless of whether an existing shellout is in flight. That's reasonable for lots of parsing applications that take much less time than that, but for something as heavy as these score parses, we should extend that interval, which would have the effect of cutting down on the request rate to shellbox.

{{TOC|align=right}}",,* 13:14 (ProbeDown) firing: Service shellbox:4008 has failed probes (http_shellbox_ip4) #page - [[Network monitoring#ProbeDown|https://wikitech.wikimedia.org/wiki/Network_monitoring#ProbeDown]] - https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=All,"[[File:Mobileapps 2022-07-11 grafana.png|thumb|Mobileapps spike in HTTP errors from 13:25-13:38.]]

* 13:14 (ProbeDown) firing: Service shellbox:4008 has failed probes (http_shellbox_ip4) #page
* 13:18 Discussion on -security about the nature of the issue. URLs mentioned above seen as heavy hitters.
* 13:34 Parsoid and shellbox recovering  RECOVERY - Mobileapps LVS eqiad on mobileapps.svc.eqiad.wmnet is OK: All endpoints are healthy [[Mobileapps (service)|https://wikitech.wikimedia.org/wiki/Mobileapps_%28service%29]]
* 13:37  Incident document created.  Simon becomes IC.
* 13:21 Incident agreed as resolved.
* 13:19 Paging again:  <jinxer-wm> (ProbeDown) firing: Service shellbox:4008 has failed probes (http_shellbox_ip4)
* 14:22 Continuing the same document and IC.
* 14:26 (ProbeDown) resolved: Service shellbox:4008 has failed probes (http_shellbox_ip4) #page 
* 14:48 Follow up page a result of spillover from initial incident.","===What went well?===

*automated monitoring detected the incident
*Had a good amount of incident responders","* Reduce Lilypond shellouts from VisualEditor https://phabricator.wikimedia.org/T312319
* Update MobileApp to set a proper User-Agent https://phabricator.wikimedia.org/T314663
* Add X-IP header to proxied traffic",2022-07-11,Unknown,Monitoring,Unknown,Unknown,Unknown,2022-07-11_Shellbox_and_parsoid_saturation.wikitext
"{{Incident scorecard
| task = T309957
| paged-num = 26
| responders-num = 9
| coordinators = Brandon Black
| start = 2022-07-12 15:45:00
| end = 2022-07-12 16:00:00
| impact = No apparent user-facing impact, but lots of internal clean up, e.g. for Ganeti VMs.
}}During the scheduled maintenance to upgrade the PDUs in rack A5, CyrusOne flipped the incorrect breaker on the breaker panel, prior to pulling the PDU's power cord out from its circuit.  This resulted in all servers in rack A5 losing power to both its primary and secondary power feeds.  The affected hardware in rack A5 booted back up, once CyrusOne realized the mistake and flipped the breaker back on.

* 15:45 <+icinga-wm> PROBLEM - Host graphite2003 #page is DOWN: PING CRITICAL - Packet loss = 100%
* 15:45 <+icinga-wm> PROBLEM - Host maps2005 is DOWN: PING CRITICAL - Packet loss = 100%
* 15:55 <+icinga-wm> PROBLEM - MariaDB read only s8 #page on db2079 is CRITICAL: Could not connect to localhost:3306
* 15:56 <+icinga-wm> PROBLEM - MariaDB read only m1 #page on db2132 is CRITICAL: Could not connect to localhost:3306
* ..
* 16:00 <+icinga-wm> RECOVERY - MariaDB read only s8 #page on db2079 is OK",,,,,,2022-07-12,Unknown,Unknown,Unknown,Unknown,Unknown,2022-07-12_codfw_A5_powercycle.wikitext
"{{Incident scorecard
| task = 
| paged-num = 10
| responders-num = 3
| coordinators = Filippo
| start = 2022-07-13 13:42
| end = 2022-07-13 14:09
| impact = For 20 minutes, there was a small increase in error responses for images served from the Eqsin data center (Singapore).
}}

Brief outbound bandwidth spike for upload in [[Eqsin data center|Eqsin dc]]. Recovered by itself.

* [https://docs.google.com/document/d/15pEoJM05a8oS2LHAyq3F0ZHMpvGUO3dS6aANAzl1x38/edit# Restricted details].
* [https://grafana.wikimedia.org/d/-K8NgsUnz/home?orgId=1&from=1657717200000&to=1657722600000 Grafana]",,,,,"* [[phab:T310997|T310997]] Improve webrequest log
* https://gerrit.wikimedia.org/r/c/operations/puppet/+/768723",2022-07-13,Unknown,Unknown,Unknown,Unknown,Unknown,2022-07-13_brief_outbound_bandwidth_spike_eqsin.wikitext
"{{Incident scorecard
| task = T313382
| paged-num = 11
| responders-num = 3
| coordinators = rzl
| start = 2022-07-20 03:16
| end = 2022-07-20 03:25 (mostly)
| metrics = No relevant SLOs exist. See impact metrics below.
| impact = The network was partitioned for 6 minutes.
* About 970,000 external requests failed to reach varnish-frontend during 11 minutes, or 0.2% of total traffic.
* About 1.2 million requests failed to reach appservers (app and API combined) during 16 minutes, or 26% of expected ''uncached'' traffic. This is higher as it includes certain internal requests.
* CDN HTTP 5xx error briefly increased from under 1/s to 10-20/s.
* Phabricator was unavailable for 32 minutes.
* The Kubernetes API was at least partially unavailable for 52 minutes, but during a period where no control operations are normally in progress.
}}
<!-- Reminder: No private information on this page! -->[[File:CDN and appserver 2022-07-20 grafana.png|thumb|CDN and appserver impact]]
At 03:16, the top-of-rack switch asw2-c-eqiad virtual chassis lost connectivity to FPC5, partitioning the network. This caused a hard down event for all hosts in rack C5 ([https://netbox.wikimedia.org/dcim/racks/21/ netbox]). It also caused additional instability due to how the virtual chassis works, and because it's incorrectly cabled up.

We received a burst of both paging and non-paging alerts: Icinga reporting hosts down; BGP status; application-level errors; and MariaDB replica alerts. At least one user also reported via IRC that they couldn't access metawiki (almost certainly uncacheable traffic, due to logged-in state).

At 03:22, asw2-c-eqiad:fpc5 came back online. Most systems recovered automatically, but some needed manual attention:

* We received [[HAProxy#Failover|HAProxy failover]] alerts on dbproxy1018 through 1021, and those needed to be resolved by reloading haproxy manually, as expected.
* Phabricator's dbproxy had failed over to a read-only replica (as expected) but Phabricator was unavailable for read-only tasks in read-only mode. When users attempted to view a task, they got an error page saying, <code>Unhandled Exception (""AphrontQuery Exception"") #1290: The MariaDB server is running with the --read-only option so it cannot execute this statement</code> This was resolved by reloading haproxy, but Phab was expected to be available for reads.
* The Kubernetes API server alerted for high latency until kube-apiserver was [[Kubernetes#Restarting the API server|manually restarted]] on both hosts. 
Documentation:

* [https://grafana.wikimedia.org/d/-K8NgsUnz/home?orgId=1&from=1658282400000&to=1658293200000 Grafana dashboard: Home]",,,,,"*[[phab:T313384|T313384]] Recable eqiad row C switch fabric, so that in the future a failure like this will only impact servers in rack C5.
*[[phab:T313382#8090176|T313382#8090176]] Move critical hosts, like DB masters, away from rack C5 until its top-of-rack switch is trustworthy.
*{{Done}} [[phab:T313382#8090224|T313382#8090224]] Add LibreNMS alerting (and [[Network monitoring#virtual-chassis crash|runbook]]) for this scenario, which will speed up troubleshooting.
*[[phab:T313879|T313879]] Make read-only Phabricator operations possible when its database is in read-only mode.",2022-07-20,Unknown,Unknown,Unknown,Unknown,Unknown,2022-07-20_network_interruption.wikitext
"{{Incident scorecard
| task = T314941
| paged-num = 
| responders-num = 
| coordinators = Eric Evans
| start = 2022-08-10 12:55:00
| end = 2022-08-10 18:22:00
| impact = During planned downtime, other hosts ran out of space due to accumulating logs. No external impact.
}}

A subset of redundant Cassandra hosts underwent a scheduled administrative shutdown to conduct power maintenance in Codfw. During the planned outage, other Cassandra hosts were running out of disk space due to accomulating log files that Cassandra uses to repair clusters after downtime. The issue was resolved by bringing the remaining Cassandra nodes back online, ahead of schedule.",,,"Free space became critically low on a volume housing auxiliary data on many [[Cassandra]] hosts.

A number of Cassandra hosts in codfw ([[RESTBase]] cluster) were administratively taken down to conduct PDU maintenance.  The downtime scheduled was limited to hosts in the same row, a condition this cluster has been configured to tolerate; There was expected to be no impact.  However, during the planned outage, the Hinted-handoff writes resulted in unexpectedly high utilization of the corresponding storage volumes on hosts located in the eqiad datacenter.

From the [https://cassandra.apache.org/doc/trunk/cassandra/operating/hints.html#hinted-handoff Cassandra documentation]:<blockquote>Hinting is a data repair technique applied during write operations. When replica nodes are unavailable to accept a mutation, either due to failure or more commonly routine maintenance, coordinators attempting to write to those replicas store temporary hints on their local filesystem for later application to the unavailable replica.</blockquote>As [[Eqiad data center|eqiad]] was the active data-center at the time of the maintenance, nodes there served as coordinators for [[Codfw data center|codfw]] replicas, and as such were tasked with storing hinted writes for the down hosts.  Hints are stored for a configurable period of time (<code>max_hint_windowin_ms</code>), 3 hours in our configuration, after which they are truncated.  While the loss of an entire row is something we had designed/planned for, it is not something that we have ever tested, and the storage provided is simply not large enough to hold the needed data.","=== Volume Sizing ===
Ostensibly, you would need a storage device large enough to hold <code>max_hint_windowin_ms</code> (currently 3 hours) worth of writes, for as many nodes as might go down.  This requires knowing not only write throughput at the time of provisioning, but also the number of nodes in the cluster (both of which are likely to change over time).  Even if you could reliably predict these values, the pathological worst-case (a partition) would require storing hints for N-1 nodes (where N is the total number of nodes), this does not seem practical.  

As hinted-handoff is an only an optimization, we should focus instead on sizing storage to cover the common case (random transient node outages), and be prepared to deal with exceptional circumstances by disabling hints and/or truncating storage.

In the current example, hint storage is quite small (~3G after space for commitlog), and we should consider provisioning more for future clusters to make them less sensitive to this sort of event.  However since we'll never size them large enough to rule it out, and since this is a first occurrence, it is probably not worth taking action to retrofit.","*[[phab:T315517|T315517]] Create (and document) a process for disabling hinted-handoff during maintenance events
*[[phab:T315517|T315517]] Create (and document) a process for truncating hinted-handoff 
*[[phab:T315517|T315517]] Ensure that future clusters have a dedicated storage volume for hinted-handoff
*[[phab:T315517|T315517]] Establish (and document) best practice for sizing of hinted-handoff volumes",2022-08-10,Unknown,Unknown,Unknown,Downtime,Unknown,2022-08-10_cassandra_disk_space.wikitext
"{{Incident scorecard
| task = T313825
| paged-num = 0
| responders-num = 3
| coordinators = Jaime
| start = 2022-08-10 10:32:00
| end = 2022-08-10 13:31:00
| impact = No external impact.
}}

A Puppet patch ([[gerrit:c/operations/puppet/+/817307|change 817307]]) was merged which would accidentally install confd on a significant number of production hosts. The Puppet provisioning for these confd installations failed half-way due to having no corresponding Icinga checks defined. This in turn fired an Icinga alert:
 10:44 <icinga-wm> PROBLEM - Widespread puppet agent failures on alert1001 is CRITICAL: 0.1057 ge 0.01 
Engineers started work on reversing it, by cleaning up the inadvertent installs of confd via Cumin. Security posture was not compromised and there was no external user impact.

'''Documentation''':

* puppet failures - https://logstash.wikimedia.org/goto/a5b60af08e257d90a469a78d12056ec2
*[[Confd]]
* [[Cumin]]",,,,,"* Git defaults to shows the author's date, not the commit date. Consider adding the following aliases to your git config as fix:
** <code>lola = log --graph --pretty=format:\""%C(auto)%h%d%Creset %C(cyan)(%cr)%Creset %C(green)%cn <%ce>%Creset %s\"" --all</code>
** <code>grephist = log --graph --pretty=format:\""%C(auto)%h%d%Creset %C(cyan)(%cr)%Creset %C(green)%cn <%ce>%Creset %s\"" --all -S</code>",2022-08-10,Unknown,Unknown,Unknown,Unknown,Unknown,2022-08-10_confd_all_hosts.wikitext
"{{Incident scorecard
| task = T315350
| paged-num = 0
| responders-num = 8
| coordinators = TheresNoTime
| start = 2022-08-16 17:57:00
| end = 2022-08-17 00:57:00
| metrics = beta has a best effort SLA
| impact = For 7 hours, all Beta Cluster sites were unavailable. This also affected daily Selenium test jobs.
}}
After an inadvertent restart of some WMCS cloudvirts and their associated VMs, all sites within the Beta Cluster (e.g. https://meta.wikimedia.beta.wmflabs.org/wiki/Main_Page) failed to load, with ''<code>Error: 502, Next Hop Connection Failed</code>'' — this persisted post-restart of the relevant VMs.

Drafting: possibly an apache config/puppet failure (https://phabricator.wikimedia.org/T315350#8159826), restarting trafficserver seems to have fixed it (https://phabricator.wikimedia.org/T315350#8159954)

The incident was complicated by the lack of Beta Cluster's maintenance meaning ongoing ""normal"" errors distracted from the cause.

{{TOC|align=right}}",,"* [[User:Samtar|TheresNoTime]] has an uptime monitor at https://uptime.theresnotime.io/status/wmf-beta which ""paged"" her
* User reports
* CI errors from beta sync","''All times in UTC.''

*2022-08-16 17:57 '''OUTAGE BEGINS:''' Uptime monitoring ""pages"" [[User:Samtar|TheresNoTime]], who informs #wikimedia-operations & #wikimedia-releng
**This coincides with the WMCS VM accidental restarts ([Cloud] [Cloud-announce] Some cloud-vps servers just rebooted)
*2022-08-16 18:21 [[phab:T315350|T315350]] logged
*2022-08-17 00:57 '''OUTAGE ENDS'''","===What went well?===

* A number of volunteers were available to triage","* [[phab:T315379|logspam watch broken on beta]] {{done}}
* [[phab:T315394|Remove two cherry-picked reverts from deployment-puppetmaster04]] {{done}}
* [[phab:T315395|Rebase & merge or re-cherry-pick 668701 on deployment-puppetmaster04]]
* [[phab:T315386|Replace certificate on deployment-elastic09.deployment-prep]] {{done}}
* [[phab:T315695|Add basic alerting]]

<mark>Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.</mark>

<mark>Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFIRE (Pending Review & Scorecard)]]  Phabricator tag to these tasks.</mark>",2022-08-16,Unknown,User reports,Unknown,Unknown,Unknown,2022-08-16_Beta_Cluster_502.wikitext
"{{Incident scorecard
| task = T315274
| paged-num = 9
| responders-num = 7
| coordinators = CWhite
| start = 04:07:58
| end = 04:43:22
| metrics = 
| impact = For 36 minutes, errors were noticeable for some editors. While saving edits was unaffected (verified by edit rate), the errors looked like they could impact editing due to appearing during edit-related actions. ~1 hour of volatile cache data was lost.
}}

When replication broke between local MainStash databases, a previously undiscovered bug in how MediaWiki handles failures of local MainStash replicas, produced user-noticeable exceptions (mainly during edit-related activities).

Unlike thought at the time, write failures to the MainStash database were not fully prevented from being noticed by MediaWiki's shutdown handler. ChronologyProtector was (wrongly) enabled for the [[X2|x2 database section]], and so similar to core databases, MainStash required at least one replica to be up and not lagged in order to allow MainStash writes to succeed.

{{TOC|align=right}}

Earlier in the day, cross-dc replication for x2 databases broke, this was caused by the application requiring <code>STATEMENT</code>-based replication while the databases were (wrongly) configured with the incompatible <code>ROW</code>-based replication. This caused a split brain state, where eqiad and codfw drifted apart in their respective datasets. This by itself had '''low to no impact''' as codfw traffic at the time was restricted to testwiki, test2wiki and mediawiki.org. Service itself would have not been impacted even if codfw received more significiant traffic as MainStash does not require or observe data consistency.

In response to alerts about x2 replication breakage, operators tried to fix the then lower priority issue, by running <code>SET GLOBAL sql_slave_skip_counter = X</code>, for values of 1 and 2 a few times. This caused replication to break further, also affecting local replicas within Eqiad. Current understanding of MainStash service only requires the primary DB to be up and tolerates replica failure, so replication breaking or stopping within a datacenter shouldn't cause an outage, as the replicas are meant to be passive standby hosts. However, because [[phabricator:T312809|ChronologyProtector was still enabled]], this caused MainStash to observe the replication lag and thus prevent MainStash writes to the primary DB. These write failures in turn triggered a bug that allowed MainStash write failures to be insufficiently silenced and thus cause user-noticeable error messages in certain editing-related MediaWiki actions. Saving of edits was unaffected (as verified by edit rate metric), although error rates were comparable to edit rates in terms of volume:

 Explicit transaction still active; a caller might have failed to call endAtomic() or cancelAtomic().

While a few other things were attempted, such as wiping out the ephemeral MainStash dataset and resetting replication, the way replication was restored was by:
# Switching active write hosts to STATEMENT-based replication.
# Disabling GTID (this fixed the cross-dc replication).
# Skipping statements on the replicas with CHANGE MASTER until they started replicating STATEMENT binlog positions.

Approximately 1 hour of MainStash data from x2 was lost and the servers ended up replicating but with different data each. Because both data and logs were purged at earlier states of debugging, later root cause findings were much harder to find.

{{note|'''Edit:''' The root cause was initially understood as an operator error. Upon further research, we understand that operator actions were informed by MainStash service expectations, that the chosen mitigation would be safe to perform even if (as it did) caused the degraded X2 database to fail further. There was an unknown application bug leading to hard failures when X2 is down. The incicent coordinator decided to give more weight to this bug as the ''real'' cause of the incident.}}",,"[https://wm-bot.wmflabs.org/libera_logs/%23wikimedia-operations/20220816.txt 2 alert pages] were sent as soon as internal replication broke on the primary x2 servers. This alert was promptly attended.

The user-facing outage started later, with more pages about the replicas. Although we're unsure if sufficiently different from the first 2 pages to indicate the severity. User reports reached SRE at this point too, but people were already working on a resolution by then.","''All times in UTC.''
[[File:Atomic MediaWiki errors.png|thumb|right|Exception error log during the incident]]
[[File:Atomic edit rate.png|thumb|right|Edit rate during the incident]]
* '''01:38:00''' mediawiki.org [https://gerrit.wikimedia.org/r/c/operations/puppet/+/823113 moved to multi-dc]
* '''03:08:12''' <code><nowiki>Delete query sent to codfw x2 primary: #220816  3:08:12 server id 180363291  end_log_pos 1038450247 CRC32 0xcc3de9f1   Annotate_rows:
#Q> DELETE /* SqlBagOStuff::deleteServerObjectsExpiringBefore  */ FROM `objectstash` WHERE (exptime < '20220816020812') AND keyname IN ('azwikibooks:captcha:1072943127',...)</nowiki></code> (full query on internal doc)
Note this was the query that blew up, not necessarily the one that triggered the issue- logs and data were deleted so research is hard- the main thing is both primary servers had, even if for some time, different data when executing this.
* '''03:08:23''' <code>SERVICE ALERT: db2144;MariaDB Replica SQL: x2 #page;CRITICAL;SOFT;1;CRITICAL slave_sql_state Slave_SQL_Running: No, Errno: 1032, Errmsg: Could not execute Delete_rows_v1 event on table mainstash.objectstash: Can't find record in 'objectstash', Error_code: 1032: handler error HA_ERR_KEY_NOT_FOUND: the event's master log db1151-bin.000810, end_log_pos 5460559</code>
* '''03:10:03''' <code>SERVICE ALERT: db1151;MariaDB Replica SQL: x2 #page;CRITICAL;SOFT;1;CRITICAL slave_sql_state Slave_SQL_Running: No, Errno: 1032, Errmsg: Could not execute Delete_rows_v1 event on table mainstash.objectstash: Can't find record in 'objectstash', Error_code: 1032: handler error HA_ERR_KEY_NOT_FOUND: the event's master log db2144-bin.000813, end_log_pos 1038458517</code> - At this moment, there is a split brain, but given the, at the time, load on codfw, no big user impact.
* '''03:42''' Cwhite escalated by page to Amir, Manuel, Jaime. Amir was waken up by this. (Manuel was on holidays at the time)
* '''03:54''' Reverted multi-dc patch https://gerrit.wikimedia.org/r/c/operations/puppet/+/823231
* '''04:07:58''' Amir tries a few things on db1151: <code>START/STOP slave</code>, <code>sql_skip_slave_counter</code> several times ('''OUTAGE STARTS HERE'''), then <code>TRUNCATE TABLE</code>, then <code>RESET SLAVE</code>, unsuccessfully.
* '''04:08:07''' <code>SERVICE ALERT: db2143;MariaDB Replica IO: x2 #page;CRITICAL;SOFT;1;CRITICAL slave_io_state Slave_IO_Running: No, Errno: 1236, Errmsg: Got fatal error 1236 from master when reading data from binary log: 'could not find next log: the first event 'db2144-bin.000670' at 517376598, the last event read from 'db2144-bin.000813' at 1044345737, the last byte read from 'db2144-bin.000813' at 1044345768.'</code>
* '''04:20''' Amir called in Jaime
* '''04:26''' Incident opened. cwhite becomes IC.
* '''04:37''' Switched MainStash back to Redis patch is prepared (never deployed) https://gerrit.wikimedia.org/r/c/operations/mediawiki-config/+/823275/
* '''04:39''' Jaime switched master to STATEMENT, flushed logs to apply it
* '''04:39''' Switched codfw master to disable GTID, that made it work (this was done on codfw first as a test)
* '''04:39''' Switched eqiad replicas to both disable GTID and jump to latest STATEMENT coord, skipping ~1h of data. Untouched codfw replicas for debugging later. Replicas start caching up.
* '''04:43:22''' “Explicit transaction still active” errors stopped. '''OUTAGE ENDS HERE'''
* '''04:44''' Reverted switching MainStash back to Redis https://gerrit.wikimedia.org/r/c/operations/mediawiki-config/+/823232
* '''04:45:40'''  SERVICE ALERT: db1152;MariaDB Replica Lag: x2 #page;OK;HARD;10;OK slave_sql_lag Replication lag: 0.00 seconds
* '''04:48''' eqiad X2 replication restored, users report able to save edits again
* '''05:00''' Decision is made to leave replication broken on db2142 and db2143 for forensics during working hours.
<!-- Reminder: No private information on this page! -->","===What went well?===

* Coordination of responders and decisions went as well as it can be organized- pages were attended, people escalated the issue appropriately and promptly
* This didn't happen on a regular metadata or content section db, which would have required mass, multi-terabyte-data backup recovery
* Decision to depool codfw for mediawiki was taken quickly","* {{phab|T315427}} Switch x2 to statement-based replication, and in general re-review db configuration (disable GTID, read-write, slave_exec_mode)
* Review runbooks/procedures for x2 (and parsercache), specifically regarding replication issues
** E.g. Sometimes doing nothing and having a split brain is better than trying to fix stuff manually (e.g. waiting & depooling a dc, and cloning afterwards)
* Restore replication on codfw replicas, repool them, remove downtimes/silences, recheck read-only mode {{done}}
* Validate MW’s concept of multi-master conflict resolution by performing simultaneous writes and simultaneous purges on both DCs
** As far as I was told, UPSERTs seemed to work but purges may need review (DELETE IGNORE?)
* Fix uncaught exception from LoadBalancer::approvePrimaryChanges() which caused total failure rather than graceful failure https://gerrit.wikimedia.org/r/c/mediawiki/core/+/823791
* Remove chronology protector checks so databases don't go into read only if local replicas crash or get replication broken: {{phab|T312809}}
* Re-enable multi-DC mode on testwiki, test2wiki and mediawikiwiki https://gerrit.wikimedia.org/r/c/operations/puppet/+/824039
* Make attempts to mistakenly depool a primary db have a more helpful message: {{phab|T314658}}
* '''See tasks {{phab|T315271}}''' (initial replication breakage/multi-dc implications) and '''{{phab|T315274}}''' (user visible outage, mw errors, within-dc replication issues) for discussion. See https://docs.google.com/document/d/1_nDpRvLEK9dGI2XVNeZVC1TcA89EYBCptiVCKItlWug for full logs and other data.",2022-08-16,Unknown,User reports,Unknown,Unknown,Unknown,2022-08-16_x2_databases_replication_breakage.wikitext
"{{Incident scorecard
| task = T316188
| paged-num = 
| responders-num = 
| coordinators = 
| start = 
| end = 
| impact = For approximately 17h, media requests (mostly thumbnail reads, but also uploads) failed at a rate of ~2000 HTTP errors per minute. This caused approximately 1.9 million uncached media requests to upload.wikimedia.org to fail, and 609 unsuccessful upload attempts (this part was quite visible to Commons contributors).
}}

…

<!-- Reminder: No private information on this page! --><mark>Summary of what happened, in one or two paragraphs. Avoid assuming deep knowledge of the systems here, and try to differentiate between proximate causes and root causes.</mark>

{{TOC|align=right}}",,"<mark>Write how the issue was first detected.  Was automated monitoring first to detect it? Or a human reporting an error?</mark>

<mark>Copy the relevant alerts that fired in this section.</mark>

<mark>Did the appropriate alert(s) fire? Was the alert volume manageable? Did they point to the problem with as much accuracy as possible?</mark>

<mark>TODO: If human only, an actionable should probably be to ""add alerting"".</mark>","<mark>Write a step by step outline of what happened to cause the incident, and how it was remedied.  Include the lead-up to the incident, and any epilogue.</mark>

<mark>Consider including a graphs of the error rate or other surrogate.</mark>

<mark>Link to a specific offset in SAL using the SAL tool at https://sal.toolforge.org/ ([https://sal.toolforge.org/production?q=synchronized&d=2012-01-01 example])</mark>

''All times in UTC.''

*00:00 (TODO) '''OUTAGE BEGINS'''
*00:04 (Something something)
*00:06 (Voila) '''OUTAGE ENDS'''
*00:15 (post-outage cleanup finished)

<!-- Reminder: No private information on this page! -->
<mark>TODO: Clearly indicate when the user-visible outage began and ended.</mark>",<mark>OPTIONAL: General conclusions (bullet points or narrative)</mark>,"*…

<mark>Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.</mark>

<mark>Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFIRE (Pending Review & Scorecard)]]  Phabricator tag to these tasks.</mark>",2022-08-24,Unknown,Monitoring,Unknown,Unknown,Unknown,2022-08-24_swift.wikitext
"{{Incident scorecard
| task = T316337
| paged-num = 0
| responders-num = 5
| coordinators = [[User:Jcrespo|Jcrespo]]
| start = 2022-08-26 09:56:00
| end =  2022-08-26 10:13:00
| metrics = no published SLOs
| impact = For approximately 17 minutes, some users accessing Wikimedia services had their cookies/session headers being removed, causing them to be logged out and unable to log in, effectively making any logged action impossible. It didn't affect MediaWiki, but it affected other services, the one that was most noticeable was Phabricator.
}}

{{TOC|align=right}}
After testing https://gerrit.wikimedia.org/r/c/operations/puppet/+/826785 in cp6016. ATS layer prevented Phabricator's (and theoretically, other services too, but it was less impacting) session cookies reaching the service's origin server. <code>474fb2d</code> didn't work as expected because it hit an ATS bug/missdocumented feature. Cookie data was stored in ts.ctx during <code>do_global_post_remap()</code> and restored in <code>do_global_cache_lookup_complete()</code> but for some reason ts.ctx gets wiped in the middle of those two hooks. 474fb2d also missed the step now performed in hide_cookie_store_response().

After the change was reverted, login issues stopped and users were able to perform regular Phabricator logged-in actions.",,"The issue was quickly pointed out by several people on IRC, in the SRE channel as it impacted highly visible ongoing work of several engineers and volunteers (dcaro, dhinus, jynus, claime).

No alerts where sent because it only affected logged-in users, while the site acted normally for anonymous users.","''All times in UTC.''

* '''[09:56]''' <vgutierrez> !log testing https://gerrit.wikimedia.org/r/c/operations/puppet/+/826785 in cp6016 '''OUTAGE STARTS HERE'''
* '''[10:07]''' <dcaro_away> anyone is doing anything with Phabricator? I'm getting logged out after any action. Several other users agree they are logged out and on log-in they get logged out again.
* '''[10:16]''' ''Incident opened.''  Jcrespo becomes IC.
* '''[10:13]''' <vgutierrez> !log stop testing https://gerrit.wikimedia.org/r/c/operations/puppet/+/826785 in cp6016 '''OUTAGE STOPS HERE''' (but may be affected for longer, as they realize later they are being logged out)
* '''[10:18]''' <vgutierrez> if RhinosF1 is affected, my experiment is unrelated
* '''[10:23]''' Jynus reaches out to Hashar (release engineering)
* '''[10:26]''' People seem to be able to log in again, and stay logged
* '''[10:29]''' Antoine texts Andre
* '''[10:33]''' <andre> I don't see any pointers in https://phabricator.wikimedia.org/people/logs/query/all/ , but indeed a lot of folks seem to get stuck at ""Login: Partial Login""
* '''[10:35]''' The issue is identified as traffic-related: <vgutierrez> cp6016 was stripping the phab session cookie and returning a hit for an anonymous user
* '''[10:35]''' The ticket is created https://phabricator.wikimedia.org/T316337
* '''[10:47]''' ''Issue considered resolved.'' Followups to continue on ticket.","This probably should be written by the service owner, but in my understanding, a new test was setup on production with an undetected bug, causing traffic disruption (cookie filtering). I don't know if there is much else to do as the same bug is unlikely to hit again; except maybe enabling some kind of automatic monitoring detection of a similar issue.",* Improve monitoring to detect ability to log in/perform logged in actions on production Phabricator (?),2022-08-26,Unknown,Unknown,Unknown,Unknown,Unknown,2022-08-26_Phabricator_login_issues.wikitext
"{{Incident scorecard
| task = T317340
| paged-num = 1
| responders-num = 3
| coordinators = claime
| start = 2022-09-08 15:18:18
| end = 2022-09-08 15:51:18
| metrics = Response time and 5xx rate
| impact = For 2 minutes, appserver and api_appserver and in Codfw were in a degraded state.
For 16 minutes, parsoid in codfw was in a degraded state.
}}

An Nginx server restart (RC) triggered an <code>etcdmirror</code> outage that started affecting end-users during a subsequent MediaWiki deployment. The etcd outage led to php-fpm not being able to contact its configuration server and failing to restart for the deployment. The appservers got depooled because of the failure until pybal depool protection kicked in. When etcdmirror was restarted to resolve the restart issue, the configuration state with the depooled servers was synchronized, which triggered the depooling of 50% of codfw api-https, api_appserver, appserver, and parsoid servers.

{{TOC|align=right}}",,"<mark>Write how the issue was first detected.  Was automated monitoring first to detect it? Or a human reporting an error?</mark>

claime reports errors during scap sync-file, jayme picks up on conf2005/etcdmirror being in a CRITICAL state

<mark>Copy the relevant alerts that fired in this section.</mark>

15:22:02   +icinga-wm | PROBLEM - etcdmirror-conftool-eqiad-wmnet service on conf2005 is CRITICAL: CRITICAL - Expecting active but unit etcdmirror-conftool-eqiad-wmnet is failed

<mark>Did the appropriate alert(s) fire? Was the alert volume manageable? Did they point to the problem with as much accuracy as possible?</mark>

Alert fired on IRC but no page went out.

<mark>TODO: If human only, an actionable should probably be to ""add alerting"".</mark>","[[File:20220908 1532-1538-appserver latency.png|thumb|appserver latency ([https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red?orgId=1&var-site=codfw&var-cluster=appserver&var-method=GET&var-code=200&var-php_version=All&from=1662651131187&to=1662651531187 Grafana])]]
[[File:20220908_1532-1538-api_appserver-latency.png|thumb|api_appserver latency ([https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red?orgId=1&var-site=codfw&varcluster=api_appserver&var-method=GET&var-code=200&var-php_version=All&from=1662651131187&to=1662651531187 Grafana])]]''All times in UTC.''
*15:11: <wikibugs> (Merged) jenkins-bot [mediawiki-config] - https://gerrit.wikimedia.org/r/830803 ([[phab:T317025|T317025]])
*15:17:21: moritzm updates nginx-light on conf1009, the update triggers a daemon restart (port 4001 on the conf* hosts serves the etcd tlsproxy which is accessed by etcdmirror)
*15:17:24: conf2005 systemd[1]: etcdmirror-conftool-eqiad-wmnet.service crashes
*15:17: scap@deploy1002: Start sync-common
**scap.poolcounter.client: ""[WARNING] lvs2009:9090 reports pool api-https_443/mw2306.codfw.wmnet as <code>enabled/up/pooled</code>, should be <code>disabled/*/not pooled</code>.  [ERROR] Error depooling the servers: enabled/up/pooled. [ERROR] Error running command with poolcounter: Failed executing ServiceRunner.run, return code 127""
*15:18:18: claime launches scap sync-file and notices errors
*15:22:02: <icinga-wm> PROBLEM - etcdmirror-conftool-eqiad-wmnet service on conf2005 is CRITICAL: CRITICAL - Expecting active but unit etcdmirror-conftool-eqiad-wmnet is failed 
*15:23:45: <jinxer-wm> (JobUnavailable) firing: Reduced availability for job etcdmirror in ops@codfw.
*15:28: scap@deploy1002: End of sync-common (duration: 12m 48s).
**WARNING: 58 hosts had failures restarting php-fpm, 58 hosts had failures restarting php-fpm, 18 hosts had failures restarting php-fpm).
*15:28:36: jayme notices issues with conf2005/etcdmirror
*15:33:29: akosiaris restarts etcdmirror
*15:34:00~: '''Start of degradation for clients'''
*15:38:16: <icinga-wm> PROBLEM - High average GET latency for mw requests on appserver in codfw: <code>cluster=appserver code=200 handler=proxy:unix:/run/php/fpm-www-7.2.sock</code> https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=9&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=codfw+prometheus/ops&var-cluster=appserver&var-method=GET
*15:34:42: _joe_ notices https://config-master.wikimedia.org/pybal/codfw/api-https
*15:35:36: _joe_ repools api-https
*15:36:32: _joe_ repools api_appserver
*15:36:42: _joe_ repools appserver
*15:36:42~: '''End of degradation for clients'''
*15:50:32: claime repools parsoid",<mark>OPTIONAL: General conclusions (bullet points or narrative)</mark>,"*[[phab:T317402|Page on etcdmirror alert]]
*[[phab:T317535|Add etcdmirror connection retry on etcd-tls-proxy unavailability]]
*[[phab:T317537|Update Etcd/Main cluster#Replication with safe restart conditions and information]]
*[[phab:T317403|Add etcdmirror status check to scap]]
*[[phab:T317405|Add failure rate triggered rollback to scap]]

<mark>Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.</mark>

<mark>Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFIRE (Pending Review & Scorecard)]]  Phabricator tag to these tasks.</mark>",2022-09-08,Unknown,Monitoring,Unknown,Unknown,Unknown,2022-09-08_codfw_appservers_degradation.wikitext
"{{Incident scorecard
| task = T317381
| paged-num = 0
| responders-num = 3
| coordinators = 
| start = 2022-09-09 00:18
| end = 2022-09-09 08:10
| impact = For 4-8 hours, Wikipedia autocomplete search results were often weird and unhelpful.
}}

Wikimedia users noticed ""weird and unhelpful search results,"" specifically with regards to autocomplete, as documented [[:en:Wikipedia:Village_pump_(technical)#Weird_and_unhelpful_autocompleted_search_results|here]]. This was an unexpected result of changes related to [[phab:T308676|Search Platform's Elasticsearch 7 upgrade]].

{{TOC|align=right}}",,"Detection: [[:en:Wikipedia:Village_pump_(technical)/Archive_200#Weird_and_unhelpful_autocompleted_search_results|users reported the error]]

Alerts: None","''All times in UTC.''

*1 September 2022: The Elasticsearch 7 upgrade plan includes a planned switch from Eqiad to Codfw, where Codfw is upgraded first, then we switch traffic, and once Eqiad is upgraded we switch back. This was [[gerrit:c/operations/mediawiki-config/+/824787|set in wmf-config]] by automatically switching based on the next MW branch.
*7 September 2022: ElasticSearch maintenance script runs from MediaWiki 1.39-wmf.27 branch (old branch, only compatible with ES6) and tries to build indices on Codfw, which run ES7 already. Unbeknownst to us, this rebuild silently failed in Codfw.
*8 September 2022, 21:18:44: The train rolls out 1.39-wmf.28 to all wikis ([https://wikitech.wikimedia.org/w/index.php?title=Deployments&oldid=2012315#deploycal-item-20220908T1800 schedule], [https://sal.toolforge.org/production?p=0&q=wikiversions&d=2022-09-08 SAL]) and traffic switches to Codfw as planned, effectively exposing the failed indices to live traffic.
9 September 2022:
*00:18: Autocomplete caches expire.  '''START OF ISSUE.''' Matching time Wikipedia editors report first observing it (""as of around an hour"" at 1AM).
*01:00: [[:en:Wikipedia:Village_pump_(technical)/Archive_200#Weird_and_unhelpful_autocompleted_search_results|Wikipedia editors report ""weird and unhelpful search results""]], specifically with regards to autocomplete.
*02:13:24: Legoktm reports the issue to the team in IRC #wikimedia-search.
*05:10:37: ebernhardson switches $wgCirrusSearchUseCompletionSuggester to use <code>build</code> (disabling the completion suggester, falling back to prefix search) to work around the issue. Autocomplete caches for 3 hours, so in worst-case scenario, users impacted until 08:10:37.
*08:10: '''END OF ISSUE.'''
*14:00: PoolCounter rejections increase to around 5% due to use of prefix search as opposed to CirrusSearch-Completion (this is a QoS limitation as opposed to a resource limitation).
*14:30: PoolCounter rejections drop as CompletionSuggestor is re-activated via [[gerrit:c/operations/mediawiki-config/+/830978|this patch]].
<!-- Reminder: No private information on this page! -->==Detection==
Detection: [[:en:Wikipedia:Village_pump_(technical)/Archive_200#Weird_and_unhelpful_autocompleted_search_results|users reported the error]]

Alerts: None","===What went well?===

* ebernhardson quickly realized the root cause and worked around it.","*Create documentation on UpdateSuggesterIndex.php, probably should go on [[labsconsole:Search|the Search page]]
*[[phab:T313095|Better monitoring]], specifics to be added later
*[[phab:T317442|Sanity-checking for index size/age]] 
*Pool counter limits should be verified against what's running in production (CompletionSuggest limits are much higher than PrefixSearch, and when we gracefully degrade to PrefixSearch, we need more slots for PrefixSearch). Probably add this as a test this in mediawiki-config.
*Better communication, so others are aware when we roll out a major version change.",2022-09-09,Unknown,Unknown,Unknown,Unknown,Unknown,2022-09-09_Elastic_Autocomplete_Missing.wikitext
"{{Incident scorecard
| task = 
| paged-num =0 
| responders-num =2 
| coordinators =hnowlan 
| start =2022-09-15 12:25 
| end =2022-09-15 12:38 
|metrics=No SLO exists. centrallogin and session loss metrics spiked as a result|impact=Roughly 125 login errors per minute, 215 session loss errors.}}

During routine maintenance on a [[SessionStorage|sessionstore]] Cassandra node, the [[Kask]] service developed a split brain issue where it did not recognise the newly re-added Cassandra node in the cluster despite it being healthy from a Cassandra perspective. This lead to Kask falsely failing to get read and write quorum on the cluster, leading to a disruption of session-related functions on the wikis.

This issue was ultimately thought to be a result of [[phab:T253244|an outstanding bug in gocql]] that leads to an inconsistent view of cluster health. 

The initial mitigation of this issue consisted of removing the newly imaged host from the cluster. The medium-term fix to regain stability consisted of depooling eqiad (where the initial host was situated), restarting the sessionstore1001 Cassandra instance, ensuing that the cluster was healthy, restarting Kask and then re-pooling eqiad.

{{TOC|align=right}}",,"The issue was first human-detected and announced by Tamzin in #wikimedia-operations. However, some alerts also announced the degradation:<syntaxhighlight lang=""irc"">
12:31 <+icinga-wm> PROBLEM - Sessionstore eqiad on sessionstore.svc.eqiad.wmnet is CRITICAL: /sessions/v1/{key} (Store value for key) is CRITICAL: Test Store value for key returned the unexpected status 500 (expecting: 201) https://www.mediawiki.org/wiki/
Kask
12:35 <+icinga-wm> PROBLEM - MediaWiki centralauth errors on graphite1004 is CRITICAL: CRITICAL: 60.00% of data above the critical threshold [1.0] https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000438/mediawiki-
alerts?panelId=3&fullscreen&orgId=1
12:40 <+icinga-wm> PROBLEM - MediaWiki edit session loss on graphite1004 is CRITICAL: CRITICAL: 100.00% of data above the critical threshold [50.0] https://wikitech.wikimedia.org/wiki/Application_servers https://grafana.wikimedia.org/d/000000208/edit-count?orgId=1&viewPanel=13
</syntaxhighlight>None of these alerts paged. 

The errors seen in Cassandra logs were along the lines of the following, and were generated by Kask rather than any Cassandra-level cluster inconsistencies: <syntaxhighlight lang=""text"">
DEBUG [prometheus-http-1-1] 2022-09-15 12:26:35,180 StorageProxy.java:2537 - Hosts not in agreement. Didn't get a response from everybody: 10.64.48.178,10.64.32.85,10.192.16.95
</syntaxhighlight>","[https://sal.toolforge.org/production?d=2022-09-15&p=1 SAL context]
[[File:Screenshot 2022-10-10 at 11.45.58.png|thumb|centrallogin errors]]
[[File:Screenshot 2022-10-10 at 11.46.09.png|thumb|Session loss errors]]


''All times in UTC.''

*11:27 hnowlan completes reimage of sessionstore1001 to buster.
*12:25 sessionstore1001 reimage to buster completes, cassandra started
*12:25 '''OUTAGE BEGINS'''
*12:25 sessionstore begins to fail to achieve write quorum
*12:37 Reports of ongoing issues with sessions and logins in #wikimedia-operations
*12:38 hnowlan stops cassandra-a on sessionstore1001, errors subside
*12:38 '''OUTAGE ENDS'''
*13:00 Incident opened. Hnowlan becomes IC.
*15:17 hnowlan depools sessionstore in eqiad
*15:22 hnowlan reenables and restarts cassandra on sessionstore1001
*15:27 hnowlan runs a roll-restart of kask/sessionstore in eqiad
*15:28 hnowlan repools eqiad - no errors in mediawiki or kask service logs","===What went well?===

*Once the issue was identified, resolution was relatively quick (disabling of the restored cassandra instance)
*Being able to depool eqiad easily and without concerns as regards capacity in codfw for testing and switchover.","*[[phab:T320398|Expand documentation for Kask debugging]]
*[[phab:T253244#8258533|Incorporate the upstream fix for this issue into Kask]]
*[[phab:T320401|Alert on Kask error rate]]",2022-09-15,Unknown,Unknown,Mediawiki,Unknown,Unknown,2022-09-15_sessionstore_quorum_issues.wikitext
"{{Incident scorecard
| task = 
| paged-num = 
| responders-num = 
| coordinators = 
| start = 
| end = 
}}

…

<!-- Reminder: No private information on this page! --><mark>Summary of what happened, in one or two paragraphs. Avoid assuming deep knowledge of the systems here, and try to differentiate between proximate causes and root causes.</mark>

{{TOC|align=right}}",,"<mark>Write how the issue was first detected.  Was automated monitoring first to detect it? Or a human reporting an error?</mark>

<mark>Copy the relevant alerts that fired in this section.</mark>

<mark>Did the appropriate alert(s) fire? Was the alert volume manageable? Did they point to the problem with as much accuracy as possible?</mark>

<mark>TODO: If human only, an actionable should probably be to ""add alerting"".</mark>","<mark>Write a step by step outline of what happened to cause the incident, and how it was remedied.  Include the lead-up to the incident, and any epilogue.</mark>

<mark>Consider including a graphs of the error rate or other surrogate.</mark>

<mark>Link to a specific offset in SAL using the SAL tool at https://sal.toolforge.org/ ([https://sal.toolforge.org/production?q=synchronized&d=2012-01-01 example])</mark>

''All times in UTC.''

*00:00 (TODO) '''OUTAGE BEGINS'''
*00:04 (Something something)
*00:06 (Voila) '''OUTAGE ENDS'''
*00:15 (post-outage cleanup finished)

<!-- Reminder: No private information on this page! -->
<mark>TODO: Clearly indicate when the user-visible outage began and ended.</mark>",<mark>OPTIONAL: General conclusions (bullet points or narrative)</mark>,"* …

<mark>Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.</mark>

<mark>Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFIRE (Pending Review & Scorecard)]]  Phabricator tag to these tasks.</mark>",2022-10-04,Unknown,Monitoring,Unknown,Unknown,Unknown,2022-10-04_jupyterhub-conda_outage.wikitext
"{{Incident scorecard
| task =T313463 
| paged-num =4 
| responders-num =5 
| coordinators =N/A 
| start =2022-10-07 14:50:00 
| end =2022-10-07 14:52:00 
|impact=For 2 minutes eqiad row D suffered a partial connectivity outage (traffic coming through cr1-eqiad was blackholed).
This had an impact on all types of clients. See for example https://grafana.wikimedia.org/d/-K8NgsUnz/home?orgId=1&from=1665067500000&to=1665069300000}}

After the row C uplinks change (part of [[phab:T313463|T313463]]) was completed successfully, the same procedure got applied to row D's link to cr1-eqiad. While the asw side went fine (and took down the link as planned, waiting for the cr side to be reconfigured), the configuration change on the cr1 discarded traffic toward that switch. Traffic flowing from cr2 to row D was not impacted. Additionally the VRRP gateway was set to cr2, so outbound traffic from row D was not impacted as well.

Troubleshooting was made more difficult as bast1003 is in row D causing management access to be lost. The change was done with an automatic rollback timeout of 2min. At that 2 min mark, the change got automatically reverted, restoring full connectivity before I was able to connect through a different bast host.

The exact root cause of why the traffic was discarded is so far still unknown. Safe troubleshooting (eg. remove ae4 IP config, to test lower layer connectivity) will be done at a later date.

The 2 dbproxies affected (for m3, m5) were passive, they were reloaded manually afterwards to point back into the usual primary hosts.

{{TOC|align=right}}",,"Ayounsi figured something was wrong when he lost connectivity to cr1-eqiad and bast1003.

Multiple alerts triggered, some of the relevant ones:

* 14:53 <jinxer-wm> (ProbeDown) firing: Service api-https:443 has failed probes (http_api-https_ip4) #page - [[Runbook#api-https:443|https://wikitech.wikimedia.org/wiki/Runbook#api-https:443]] - <nowiki>https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=All</nowiki> - <nowiki>https://alerts.wikimedia.org/?q=alertname%3DProbeDown</nowiki>
* 14:53 <icinga-wm> PROBLEM - High average GET latency for mw requests on appserver in eqiad on alert1001 is CRITICAL: cluster=appserver code=200 handler=proxy:unix:/run/php/fpm-www-7.4.sock [[Monitoring/Missing notes link|https://wikitech.wikimedia.org/wiki/Monitoring/Missing_notes_link]] <nowiki>https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=9&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=eqiad+prometheus/ops&var-cluster=appserver&var-method=GET</nowiki>
* 14:54 <jinxer-wm> (PHPFPMTooBusy) firing: Not enough idle php7.4-fpm.service workers for Mediawiki api_appserver at eqiad #page - <nowiki>https://bit.ly/wmf-fpmsat</nowiki> - <nowiki>https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=54&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=eqiad%20prometheus/ops&var-cluster=api_appserver</nowiki> - <nowiki>https://alerts.wikimedia.org/?q=alertname%3DPHPFPMTooBusy</nowiki>
* 14:55 <icinga-wm> PROBLEM - haproxy failover on dbproxy1016 is CRITICAL: CRITICAL check_failover servers up 2 down 1: [[HAProxy|https://wikitech.wikimedia.org/wiki/HAProxy]]
* 14:55 <icinga-wm> PROBLEM - haproxy failover on dbproxy1017 is CRITICAL: CRITICAL check_failover servers up 2 down 1: https://wikitech.wikimedia.org/wiki/HAProxy
As it was during a maintenance the root cause was easy to identify.

However, if this had happened on its own (even though unlikely), the root cause would have taken more time to identify. Especially as Icinga is running from row C, and thus not seeing the failure.","''All times in UTC.''

*14:50 configuration change pushed '''OUTAGE BEGINS'''
*https://grafana.wikimedia.org/d/-K8NgsUnz/home?orgId=1&from=1665067500000&to=1665069300000&viewPanel=8
*https://grafana.wikimedia.org/d/m1LYjVjnz/network-icmp-probes?from=1665067500000&to=1665069300000&viewPanel=2&orgId=1&var-site=All&var-target_site=eqiad&var-role=host&var-family=All
*14:52 configuration change automatically rolled back '''OUTAGE ENDS'''
[[File:2022-10-06-cdn-errors.png|thumb]]
[[File:2022-10-06-icmp-latency-to-eqiad.png|thumb]]


[[File:2022-10-06-appserver errors.png|thumb]]
Most of the alerts triggered after the network stabilized, and the graphs show an impact multiple minutes after it as well. My guess is that workers queued up on the row D servers waiting on row A/B (and potentially E/F) servers (as their default gateway is on cr1) and took some time to catch up once connectivity was restored.","===What went well?===

* Issue happened during a maintenance window
* The automatic rollback Juniper feature did its job
* Everything recovered on its own","*Root cause analysis: Cr1-eqiad comms problem when moving to 40G row D handoff - [[phab:T320566|T320566]]
*To be discussed: how can we make the servers more resilient in face of such event?",2022-10-06,Unknown,Monitoring,Mediawiki,Unknown,Unknown,2022-10-06_eqiad_row_D_networking.wikitext
"{{Incident scorecard
| task = T320990
| paged-num = 17
| responders-num = 3
| coordinators = rzl
| start = 2022-10-15 22:37
| end = 2022-10-15 23:01
| metrics = No relevant SLOs exist; frwiki, jawiki, ruwiki, and wikitech were read-only for 24 minutes
| impact = frwiki, jawiki, ruwiki, and wikitech were read-only for 24 minutes (starting Sunday 12:37 AM in France, 7:37 AM in Japan, and very early- to mid-morning in Russia).
}}The s6 master, db1131, went offline due to a bad DIMM. We rebooted it via ipmitool and restarted mariadb, then failed over to db1173 and depooled db1131.{{TOC|align=right}}",,"We were paged once for <code>Host db1131 #page is DOWN: PING CRITICAL - Packet loss = 100%</code> about three minutes after user impact began.

We also got paged for <code>MariaDB Replica IO: s6 #page on db1173 is CRITICAL: CRITICAL slave_io_state Slave_IO_Running: No, Errno: 2003, Errmsg: error reconnecting to master repl@db1131.eqiad.wmnet:3306 - retry-time: 60 maximum-retries: 86400 message: Cant connect to MySQL server on db1131.eqiad.wmnet (110 Connection timed out)</code> and identical errors for db1098, db1113, and db1168. Those alerts were redundant but the volume wasn't unmanageable.","''All times in UTC, 2022-10-15.''

*17:19 db1131 records a SEL entry: <code>Correctable memory error logging disabled for a memory device at location DIMM_A6.</code> At this point the DIMM has started to fail but the errors are still correctable.
*22:37 Last pre-outage edits on [https://fr.wikipedia.org/w/index.php?title=Ivan_Goloubev-Monatkine&curid=15236581&diff=197826549&oldid=197826342 frwiki], [https://ja.wikipedia.org/w/index.php?title=%E6%B8%85%E5%8E%9F%E3%82%B5%E3%83%BC%E3%83%93%E3%82%B9%E3%82%A8%E3%83%AA%E3%82%A2&curid=978988&diff=91976699&oldid=89087606 jawiki], [https://ru.wikipedia.org/w/index.php?title=%D0%9A%D0%B0%D1%82%D0%B5%D0%B3%D0%BE%D1%80%D0%B8%D1%8F:%D0%94%D0%B5%D0%BF%D1%83%D1%82%D0%B0%D1%82%D1%8B_%D0%9B%D1%83%D0%B3%D0%B0%D0%BD%D1%81%D0%BA%D0%BE%D0%B3%D0%BE_%D0%B3%D0%BE%D1%80%D0%BE%D0%B4%D1%81%D0%BA%D0%BE%D0%B3%D0%BE_%D1%81%D0%BE%D0%B2%D0%B5%D1%82%D0%B0&curid=7215375&diff=126082951&oldid=89142386 ruwiki].
*22:37 db1131, s6 master, experiences multi-bit memory errors in DIMM A6. The machine reboots and then locks up. '''[OUTAGE BEGINS]'''
*22:40 First paging alert: <code><icinga-wm> PROBLEM - Host db1131 #page is DOWN: PING CRITICAL - Packet loss = 100%</code>
*22:43 Amir1 begins working on an emergency switchover. ({{phab|T320879}})
*22:43 Pages for “error connecting to master” from replicas db1173, db1098, db1113, db1168.
*22:44 Amir1 downtimes A:db-section-s6 for the switchover, suppressing any further replica pages.
*22:48 Incident opened; rzl becomes IC.
*22:48 Amir1 asks for db1131 to be power-cycled (which is much faster than switching over with the master offline).
*22:50 [https://wikimedia.statuspage.io/incidents/hnm5c223c26v rzl posts a new incident as “Investigating” to statuspage.]
*22:54 Amir1 sets s6 to read-only
*22:54 rzl powercycles db1131 via ipmitool
*22:57 db1131 begins answering ping; Amir1 starts mariadb
*22:59 <Amir1> it should be rw now
*23:01 rzl notes no activity on frwiki's RecentChanges, tries a test edit, and gets ""The primary database server is running in read-only mode."" as MediaWiki is still set to read-only.
*23:01 Amir1 turns off read-only mode. '''[OUTAGE ENDS]'''
*23:01 First post-outage edits on affected wikis.
*23:02 Amir1 resumes the originally-planned master switchover from db1131 to db1173, now to prevent recurrence.
*23:22 Replicas are finished moving to db1173. Amir1 begins the critical phase of the switchover, which requires another minute of read-only time.
*23:24 s6 switchover complete.
*23:27 Amir1 depools db1131.
*23:47 Incident closed. [https://wikimedia.statuspage.io/incidents/hnm5c223c26v rzl resolves the statuspage incident.]","===What went well?===

*The paging alerts for ping lossage on DB masters meant we got paged within a few minutes, with an immediately clear and actionable signal about the problem and severity.","*{{Phabricator|T320994}}: Check and replace the suspected-failed DIMM.
*{{Phabricator|T196366}}: Implement (or refactor) a script to move replicas when the master is not available",2022-10-15,Unknown,Unknown,Unknown,Unknown,Unknown,2022-10-15_s6_master_failure.wikitext
"{{Incident scorecard
| task =T321135 
| paged-num =6 (2 of them on-call, 4 opt-in for 24/7 pages)
| responders-num =5 (2 on-call, 3 responded without having been paged)
| coordinators =dzahn 
| start =2022-10-17T20:32:09Z 
| end =2022-10-17T23:07:09Z 
|metrics=No relevant SLOs exist|impact=delayed mail delivery, users of VRT and general email recipients received mail delayed and received spam email}}

…

<!-- Reminder: No private information on this page! --><mark>A wave of spam email to an info@ address was routed from mail servers to the VRT machine (otrs1001).</mark>

Many Perl processes were spawned which used up all the RAM of the virtual machine. oom-killer killed clamav-daemon.

Without clamav mail delivery stopped.

More mails started queing up on both otrs1001 and then the mail server mx1001.

When the mail queue reached a critical threshold on mx1001, SRE got paged.

Measures taken included increasing RAM available on the otrs1001 VM and deleting spam email.

Eventually all mail was delivered, just with a delay.

{{TOC|align=right}}",,"On-call SRE got paged by Splunk-On-Call (VictorOps)

incident name was: '''Critical: [FIRING:1] MXQueueHigh misc (node ops page prometheus sre)''', incident ID: [https://portal.victorops.com/ui/wikimedia/incident/3094/details VictorOps 3094]","<mark>Consider including a graphs of the error rate or other surrogate.</mark>

''All times in UTC.''

*2022-10-17T20:32:09Z '''OUTAGE BEGINS''' with ""20:32 <+jinxer-wm> (MXQueueHigh) firing: MX host mx1001:9100 has many queued messages: 7353 #page..""
*2022-10-17:20:53:00Z It's identified that mail delivery fails because clamav-daemon gets killed by OOM-killer
*2022-10-17:21:02:00Z The ""max_threads"" setting in the clamav-daemon config is changed from 12 to 2 and subsequently 1, in an attempt to keep it from being killed.
*2022-10-17:21:46:00Z A bash script is executed that removes spam mail matching certain patterns (mini cooper) on mx1001.
*2022-10-17:22:02:00Z The same script is executed on otrs1001 and the mail queue has been reduced by a lot
*2022-10-17:22:34:00Z 'gnt-instance command is executed to increase RAM of the VM from 4GGB to 8GB
*2022-10-17:22:39:00Z Number of mails in the queue (exiqgrep -c) starts to go down.
*2022-10-17:22:45:00Z Puppet is renabled and run which reverts the previous changes to max_threads of clamav-daemon. It is using 12 threads again.
*2022-10-17T22:51:00Z 'exim4 -qf' is executed on mx1001 to re-deliver queued mails, swapping continues but no more OOMs
*2022-10-17T22:54:00Z memory freed up after an initial burst of activity
*2022-10-17T23:07:09Z '''OUTAGE ENDS''' with ""<+jinxer-wm> (MXQueueHigh) resolved: MX host mx2001:9100 has many queued messages: 4623.."" when mail qeue is under threshold again.

* https://wikitech.wikimedia.org/wiki/Server_Admin_Log#2022-10-17","<mark>We should have more than a single VRTS server.</mark>

<mark>Spam should not take down the VRTS machine.</mark>","* [https://phabricator.wikimedia.org/T321418 Increase RAM assigned to otrs1001 VM (done, increased from 4 to 8GB RAM)]",2022-10-17,Unknown,Unknown,Unknown,Unknown,Unknown,2022-10-17_mx_and_vrts.wikitext
"{{Incident scorecard
| task = T322360
| paged-num = 0
| responders-num = 10
| coordinators = denisse
| start = 2022-11-03 17:06:00
| end = 2022-11-03 18:09:00
| metrics = No impact to the etcd SLO. Metrics: conf* filesystem usage  and etcd req/s
| impact = No user impact. confd service failed for ~33 minutes
}}

A bug introduced to the MediaWiki codebase caused an increase in connections to [[Confd]] hosts from systems responsible for [[Dumps]] which in turn lead to a high volume of log events and ultimately a filled up filesystem.

{{TOC|align=right}}",,The last symptom of his issue was detected by an Icinga alert: <code>conf1008 icinga alert: <icinga-wm> PROBLEM - Disk space on conf1008 is CRITICAL: DISK CRITICAL - free space: / 2744 MB (3% inode=98%): /tmp 2744 MB (3% inode=98%): /var/tmp 2744 MB (3% inode=98%): <nowiki>https://wikitech.wikimedia.org/wiki/Monitoring/Disk_space</nowiki></code>,"''All times in UTC.''

*2022-09-06: A bug is introduced on MediaWiki core codebase on [[gerrit:c/mediawiki/core/+/798678|5b0b54599bfd]], causing configuration to be checked for every row of a database query on WikiExport.php, but the feature is not yet enabled.
*2022-10-24: The feature is enabled: https://gerrit.wikimedia.org/r/c/operations/mediawiki-config/+/848201
*2022-11-03 08:09 Systemd timer starts dump process on snapshot10[10,13,12,11] that starts accessing dbctl/etcd (on conf1* hosts) once per row from a database query result.
*17:06 '''OUTAGE BEGINS''' <code>conf1008 icinga alert: <icinga-wm> PROBLEM - Disk space on conf1008 is CRITICAL: DISK CRITICAL - free space: / 2744 MB (3% inode=98%): /tmp 2744 MB (3% inode=98%): /var/tmp 2744 MB (3% inode=98%): <nowiki>https://wikitech.wikimedia.org/wiki/Monitoring/Disk_space</nowiki></code>
*17:10 Incident opened, elukey notifies of conf1008 root partition almost full
*17:13 Disk space is freed with <code>apt-get clean</code>
*17:37 Some nodes reach 100% disk usage
*17:37 nginx logs are truncated
*17:39 etcd_access.log.1 are truncated in the 3 conf100* nodes
*17:39 '''OUTAGE ENDS''': Disk space is under control
*17:46 DB maintenance is stopped
*17:48 denisse becomes IC
*17:50 All pooling/depooling of databases is stopped
*17:52 The origin of the issue is identified as excessive connections from <code>snapshot[10,13,12,11]</code>
*17:58 snapshot hosts stopped hammering etcd after pausing dumps
*18:15 Code change of fix merged https://sal.toolforge.org/log/4iLgPoQBa_6PSCT93YhE

[[File:Conf1008 utilization.png|thumb|conf1008 utilization]]
[[File:Etcd req-s.png|thumb|etcd req/s]]","===What went well?===

* confd/etcd designed to not be a SPOF prevented further bad things from happening","* conf* hosts ran out of disk space due to log spam; {{PhabT|T322360}}
* Monitor high load on etcd/conf* hosts to prevent incidents of software requiring config reload too often; {{PhabT|T322400}}",2022-11-03,Unknown,Monitoring,Unknown,Unknown,Unknown,2022-11-03_conf_disk_space.wikitext
"{{Incident scorecard
| task = https://phabricator.wikimedia.org/T322424
| paged-num = 2
| responders-num = 10
| coordinators = denisse
| start = 2022-11-04 14:32
| end = 2022-11-04 15:21
| metrics = No relevant SLOs exist
| impact = Swift has reduced service availability affecting Commons/multimedia
}}…<!--Reminder: No private information on this page!-->

For 49 minutes the Swift/mediawiki file backend returned errors (how many? Which percentage?) for both reads and new uploads. {{TOC|align=right}}",,"The issue was detected automatically and the engineers On Call received a page from Splunk on Call

Alerts that fired during the incident:

* [https://portal.victorops.com/ui/wikimedia/incident/3133 Incident #3133]
* [https://portal.victorops.com/ui/wikimedia/incident/3134 Incident #3134]
* [https://portal.victorops.com/ui/wikimedia/incident/3135 Incident #3135]
* [https://portal.victorops.com/ui/wikimedia/incident/3136 Incident #3136]
* [https://portal.victorops.com/ui/wikimedia/incident/3137 Incident #3137]

The alerts that fired were useful for the engineers to solve the incident.","[[File:Screenshot 2022-11-06 at 22-39-13 2022-11-01 Swift issues.png|thumb|A Grafana dashboard showing a ""client errors"" and a ""server errors"" graph for the Swift service at the time of the incident]]
''All times are in UTC.''
[[File:Swift service errors.png|thumb|A Grafana dashboard showing the memory usage of the Swift instances at the time of the incident]]

* 14:32 ('''INCIDENT BEGINS)'''
* 14:32 jinxer-wm: (ProbeDown) firing: Service thumbor:8800 has failed probes (http_thumbor_ip4) #page - <nowiki>https://wikitech.wikimedia.org/wiki/Runbook#thumbor:8800</nowiki> - <nowiki>https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=All</nowiki> - <nowiki>https://alerts.wikimedia.org/?q=alertname%3DProbeDown</nowiki>
* 14:32 icinga-wm: PROBLEM - Swift https backend on ms-fe1010 is CRITICAL: CRITICAL - Socket timeout after 10 seconds <nowiki>https://wikitech.wikimedia.org/wiki/Swift</nowiki>
* 14:38 Incident opened.  denisse becomes IC.
* 14:41 [https://sal.toolforge.org/log/9yYTQ4QBa_6PSCT9dW78 depool ms-fe2009 on eqiad]
* 14:44 [https://sal.toolforge.org/log/ELAZQ4QB8Fs0LHO5gexm restart swift-proxy on ms-fe1010]
* 14:48 [https://sal.toolforge.org/log/oCYdQ4QBa_6PSCT9hHbo restart swift-proxy on ms-fe1011]
* 14:51 [https://sal.toolforge.org/log/k7AgQ4QB8Fs0LHO5UvHS restart swift-proxy on ms-fe1012]
* 14:52 r[https://sal.toolforge.org/log/bCYgQ4QBa_6PSCT9n3k7 epool ms-fe2009 on eqiad]
* 14:54 [https://www.wikimediastatus.net/incidents/lb4gcj9w5wl5 Statuspage incident] posted “Errors displaying or uploading media files.”
* 15:00 [https://sal.toolforge.org/log/yuAoQ4QB6FQ6iqKixKNd restart swift-proxy on codfw]
* 15:01 recovery of the ms-fe2* instance
* 15:21 ('''INCIDENT RESOLVED)''' (Statuspage updated) 
<!--Reminder: No private information on this page!-->","=== What went well? ===

* Automated monitoring detected the incident
* Several engineers helped debug the issue","* Investigate why the alerts scalated to batphone even when the engineers on call have already ACK'd the initial alert.

* Add runbooks, documentation on how to troubleshoot this issues.",2022-11-04,Unknown,Unknown,Unknown,Unknown,Unknown,2022-11-04_Swift_issues.wikitext
"{{Incident scorecard
| task = T323094
| paged-num = 0
| responders-num = 
| coordinators = 0
| start = 2022-11-15 04:51:00
| end = 2022-11-15 04:56:00
| metrics = No relevant SLOs exist - 5xx errors graph shows the impact https://grafana.wikimedia.org/d/-K8NgsUnz/home?orgId=1&from=1668487382007&to=1668488908178&viewPanel=8
| impact = For about 5min, users in the APAC region (using eqsin) could have been served 5xx errors instead of the requested page.
}}


<!-- Reminder: No private information on this page! -->Connecting a new server to our eqsin top of rack switches triggered a Juniper [https://prsearch.juniper.net/problemreport/PR1080132 bug]  which caused one of its processes to be killed and interrupting traffic transiting through the switch. This event caused also a Virtual-Chassis master switchover extending the outage. The process got automatically re-started and the situation stabilized by itself in about 5min.

{{TOC|align=right}}",,"<mark>Write how the issue was first detected.  Was automated monitoring first to detect it? Or a human reporting an error?</mark>

<mark>Copy the relevant alerts that fired in this section.</mark>

<mark>Did the appropriate alert(s) fire? Was the alert volume manageable? Did they point to the problem with as much accuracy as possible?</mark>

<mark>TODO: If human only, an actionable should probably be to ""add alerting"".</mark>","<mark>Write a step by step outline of what happened to cause the incident, and how it was remedied.  Include the lead-up to the incident, and any epilogue.</mark>

<mark>Consider including a graphs of the error rate or other surrogate.</mark>

<mark>Link to a specific offset in SAL using the SAL tool at https://sal.toolforge.org/ ([https://sal.toolforge.org/production?q=synchronized&d=2012-01-01 example])</mark>

''All times in UTC.''

*04:51 (TODO) '''OUTAGE BEGINS'''
*00:04 (Something something)
*00:06 (Voila) '''OUTAGE ENDS'''
*00:15 (post-outage cleanup finished)

<!-- Reminder: No private information on this page! -->
<mark>TODO: Clearly indicate when the user-visible outage began and ended.</mark>",<mark>OPTIONAL: General conclusions (bullet points or narrative)</mark>,"* Upgrade POPs asw to Junos 21 - https://phabricator.wikimedia.org/T316532
* We're phasing out virtual chassis in the new POP network designs (cf. drmrs). Even though such bugs might always be a possibility, the new design is more resilient (each switch is independent)",2022-11-15,Unknown,Monitoring,Unknown,Unknown,Unknown,2022-11-15_asw1-eqsin.wikitext
"{{Incident scorecard
| task = T323262
| paged-num = -
| responders-num = 4ish
| coordinators = -
| start = 2022-11-17 09:00:00
| end = 2022-11-17 12:00:00
| metrics = No relevant SLOs exist. [https://grafana.wikimedia.org/d/MeOVgCPWz/process-upstream?orgId=1&from=1668664635159&to=1668693908620 Uptime interruptions for 2+ hours]
| impact = Gerrit was unavailable for almost three hours
}}

An upgrade of Gerrit from 3.4.8 to 3.5.4 was scheduled on November 17th at 9:00 UTC. After the upgrade, the root partition filled up causing Gerrit to no longer be able to write to its indexes. The first symptoms were inability to write a comment or cast a vote. The service got stopped entirely to relocate Gerrit data to a dedicated partition. The upgraded Gerrit was back at 11:45 UTC.

{{TOC|align=right}}",,"The first report was at 9:16 by Valentin. He was getting no content beside header/footer in https://gerrit.wikimedia.org/r/dashboard/self . Same for https://gerrit.wikimedia.org/r/q/status:open+-is:wip . It was unclear whether it was related and it got dismissed based on browser caches being out of sync.

Timo reports at 10:36 that no write action works. It is when the root partition had filled.

Icinga alerts did not trigger since [[Gerrit/Upgrade#Deploying]] asks to put both hosts in maintenance mode, hence none of the probes (such as the disk space check) emit any alarm.","[https://sal.toolforge.org/production?p=0&q=gerrit&d=2022-11-17 Sal entries mentioning Gerrit on November 17th 2022].

''All times in UTC.''

The synchronization occurred in <code>#wikimedia-operations</code>. Some specific events related to the service have their timestamps highlighted in bold ('''##:##''').

*'''09:00''' Maintenance window starts
*09:00 Icinga monitoring is disabled for both gerrit1001 and gerrit2002 hosts
*09:04 Deployment to gerrit2002 (replica)
*09:07 gerrit-replica on gerrit2002 is upgraded to 3.5.4
*09:10 Deployment to gerrit1001 (primary)
*09:12 gerrit on gerrit1001 is upgraded to 3.5.4
*09:14 '''Valentin''' mentions pages only having the header/footer. It is assumed it is a web browser cache issue since it works for others. Antoine witnessed similar issues locally and on WMCS devtools when preparing the upgrade.
*'''09:34''' (found after the facts) '''Outage begins.''' Indexing stops processing. The root partition is full. Gerrit is unable to write updates made to change in the index.
*09:36 After monitoring various metrics for 20 minutes, '''Antoine''' (erroneously) claims the service to be operational. Gerrit is still online, reindexing all changes
*09:36 '''Timo''' reports he is unable to do any write action (submit +2, rebase, remove +2 vote). He get a modal window error stating there is a 500 Internal Server Error. Antoine had not yet checked the logs.
*09:40 '''Antoine''' reports <code>java.io.IOException: No space left on device</code>. The root partition is full.
*09:42 some files are deleted and Antoine suggests moving the <code>cache</code> directory to the larger <code>/srv</code> partition.
*09:44 '''Thiemo Kreuz''' sends an [[https://lists.wikimedia.org/hyperkitty/list/wikitech-l@lists.wikimedia.org/thread/55CC4WNUBPRYPNSJSGSF62WFNL3FFLWW/ | error report on wikitech-l]]
*09:49 A thread holds a lock on the <code>gerrit_file_diff</code> disk cache <code>org.h2.jdbc.JdbcSQLException: Timeout trying to lock table ""DATA""; SQL statement:</code>. On disk there is an extra half written file.
*'''09:52''' '''Antoine stops Gerrit''' to flush the lock and do a full offline reindex of all changes.
*09:56 Various Icinga probes start alerting due to <code>git fetch</code> failing in some units
*09:59 Indexing 33% done
*10:04 Indexing 72% done (a lot of changes already got reindexed when Gerrit 3.5.4 was up (09:12 to 09:52)).
*10:17 Indexing 80% done
*10:19 '''Antoine''' keeps freeing old files from <code>/srv</code> and from the root partition
*10:22 Indexed 684k changes out of 847k
*10:43 Indexing 83% done
*10:58 Indexing 99% done
*10:59 Indexing of <code>changes</code> change has completed with 203 failed task. Disk space errors surfaced, causing the root partition to run out of disk space again.
*'''11:00''' The indexing being interrupted, there is nothing locking the caches and Puppet (or systemd) '''brings back Gerrit'''.  The root partition only has 2GB left.
*11:06 '''Giuseppe''' offers SRE help and they take over from there
*11:10 '''Filippo''' mentions there is 80G free in the volume group.
*11:11 git_file_diff.h2.db is 8G, gerrit_file_diff.h2.db is 12G
*11:17 '''Giuseppe''' asks, ""should we do a failover?"" It is ruled out cause it hasn't been done in a while and seems simpler to give more disk space immediately.
*11:26 We have considered flushing the cache but <code>gerrit flush-cache</code> only flushes the in-memory cache.  <code>gerrit.war init</code> has a parameter to delete all disk caches but we never used it.
*'''Giuseppe''' and '''Clément''' discuss to determine which directory(ies) to relocate and how to do it.
*11:26 '''Clément''' creates and mounts a 50G lv to host Gerrit data (<code>/var/lib/gerrit2</code>) and starts presync
*11:34 Presync is done and '''Antoine''' stops Gerrit for last <code>rsync</code>
*11:36 '''Lukasz''' asks whether a tracking task has been filed. He files {{phabricator|T323262}}
*11:40 '''Clément''' <code>rsync</code> of Gerrit data has completed
*11:44 <code>/var/lib/gerrit2</code> is replaced by the rsynced partition
*'''11:45''' '''Clément''' runs Puppet to bring back the Gerrit service","* When a full reindexing of changes, it would probably be better to conduct the upgrade very early in the UTC morning or over the week-end.
* The upgrade procedure requests to disable the Icinga monitoring at the host level. As a side effect it also disables the disk space checks.
* The H2 databases have probably been carried over since we originally started Gerrit. It is unclear whether they should be so large. Antoine is puzzled by <code>gerrit show-caches</code> output which reports <code>gerrit_file_diff</code> to have <code>Space: 134.20m</code> but the corresponding H2 database file <code>gerrit_file_diff.h2.db</code> file was 12 GBytes.
* Relocating the large files to a dedicated partition was the fix","* {{phabricator|T333143}} Move Gerrit data out of root partition
* {{phabricator|T323754}} Investigate why the H2 database files are so large
* <s>Document the new partition layout?</s> (partition to be removed after data get moved)
* Check with SRE collab about the partitioning scheme on gerrit1001 since the root partition is not LVM managed.
* 2022-12-16 wrote a blog post [https://phabricator.wikimedia.org/J300 Phame > Doing the needful > Shrinking H2 database files]",2022-11-17,Unknown,Unknown,Unknown,Unknown,Unknown,2022-11-17_Gerrit_3.5_upgrade.wikitext
"{{Incident scorecard
| task = T323620
| paged-num = 3
| responders-num = 2
| coordinators = Gehel
| start = 2022-11-22 15:15
| end = 2022-11-22 15:30
| metrics = wdqs
| impact = For at least 15 minutes, users of the wikidata query service experienced a lack of service and/or extremely slow responses
}}

…

<!-- Reminder: No private information on this page! -->For at least 15 minutes, users of the Wikidata Query Service either could not connect or received extremely slow responses.{{TOC|align=right}}",,"The issue was detected by monitoring (pybal alerts)

Example alert verbiage: <code>PROBLEM - PyBal backends health check on lvs1020 is CRITICAL: PYBAL CRITICAL - CRITICAL - wdqs-heavy-queries_8888: Servers wdqs1015.eqiad.wmnet, wdqs1012.eqiad.wmnet, wdqs1004.eqiad.wmnet, wdqs1014.eqiad.wmnet, wdqs1016.eqiad.wmnet, wdqs1007.eqiad.wmnet,</code> 

The appropriate alerts fired, and contained enough actionable information for humans to quickly remediate the problem.","15:15 UTC 

* Pybal alert for all public wdqs hosts in eqiad datacenter. This pages on-duty SREs akosiaris, jelto and herron.
* [https://grafana-rw.wikimedia.org/d/000000522/wikidata-query-service-frontend?orgId=1&from=1669129200000&to=1669132200000 Increased 5xx error rates]
* [https://grafana.wikimedia.org/d/000000489/wikidata-query-service?orgId=1&from=1669129200000&to=1669132200000&viewPanel=12 Increased load avg-15 for all public wdqs hosts] ( > 40 generally indicates user impact)

*15:16 dcausse, gehel and inflatador (Search Platform team and owners of the WDQS service) begin troubleshooting
*15:22 inflatador begins restarting all public wdqs hosts in eqiad datacenter. Alerts begin to clear. At this point, we presume the impact to users is over.
*'''15:34''' inflatador finishes all restarts.
*16:11 (as I write this) load avg-15 remains at reasonable levels and we consider the service to be stabilized.

<!-- Reminder: No private information on this page! -->==Detection==
The issue was detected by monitoring (pybal alerts)

Example alert verbiage: <code>PROBLEM - PyBal backends health check on lvs1020 is CRITICAL: PYBAL CRITICAL - CRITICAL - wdqs-heavy-queries_8888: Servers wdqs1015.eqiad.wmnet, wdqs1012.eqiad.wmnet, wdqs1004.eqiad.wmnet, wdqs1014.eqiad.wmnet, wdqs1016.eqiad.wmnet, wdqs1007.eqiad.wmnet,</code> 

The appropriate alerts fired, and contained enough actionable information for humans to quickly remediate the problem.","Search team has been aware of the brittle nature of WDQS for some time, and there are ongoing efforts to migrate off its current technology stack (specifically Blazegraph). We are also in the process of [[phab:T313751|defining an SLO for WDQS]].",* Email to Wikidata Users list for awareness,2022-11-22,Unknown,Monitoring,Unknown,Unknown,Unknown,2022-11-22_wdqs_outage.wikitext
"{{Incident scorecard
| task = T324994
| paged-num = 2
| responders-num = 5
| coordinators = claime (retroactive)
| start = 2022-12-09 15:04:13
| end = 2022-12-09 16:36:17
| metrics = api_appserver latency was degraded around the starvation point
| impact = No user-facing impact.
}}

…

<!-- Reminder: No private information on this page! -->[[phab:T320518#8455510|Increased mediawiki logging]]  led to eventgate-analytics congestion, starving the api_appservers of idle workers. No user-facing impact.

{{TOC|align=right}}",,"Issue detected through monitoring of api_appserver idle starvation. The alerts were accurate as to the symptom, but not the actual cause (eventgate-external pods getting CPU throttled).","''All times in UTC.''

'''2022-12-09 13:13:00'''     <hashar@deploy1002>     rebuilt and synchronized wikiversions files: all wikis to 1.40.0-wmf.13 refs T320518

'''2022-12-09 15:04:13'''     '''Incident start'''

'''2022-12-09 15:04:13'''     ''[+icinga-wm]    PROBLEM - Some MediaWiki servers are running out of idle PHP-FPM workers in api_appserver at eqiad on alert1001 is CRITICAL: 0.5645 gt 0.3 <nowiki>https://bit.ly/wmf-fpmsat</nowiki> <nowiki>https://grafana.wikimedia.org/d/fRn9VEPMz/application-servers-use-dashboard-wip?orgId=1</nowiki>''

'''2022-12-09 15:09:47'''     ''[+icinga-wm]    RECOVERY - Some MediaWiki servers are running out of idle PHP-FPM workers in api_appserver at eqiad on alert1001 is OK: (C)0.3 gt (W)0.1 gt 0.09677 <nowiki>https://bit.ly/wmf-fpmsat</nowiki> <nowiki>https://grafana.wikimedia.org/d/fRn9VEPMz/application-servers-use-dashboard-wip?orgId=1</nowiki>''
[[File:2022-12-09 api appserver RED saturation.png|alt=2022-12-09 api appserver RED showing idle worker saturation|thumb|2022-12-09 api appserver RED showing idle worker saturation]]
[[File:2022-12-09 api appserver 7 day RED.png|alt=2022-12-09 api appserver 7 day RED showing the increase in requests.|thumb|2022-12-09 api appserver 7 day RED showing the increase in requests.]]
'''2022-12-09 15:16:03'''     claime starts investigating a possible increase in calls [https://grafana.wikimedia.org/goto/BB-0tN5Vz?orgId=1 Idle workers] [https://grafana.wikimedia.org/goto/R50Z0qFVk?orgId=1 7 days RED] Initial working hypothesis is that the increase is caused by [https://sal.toolforge.org/log/CoAE94QB6FQ6iqKiyeoN train deployment of 1.40.0-wmf.13 to all wikis]  

'''2022-12-09 15:50:31'''     ''[+icinga-wm]    PROBLEM - Some MediaWiki servers are running out of idle PHP-FPM workers in api_appserver at eqiad on alert1001 is CRITICAL: 0.4194 gt 0.3 <nowiki>https://bit.ly/wmf-fpmsat</nowiki> <nowiki>https://grafana.wikimedia.org/d/fRn9VEPMz/application-servers-use-dashboard-wip?orgId=1</nowiki>''

'''2022-12-09 15:56:18'''     jayme remarks that the situation is getting worse, pings Amir

'''2022-12-09 15:59:47'''     ''[+icinga-wm]    RECOVERY - Some MediaWiki servers are running out of idle PHP-FPM workers in api_appserver at eqiad on alert1001 is OK: (C)0.3 gt (W)0.1 gt 0.03226 <nowiki>https://bit.ly/wmf-fpmsat</nowiki> <nowiki>https://grafana.wikimedia.org/d/fRn9VEPMz/application-servers-use-dashboard-wip?orgId=1</nowiki>''
[[File:2022-12-09 14.excimer-wall.api.png|alt=2022-12-09_14 Flamegraph showing MediaWiki\Extension\EventBus\EventBus::send taking a disproportionate amount of time|thumb|2022-12-09_14 Flamegraph showing MediaWiki\Extension\EventBus\EventBus::send taking a disproportionate amount of time]]
[[File:2022-12-09 10.excimer-wall.api.png|alt=2022-12-09_10 Flamegraph showing an usual pattern|thumb|2022-12-09_10 Flamegraph showing an usual pattern for comparison with during the incident]]
'''2022-12-09 15:59:49'''     Amir compares flamegraphs between [https://performance.wikimedia.org/arclamp/svgs/hourly/2022-12-09_14.excimer-wall.api.svgz 2022-12-09_14] and [https://performance.wikimedia.org/arclamp/svgs/hourly/2022-12-09_10.excimer-wall.api.svgz 2022-12-09_10] eventbus is pegged as a potential bottleneck, ottomata tagged in

'''2022-12-09 16:04:56'''     jayme remarks that [https://grafana.wikimedia.org/d/000000561/logstash?from=1670457600000&orgId=1&to=1670608800000&viewPanel=45 logstash is dropping messages]

'''2022-12-09 16:18:21'''     ''[+icinga-wm]    PROBLEM - Some MediaWiki servers are running out of idle PHP-FPM workers in api_appserver at eqiad on alert1001 is CRITICAL: 0.3871 gt 0.3 <nowiki>https://bit.ly/wmf-fpmsat</nowiki> <nowiki>https://grafana.wikimedia.org/d/fRn9VEPMz/application-servers-use-dashboard-wip?orgId=1</nowiki>''

'''2022-12-09 16:21:17'''     cdanis spots upstream errors, retries and connection fail rates in [https://grafana.wikimedia.org/goto/Mgb97H54z?orgId=1 envoy telemetry] as well as [https://grafana.wikimedia.org/goto/gESW4Hc4z?orgId=1 latencies in eventgate eqiad POST p99]

'''2022-12-09 16:23:53'''     ''[+icinga-wm]    PROBLEM - Some MediaWiki servers are running out of idle PHP-FPM workers in api_appserver at eqiad on alert1001 is CRITICAL: 0.6613 gt 0.3 <nowiki>https://bit.ly/wmf-fpmsat</nowiki> <nowiki>https://grafana.wikimedia.org/d/fRn9VEPMz/application-servers-use-dashboard-wip?orgId=1</nowiki>''

'''2022-12-09 16:27:32'''     ottomata spots pod [https://grafana.wikimedia.org/goto/v8OYaqK4k?orgId=1 cpu throttling] in eventgate deployment

'''2022-12-09 16:28:10'''     '''Decision made to increase pod replicas for eventgate-analytics to 30'''

'''2022-12-09 16:29:24'''     Amir hypothesises the increase in logs caused by mobile load.php (T324723) might have been a contributing factor

'''2022-12-09 16:31:17'''     '''[PAGE]''' (PHPFPMTooBusy) firing: Not enough idle php7.4-fpm.service workers for Mediawiki api_appserver at eqiad #page - <nowiki>https://bit.ly/wmf-fpmsat</nowiki> - <nowiki>https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=54&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=eqiad%20prometheus/ops&var-cluster=api_appserver</nowiki> - <nowiki>https://alerts.wikimedia.org/?q=alertname%3DPHPFPMTooBusy</nowiki>

'''2022-12-09 16:35:12'''     '''ottomata bumps eventgate-analytics replicas to 30'''

'''2022-12-09 16:36:17'''     '''[RECOVERY]'''    (PHPFPMTooBusy) resolved: Not enough idle php7.4-fpm.service workers for Mediawiki api_appserver at eqiad #page - <nowiki>https://bit.ly/wmf-fpmsat</nowiki> - <nowiki>https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=54&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=eqiad%20prometheus/ops&var-cluster=api_appserver</nowiki> - <nowiki>https://alerts.wikimedia.org/?q=alertname%3DPHPFPMTooBusy</nowiki>
[[File:2022-12-09 envoy error retry.png|alt=2022-12-09 envoy error retry graphs |thumb|2022-12-09 envoy error retry graphs ]]
[[File:2022-12-09 envoy latency.png|alt=2022-12-09 envoy latency graphs showing the congestion symptoms|thumb|2022-12-09 envoy latency graphs showing the congestion symptoms]]
[[File:2022-12-09 eventgate-analytics pod details.png|alt=2022-12-09 eventgate-analytics pod details showing uneven CPU throttling |thumb|2022-12-09 eventgate-analytics pod details showing uneven CPU throttling]]
'''2022-12-09 16:36:55'''     ''[+icinga-wm]    RECOVERY - Some MediaWiki servers are running out of idle PHP-FPM workers in api_appserver at eqiad on alert1001 is OK: All metrics within thresholds. <nowiki>https://bit.ly/wmf-fpmsat</nowiki> <nowiki>https://grafana.wikimedia.org/d/fRn9VEPMz/application-servers-use-dashboard-wip?orgId=1</nowiki>''

'''2022-12-09 16:44:49'''     '''claime remarks %worker in active state still a little high compared to baseline, but not by much.'''

'''2022-12-09 16:44:49'''     '''Incident closed.'''","===What went well?===

* Incident caught by monitoring
* People with the different ""platform specific"" knowledge were around to debug
* Combining debugging resources between mediawiki through flamegraph, api_appserver, envoy and eventgate-analytics monitoring through grafana allowed for a quick RCA and resolution once the right people came in
* Resolution was very quick thanks to helmfile deployment","* CR Bumping the number of replicas https://gerrit.wikimedia.org/r/c/operations/deployment-charts/+/866612
* CR Reverting the logging increase https://gerrit.wikimedia.org/r/c/mediawiki/core/+/864722
* CR Reverting the replica increase https://gerrit.wikimedia.org/r/c/operations/deployment-charts/+/867597
* An investigation into uneven load-balancing may be warranted https://phabricator.wikimedia.org/T325068",2022-12-09,Unknown,Monitoring,Api,Unknown,Unknown,2022-12-09_api_appserver_worker_starvation.wikitext
"{{Incident scorecard
| task = T324801
| paged-num = 14
| responders-num = 5
| coordinators = [[User:Legoktm|Legoktm]]
| start = 2022-12-06 12:27:18
| end = 2022-12-09 02:39:00
| impact = Visual diffs would have shown no changes (common feature, ~100,000 users of the beta feature on enwiki), and editing old revisions using the VisualEditor would not have worked (rare)
}}A change in the MediaWiki REST API caused requests for old revisions to serve the current revision. It was noticed because visual diffs were all indicating no changes happened.{{TOC|align=right}}",,The issue was manually detected by a human.,"''All times in UTC.''

2022-12-06

* 12:27 MediaWiki 1.40.0-wmf.13 deployment starts to group0 '''(problems begin)'''

2022-12-08

* 23:30 MatmaRex files [[phab:T324801|T324801: REST API serving content of current revision for old revisions]] and marks it as unbreak now!

2022-12-09

* 01:55 Arlolra alerts Transformers group chat that there's an active UBN
* 02:04 Legoktm #pages _security channel and then uses Klaxon
* 02:08 cwhite, legoktm, TheresNoTime start discussing in #wikimedia-operations
* 02:08 cscott recommends a train rollback on [[phab:T324801#8455865|Phabricator]]
* 02:17 cwhite begins train rollback from group2 to group1
* 02:39 train rollback complete '''(problems resolved on group2 wikis, but continue on group0 and group1 wikis)'''
* 04:15 ssastry identifies the culprit via git bisect on [[phab:T324801#8455911|Phabricator]]
* 04:52 ssastry uploads initial patch https://gerrit.wikimedia.org/r/866527 (PS2 uploaded at 05:15, 7 minutes after CI fails)
* 08:14 dkinzler C+2s
* 08:37 merged to master
* 10:41 ladsgroup cherry-picks to 1.40.0-wmf.13
* 10:51 ladsgroup: backport deployed to testservers ([https://sal.toolforge.org/log/OYCC9oQB6FQ6iqKiyHxX SAL])
* 11:00 initial testing by ladsgroup on group1 itwiki (still running wmf.13) is inconclusive due to caching 
* 11:02 ladgroup completes deploy after resolving test issue '''(fix is now live on group0 and group1)''' ([https://sal.toolforge.org/log/9FCM9oQB8Fs0LHO579Xd SAL])
* 13:13 hashar [https://sal.toolforge.org/log/CoAE94QB6FQ6iqKiyeoN rolls group2 forward to wmf.13]","===What went well?===

* Train rollback resolved the issue on group2 wikis, no immediate issues or regressions caused by the rollback itself","* Automated testing to verify behavior of requesting HTML of old revisions from the REST API (https://gerrit.wikimedia.org/r/866621, https://gerrit.wikimedia.org/r/866622)
** Should examine why/how Parsoid's ""old"" api-testing pathways regressed, which would/should have covered this code.  Are there other tests from the old api-testing suite which have similarly vanished?
*** The tests for retrieving revision HTML by revision ID existed and was covering this path - but the revision ID it was using for the test was the current revision. We never had a test specifically for an old revision. This has now been added. [[User:Daniel Kinzler|daniel]] ([[User talk:Daniel Kinzler|talk]]) 16:54, 15 December 2022 (UTC)
* There are alternate 'easier' ways to do rollbacks now.  Update documentation.
* Is the 22-minute long sync-wikiversions expected? No.
** The kubernetes multi-version image build took 10 minutes.  The reason is a bad behavior in the incremental image build process when wikiversions rolls back.  https://phabricator.wikimedia.org/T325576
* Documentation to make it clearer that rollback was the correct solution to this problem.
* Something around testing forward- and backward-compatibility of Parsoid/RESTBase version downgrades
* Something around testing forward-compatibility of ParserCache changes
** The documentation doesn't just need to exist, it needs to be discoverable in the the right spot. Probably in a place that developers would touch when trying to implement backwards compatibility.
* Is there some way to enhance the visibility of breakages in the ""visual diff"" feature, to make it more likely regressions in the feature will be caught during group0 or group1 rollout?
** It would be nice to have Selenium tests for this feature. Selenium tests are slow, brittle, and tricky to set up locally. It would help if we could streamline this.
* Reduce the gap between the UBN being filed and the relevant team springing into action
** Platform Engineering currently doesn't have a notification mechanism for UBNs. A bot posting to Slack would be helpful.",2022-12-09,Unknown,Unknown,Unknown,Unknown,Unknown,2022-12-09_MediaWiki_REST_API.wikitext
"{{Incident scorecard
| task = 
| paged-num = 3
| responders-num = 1
| coordinators = 1
| start = 2022-12-12 20:14:17
| end = 2022-12-12 20:30:00
| metrics = WDQS Uptime SLO
| impact = For about 15 minutes, wdqs queries routed to codfw failed
}}


A large influx in requests led to excessive thread pool usage from codfw blazegraph backends, with concomitant increases in CPU load and throttling filter state size. This triggered a known bug in Blazegraph where its improper thread management leads to deadlock.

The system self healed, perhaps due to application-level throttling state throttling the offender.

Most or all CODFW-routed wdqs requests during the incident window failed.

{{TOC|align=right}}",,"The issue was rapidly detected by pybal monitoring of WDQS. A page was emitted by the monitoring system fairly quickly.

'''2022-12-12 20:13:18''' <+jinxer-wm> (ProbeDown) firing: Service wdqs-ssl:443 has failed probes (http_wdqs-ssl_ip4) #page - https://wikitech.wikimedia.org/wiki/Runbook#wdqs-ssl:443 - https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=All - https://alerts.wikimedia.org/?q=alertname%3DProbeDown

'''2022-12-12 20:13:18''' <+jinxer-wm> (ProbeDown) firing: Service wdqs-ssl:443 has failed probes (http_wdqs-ssl_ip4) - https://wikitech.wikimedia.org/wiki/Runbook#wdqs-ssl:443 - https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=All - https://alerts.wikimedia.org/?q=alertname%3DProbeDown

'''2022-12-12 20:14:17''' <+icinga-wm> PROBLEM - PyBal backends health check on lvs2009 is CRITICAL: PYBAL CRITICAL - CRITICAL - wdqs-heavy-queries_8888: Servers wdqs2003.codfw.wmnet, wdqs2001.codfw.wmnet, wdqs2004.codfw.wmnet, wdqs2002.codfw.wmnet are marked down but pooled: wdqs-ssl_443: Servers wdqs2003.codfw.wmnet, wdqs2001.codfw.wmnet, wdqs2004.codfw.wmnet, wdqs2002.codfw.wmnet are marked down but pooled: wdqs_80: Servers wdqs2003.codfw.wmnet, wdqs2001.codfw.wmnet, wdqs20

'''2022-12-12 20:14:17''' <+icinga-wm> .wmnet, wdqs2002.codfw.wmnet are marked down but pooled https://wikitech.wikimedia.org/wiki/PyBal","'''Beginning of incident'''

'''2022-12-12 20:13:18''' <+jinxer-wm> (ProbeDown) firing: Service wdqs-ssl:443 has failed probes (http_wdqs-ssl_ip4) #page - https://wikitech.wikimedia.org/wiki/Runbook#wdqs-ssl:443 - https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=All - https://alerts.wikimedia.org/?q=alertname%3DProbeDown

'''2022-12-12 20:13:18''' <+jinxer-wm> (ProbeDown) firing: Service wdqs-ssl:443 has failed probes (http_wdqs-ssl_ip4) - https://wikitech.wikimedia.org/wiki/Runbook#wdqs-ssl:443 - https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=All - https://alerts.wikimedia.org/?q=alertname%3DProbeDown

'''2022-12-12 20:14:17''' <+icinga-wm> PROBLEM - PyBal backends health check on lvs2009 is CRITICAL: PYBAL CRITICAL - CRITICAL - wdqs-heavy-queries_8888: Servers wdqs2003.codfw.wmnet, wdqs2001.codfw.wmnet, wdqs2004.codfw.wmnet, wdqs2002.codfw.wmnet are marked down but pooled: wdqs-ssl_443: Servers wdqs2003.codfw.wmnet, wdqs2001.codfw.wmnet, wdqs2004.codfw.wmnet, wdqs2002.codfw.wmnet are marked down but pooled: wdqs_80: Servers wdqs2003.codfw.wmnet, wdqs2001.codfw.wmnet, wdqs20

'''2022-12-12 20:14:17''' <+icinga-wm> .wmnet, wdqs2002.codfw.wmnet are marked down but pooled https://wikitech.wikimedia.org/wiki/PyBal

'''2022-12-12 20:14:17''' <+icinga-wm> PROBLEM - WDQS SPARQL on wdqs2001 is CRITICAL: CRITICAL - Socket timeout after 10 seconds https://wikitech.wikimedia.org/wiki/Wikidata_query_service/Runbook

'''2022-12-12 20:14:17''' <+icinga-wm> PROBLEM - WDQS SPARQL on wdqs2003 is CRITICAL: CRITICAL - Socket timeout after 10 seconds https://wikitech.wikimedia.org/wiki/Wikidata_query_service/Runbook

'''2022-12-12 20:25:00''' Rough end of incident (end of user impact)

'''2022-12-12 20:31:00''' [WDQS] <code>ryankemper@cumin2002:~$ sudo -E cumin -b 4 wdqs2* 'systemctl restart wdqs-blazegraph'</code> By this point, the service had already recovered, but was restarted out of an abundance of caution.",<mark>OPTIONAL: General conclusions (bullet points or narrative)</mark>,{{phab|T325324}},2022-12-12,Unknown,Monitoring,Unknown,Unknown,Unknown,2022-12-12_wdqs_codfw_brief_outage.wikitext
"{{Incident scorecard
| task = T325056
| paged-num = 2
| responders-num = 4
| coordinators = Alexandros
| start = 2022-12-13 12:15:00
| end = 2022-12-13 12:30
| impact = All users were unable to edit for a period of 9 minutes.
}}


A wrong configuration change caused sessionstore pods to be unschedulable in our WikiKube cluster. This resulted in failed edits across all projects for a period of 9 minutes.

{{TOC|align=right}}",,"Automated monitoring detected the issue

(ProbeDown) firing: Service sessionstore:8081 has failed probes (http_sessionstore_ip4) #page -","''All times in UTC.''

12:15 '''OUTAGE BEGINS'''.  '''Alexandros''' becomes IC.

12:15 (ProbeDown) firing: Service sessionstore:8081 has failed probes (http_sessionstore_ip4) #page -

12:16 It becomes apparent that sessionstore is no longer serving requests, after an increase and some pod restarts

12:18 A correlation is made to a change for MatchNodeSelector for sessionstore

12:24 nodeAffinity for specific rack rows was removed manually from sessionstore deployments in k8s (basically <nowiki>https://gerrit.wikimedia.org/r/c/operations/deployment-charts/+/867572</nowiki> was done manually via kubectl edit) '''OUTAGE ENDS'''

12:25 Incident created in wikimediastatus.net

12:27 Status page updated

12:30 Resolved","===What went well?===

* The incident was immediately detected by automated monitoring and the problem was quickly identified and fixed.
* Multiple people responded
* An IC was appointed quickly","Fix for sessionstore deployments:

https://gerrit.wikimedia.org/r/c/operations/deployment-charts/+/867572",2022-12-13,Unknown,Monitoring,Unknown,Unknown,Unknown,2022-12-13_sessionstore.wikitext
"{{Incident scorecard
| task = T325477
| paged-num = 34
| responders-num = 6 (initial page, later others took over final corrections)
| coordinators = Jcrespo
| start = 2022-12-18 17:58:00
| end = 2022-12-18 18:22:00
| metrics = ?
| impact = All API users experienced 5XX errors (11-12 thousand errors per second) or unreasonable latencies for 24 minutes. After that, there was degraded performance in the form of increased -but more reasonable- latency for around 3 hours. Edit rate got low during the 24 minute hard outage period, but recovered quickly after it (to a higher level than usual).
}}

For approximately 24 minutes, uncached calls to the API on the eqiad datacenter overloaded the application servers, running out of threads (all busy) creating unreasonable latency or failing to respond to requests. This caused sending errors to some clients using the action API and Parsoid in the primary datacenter. Elevated latencies persisted for the following 3 hours, when traffic load organically went down.

This was caused by the DiscussionTools MediaWiki extension adding a ResourceLoader module on almost all page views -even non-discussion pages- which created an API call, that, combined with a significant 50% increase in overall traffic, led to an overload and increased latencies on the API cluster. codfw app server cluster was mostly unaffected due to not receiving POST uncached traffic at the time (it is read-only).

{{TOC|align=right}}",,"Monitoring and paging worked as intended, paging everybody (it was a weekend) as soon as the issue become major at 17:56:

* 17:59 Service: [FIRING:1] ProbeDown (10.2.2.22 ip4 api-https:443 probes/service http_api-https_ip4 ops page eqiad prometheus sre)
* 18:00 Service: [FIRING:1] PHPFPMTooBusy api_appserver (ops php7.4-fpm.service page eqiad prometheus sre)
* 18:00 Service: [FIRING:1] FrontendUnavailable (varnish-text page thanos sre)
* 18:01 Service: [FIRING:1] FrontendUnavailable cache_text (page thanos sre)

A [https://phabricator.wikimedia.org/T325477 task] was created also by a community member at 19:03, when the team was already analyzing the issue.

However, there were reports that ""php fpm busy has been flapping all weekend, just not enough to page"".","[[File:World cup traffic increase.png|thumb|right|Increased (around 50%) traffic on both text and eqiad cluster, trigger of the subsequent issues]]
[[File:Discussiontools load world cup.png|thumb|right|Excesive load created by the Discussiontools module, taking more execution time than even ''query'' and ''parse'', the underlying issue]]
[[File:API appserver eqiad busy workers and status codes world cup.png|thumb|right|Direct cause of the outage: running out of available API workers to execute user's requests, leading to errors and latencies]]
[[File:API appserver eqiad latencies world cup.png|thumb|right|Increased latences during the outage, and also after the saturation got resolved]]
[[File:Edit_rate_during_World_Cup.png|thumb|right|Edit rate during the World Cup final: it had elevated values except during the hard outage, probably due to editing bots being affected by the API POST and Parsoid/JS API calls being down]]
[[File:Parsercache_hit_ratio.png|thumb|right|Metrics show a dramatic increase in parsing requests after the 8th deploy and a recovery after the 19th fix]]

''All times in UTC.''",<mark>OPTIONAL: General conclusions (bullet points or narrative)</mark>,"* {{phab|T325739}} Fix sre.mediawiki.restart-appservers cookbook and doc {{done}}
* Patch {{gerrit|869169}} {{done}}
* Patch {{gerrit|869163}} {{done}}
* {{Phab|T325598}}: Avoid uncached action=discussiontoolspageinfo API calls on page load {{done}}
* {{phabricator|T321961}} {{done}}",2022-12-18,Unknown,Monitoring,Api,Unknown,Unknown,2022-12-18_World_Cup.wikitext
"{{Incident scorecard
| task = T325890
| paged-num = 2
| responders-num = 3
| coordinators = jayme
| start = 2022-12-21 09:00:00
| end = 2022-12-21 09:10:00
| metrics = No relevant SLOs exist, shellbox latency/php-fpm worker saturation
| impact = For about 10min syntaxhighlighting was slow or returning errors
}}

…

<!-- Reminder: No private information on this page! -->A sudden spike in requests to shellbox-syntaxhighlight overloaded the service leading to slow response times and increased failures.

The majority of requests where originating from jobrunners and api_appservers.

{{TOC|align=right}}",,"[[File:Shellbox-syntaxhighlight grafana.png|thumb|shellbox-syntaxhighlight grafana dashboard]]
Initially detected by a human spotting alerts on IRC, closely followed by a page.","''All times in UTC.''

*09:00 '''OUTAGE BEGINS''' non paging alerts of failing http-api probes and api_appservers running out of idle workers where observed in #mediawiki-operations; Investigation started
*09:03 paging alert: (ProbeDown) firing: Service shellbox-syntaxhighlight:4014 has failed probes (http_shellbox-syntaxhighlight_ip4)
*09:05 shellbox-syntaxhighlight req/s increased from ~5 to ~70req/s
*09:05 shellbox-syntaxhighlight scaled up from 12 to 40 replicas
*09:09 Alerts recovering
*09:10 '''OUTAGE ENDS'''
*09:11 (CirrusSearchJobQueueBacklogTooBig) firing: CirrusSearch job topic eqiad.mediawiki.job.cirrusSearchLinksUpdate is heavily backlogged with 209k messages
*09:49 cirrusSearchLinksUpdate backlog was cleared
*09:54 shellbox-syntaxhighlight scaled back to 12 replicas

<!-- Reminder: No private information on this page! -->==Detection==
[[File:Shellbox-syntaxhighlight grafana.png|thumb|shellbox-syntaxhighlight grafana dashboard]]
Initially detected by a human spotting alerts on IRC, closely followed by a page.","===What went well?===

* Problem spotted early and countermeasures where taken quickly
* [[File:Envoy telemetry dashboard.png|thumb|envoy telemetry dashboard]]One of the SREs paged knew how to increase the number of syntaxhighlight runners (it's not mentioned on [[Shellbox]])","* Figure out what caused the burst (there's a suggestion a template was changed leading to a lot of pages needing re-rendering at once)
* Document how to scale up shellbox runners? (or link to it from [[Shellbox]])",2022-12-21,Unknown,Unknown,Unknown,Unknown,Unknown,2022-12-21_shellbox-syntaxhighlight.wikitext
"{{Incident scorecard
| task = T326590
| paged-num = 2
| responders-num = 4
| coordinators = dzahn
| start = 2023-01-09 21:04:00
| end = 2023-01-09 21:14:00
| metrics = API appservers
| impact = elevated 5xx, API errors, wbsgetsuggestions failed to return suggestions on Wikidata
}}

<!-- Reminder: No private information on this page! -->",,alerting via icinga/alertmanager/victorops - SMS was sent to SREs on on-call duty shift. manual escalation via Klaxon to more SREs.,"21:02 - first alerts show up on IRC, bot notifies about 'High average GET latency on API appservers'

21:02 - an alerting page was sent for ""ProbeDown (ip4_api-https) (3215)"". It was ACKed within the same minute (46241)

21:03 - an alerting page was sent for: ""PHPFPMTOOBusy api_appserver (3216)""

21:04 - An incident was opened.  '''dzahn''' becomes IC.

21:07 - dzahn sends manual page to SRE via Klaxon to ask for help.

21:09 - DBA marostegui joins and immediately identifies db1143 as non-response, depools it from service

21:10 - A shower of recoveries starts almost instantly, traffic recovers.

21:10 - https://phabricator.wikimedia.org/T326590 is created by users noticing broken suggestions service on Wikidata.

21:12 - recovery page is sent for event 3215

21:13 - recovery page is sent for event 3216

21:17 - recovery page is sent for event 3218 *ipv4 probes prometheus eqiad*, api-https has failed probes

21:18 - recovery page is sent for event 3219, *PHPFPMTooBusy parsoid*

21:24 - recovery page is sent for event 3220 *MariaDB Replica Lag: s4*",Maybe onLinksUpdateComplete should not always lok for commons file usage changes or have some throttle.,* Ticket for MediaWiki? - possibly https://phabricator.wikimedia.org/T314020?,2023-01-09,Unknown,Unknown,Unknown,Unknown,Unknown,2023-01-09_API_errors_-_db1143.wikitext
"{{Incident scorecard
| task = T328354
| paged-num = Batphone
| responders-num = 5
| coordinators = adenisse
| start = 2023-01-10 16:00:00
| end = 2023-01-10 20:57
| impact = Users in Asia were affected for ~11 to 41 minutes
}}

…

<!-- Reminder: No private information on this page! -->eqsin is connected to the core DCs via two transport links, one of them has been suffering a long fiber cut (see [[phab:T322529|T322529]]) the other one went down due to a planned maintenance from the transport provider. 

For ~11min (+ the time user's DNS resolvers pick up eqsin depool, long tail up to 30min) users normally redirected to eqsin (mostly in the APAC region) were only able to read Wikipedia pages already cached in eqsin.{{TOC|align=right}}",,"<mark>Write how the issue was first detected.  Was automated monitoring first to detect it? Or a human reporting an error?</mark>

Automated monitoring

<mark>Copy the relevant alerts that fired in this section.</mark>

16:37 AM <+icinga-wm> PROBLEM - BGP status on cr3-eqsin is CRITICAL: BGP CRITICAL - AS1299/IPv6: Active - Telia, AS1299/IPv4: Active - Telia [[Network monitoring#BGP status|https://wikitech.wikimedia.org/wiki/Network_monitoring%23BGP_status]]

16:44 AM <+icinga-wm> PROBLEM - BGP status on cr2-eqsin is CRITICAL: BGP CRITICAL - No response from remote host 103.102.166.130 [[Network monitoring#BGP status|https://wikitech.wikimedia.org/wiki/Network_monitoring%23BGP_status]]

16:45 AM <+icinga-wm> PROBLEM - Host bast5002 is DOWN: PING CRITICAL - Packet loss = 100%

16:45 AM <+icinga-wm> PROBLEM - Host doh5001 is DOWN: PING CRITICAL - Packet loss = 100%

16:45 AM <+icinga-wm> PROBLEM - Host prometheus5001 is DOWN: PING CRITICAL - Packet loss = 100%

16:46 AM <+icinga-wm> PROBLEM - Host ncredir5001 is DOWN: PING CRITICAL - Packet loss = 100%

16:46 AM <+icinga-wm> PROBLEM - Host netflow5002 is DOWN: PING CRITICAL - Packet loss = 100%

16:46 AM <+icinga-wm> PROBLEM - Host cr2-eqsin #page is DOWN: PING CRITICAL - Packet loss = 100%

16:46 AM <+icinga-wm> PROBLEM - Host durum5002 is DOWN: PING CRITICAL - Packet loss = 100%

16:46 AM <+icinga-wm> PROBLEM - Host cr3-eqsin #page is DOWN: PING CRITICAL - Packet loss = 100%

16:46 AM <+icinga-wm> PROBLEM - Host ncredir5002 is DOWN: PING CRITICAL - Packet loss = 100%

16:46 AM <+icinga-wm> PROBLEM - Host durum5001 is DOWN: PING CRITICAL - Packet loss = 100%

16:46 AM <+icinga-wm> PROBLEM - Host install5001 is DOWN: PING CRITICAL - Packet loss = 100%

16:46 AM <+icinga-wm> PROBLEM - Host doh5002 is DOWN: PING CRITICAL - Packet loss = 100%

16:47 AM <+icinga-wm> PROBLEM - Host upload-lb.eqsin.wikimedia.org is DOWN: PING CRITICAL - Packet loss = 100%

16:47 AM <+icinga-wm> PROBLEM - Host text-lb.eqsin.wikimedia.org is DOWN: PING CRITICAL - Packet loss = 100%

<mark>Did the appropriate alert(s) fire? Was the alert volume manageable?</mark> 

Yes, the appropiate alerts fired.

No, the alert volume was hard to handle on IRC and 3 pages triggered at the same time, two of them escalated to batphone.

<mark>Did they point to the problem with as much accuracy as possible?</mark>

Yes.

<mark>TODO: If human only, an actionable should probably be to ""add alerting"".</mark>
A flood of host down alerts usually mean a network related issue.","'''Dec 22, 2022:''' 

16:06 UTC: Planned Work PWIC225900 Notification from Arelion

'''Jan 9, 2022:'''

16:06 UTC: Reminder for Planned Work PWIC225900 from Arelion

'''Jan 10, 2022:'''

16:00: Service Window for PWIC225900 starts

16:37: <+icinga-wm> PROBLEM - BGP status on cr3-eqsin is CRITICAL: BGP CRITICAL - AS1299/IPv6: Active - Telia, AS1299/IPv4: Active - Telia [[Network monitoring#BGP status|https://wikitech.wikimedia.org/wiki/Network_monitoring%23BGP_status]]

16:44: <+icinga-wm> PROBLEM - BGP status on cr2-eqsin is CRITICAL: BGP CRITICAL - No response from remote host 103.102.166.130 [[Network monitoring#BGP status|https://wikitech.wikimedia.org/wiki/Network_monitoring%23BGP_status]]

16:45 AM <+icinga-wm> PROBLEM - Host bast5002 is DOWN: PING CRITICAL - Packet loss = 100%

16:45 AM <+icinga-wm> PROBLEM - Host doh5001 is DOWN: PING CRITICAL - Packet loss = 100%

16:45 AM <+icinga-wm> PROBLEM - Host prometheus5001 is DOWN: PING CRITICAL - Packet loss = 100%

16:46 AM <+icinga-wm> PROBLEM - Host ncredir5001 is DOWN: PING CRITICAL - Packet loss = 100%

16:46 AM <+icinga-wm> PROBLEM - Host netflow5002 is DOWN: PING CRITICAL - Packet loss = 100%

16:46 AM <+icinga-wm> PROBLEM - Host cr2-eqsin #page is DOWN: PING CRITICAL - Packet loss = 100%

16:46 AM <+icinga-wm> PROBLEM - Host durum5002 is DOWN: PING CRITICAL - Packet loss = 100%

16:46 AM <+icinga-wm> PROBLEM - Host cr3-eqsin #page is DOWN: PING CRITICAL - Packet loss = 100%

16:46 AM <+icinga-wm> PROBLEM - Host ncredir5002 is DOWN: PING CRITICAL - Packet loss = 100%

16:46 AM <+icinga-wm> PROBLEM - Host durum5001 is DOWN: PING CRITICAL - Packet loss = 100%

16:46 AM <+icinga-wm> PROBLEM - Host install5001 is DOWN: PING CRITICAL - Packet loss = 100%

16:46 AM <+icinga-wm> PROBLEM - Host doh5002 is DOWN: PING CRITICAL - Packet loss = 100%

16:47 AM <+icinga-wm> PROBLEM - Host upload-lb.eqsin.wikimedia.org is DOWN: PING CRITICAL - Packet loss = 100%

16:47 AM <+icinga-wm> PROBLEM - Host text-lb.eqsin.wikimedia.org is DOWN: PING CRITICAL - Packet loss = 100%

16:48 AM <bblack> !log depooling eqsin from DNS

16:49 AM <+icinga-wm> PROBLEM - Host cr2-eqsin IPv6 is DOWN: PING CRITICAL - Packet loss = 100%

16:50 AM <+icinga-wm> PROBLEM - Host mr1-eqsin IPv6 is DOWN: PING CRITICAL - Packet loss = 100%

16:50 AM <+icinga-wm> PROBLEM - Host cr3-eqsin IPv6 is DOWN: PING CRITICAL - Packet loss = 100%

16:50 AM <+icinga-wm> PROBLEM - Host ripe-atlas-eqsin IPv6 is DOWN: PING CRITICAL - Packet loss = 100%

16:55 AM <+icinga-wm> RECOVERY - Host netflow5002 is UP: PING OK - Packet loss = 0%, RTA = 247.25 ms

16:55 AM <+icinga-wm> RECOVERY - Host durum5002 is UP: PING OK - Packet loss = 0%, RTA = 238.90 ms

16:55 AM <+icinga-wm> RECOVERY - Host doh5001 is UP: PING OK - Packet loss = 0%, RTA = 244.81 ms

16:55 AM <+icinga-wm> RECOVERY - Host durum5001 is UP: PING OK - Packet loss = 0%, RTA = 242.79 ms

16:55 AM <+icinga-wm> RECOVERY - Host install5001 is UP: PING OK - Packet loss = 0%, RTA = 232.47 ms

16:55 AM <+icinga-wm> RECOVERY - Host ncredir5001 is UP: PING OK - Packet loss = 0%, RTA = 233.62 ms

16:55 AM <+icinga-wm> RECOVERY - Host prometheus5001 is UP: PING OK - Packet loss = 0%, RTA = 250.70 ms

16:55 AM <+icinga-wm> RECOVERY - Host ncredir5002 is UP: PING OK - Packet loss = 0%, RTA = 231.49 ms

16:55 AM <+icinga-wm> RECOVERY - Host cr2-eqsin #page is UP: PING OK - Packet loss = 0%, RTA = 225.39 ms

16:55 AM <+icinga-wm> RECOVERY - Host cr3-eqsin #page is UP: PING OK - Packet loss = 0%, RTA = 245.89 ms

16:55 AM <+icinga-wm> RECOVERY - Host doh5002 is UP: PING OK - Packet loss = 0%, RTA = 253.59 ms

16:55 AM <+icinga-wm> RECOVERY - Host cr2-eqsin IPv6 is UP: PING OK - Packet loss = 0%, RTA = 224.01 ms

16:55 AM <+icinga-wm> RECOVERY - Host text-lb.eqsin.wikimedia.org is UP: PING OK - Packet loss = 0%, RTA = 231.29 ms

16:55 AM <+icinga-wm> RECOVERY - Host mr1-eqsin IPv6 is UP: PING OK - Packet loss = 0%, RTA = 245.15 ms

16:56 AM <+icinga-wm> RECOVERY - Host cr3-eqsin IPv6 is UP: PING OK - Packet loss = 0%, RTA = 243.03 ms

16:56 AM <+icinga-wm> RECOVERY - Host bast5002 is UP: PING OK - Packet loss = 0%, RTA = 254.35 ms

16:56 AM <+icinga-wm> RECOVERY - Host ripe-atlas-eqsin IPv6 is UP: PING OK - Packet loss = 0%, RTA = 251.84 ms

16:56 AM <+icinga-wm> RECOVERY - Host upload-lb.eqsin.wikimedia.org is UP: PING OK - Packet loss = 0%, RTA = 237.02 ms

16:57 AM <+icinga-wm> RECOVERY - OSPF status on cr1-codfw is OK: OSPFv2: 6/6 UP : OSPFv3: 6/6 UP [[Network monitoring#OSPF status|https://wikitech.wikimedia.org/wiki/Network_monitoring%23OSPF_status]]

17:00 AM <+icinga-wm> PROBLEM - Check unit status of netbox_ganeti_eqsin_sync on netbox1002 is CRITICAL: CRITICAL: Status of the systemd unit netbox_ganeti_eqsin_sync [[Monitoring/systemd unit state|https://wikitech.wikimedia.org/wiki/Monitoring/systemd_unit_state]]

8:33 UTC: repooling",<mark>OPTIONAL: General conclusions (bullet points or narrative)</mark>,"* Create a backup GRE tunnel - https://phabricator.wikimedia.org/T327265
* Ensure the long down transport link comes back up properly once fixed - https://phabricator.wikimedia.org/T322529
* Automatically parse maintenance notifications and alert on conflicting maintenance - https://phabricator.wikimedia.org/T230835

<mark>Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.</mark>

<mark>Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFIRE (Pending Review & Scorecard)]]  Phabricator tag to these tasks.</mark>",2023-01-10,Unknown,Monitoring,Unknown,Unknown,Unknown,2023-01-10_eqsin_network_outage.wikitext
"{{Incident scorecard
| task = T327001
| paged-num = Everyone (batphone)
| responders-num = 5 (3 SRE, 2 volunteers)
| coordinators = None
| start = 2023-01-14 08:17:00
| end = 2023-01-14 10:38:00
| impact = No user-facing impact; reduced redundancy (and inability to make some changes).
}}

…

Switch asw-b2-codfw failed; asw-b-codfw master failed over to b7 as designed. This, however, left all the systems in B2 offline. A volunteer noticed the alerts, and used Klaxon. We were unable to restore asw-b2-codfw to service, and depooled the swift and thanos frontends in B2. This left us operational, but at reduced redundancy. 

A further complication was that lvs2008 reaches all of row B via asw-b2-codfw, which meant that trying to change things in codfw on Monday was difficult; as a result of which mediawiki in codfw was depooled until asw-b2-codfw could be replaced.

{{TOC|align=right}}",,"Automated monitoring detected the outage, a human made the decision to Klaxon.

<pre>
<icinga-wm> PROBLEM - Host cp2031 is DOWN: PING CRITICAL - Packet loss = 100%
<icinga-wm> PROBLEM - Host ms-be2046 is DOWN: PING CRITICAL - Packet loss =
<icinga-wm> PROBLEM - Host elastic2041 is DOWN: PING CRITICAL - Packet loss =
	    100%
<icinga-wm> PROBLEM - Host kafka-logging2002 is DOWN: PING CRITICAL - Packet
	    loss = 100%
<icinga-wm> PROBLEM - Host mc2043 is DOWN: PING CRITICAL - Packet loss = 100%
<icinga-wm> PROBLEM - Host thanos-fe2002 is DOWN: PING CRITICAL - Packet loss
	    = 100%
<icinga-wm> PROBLEM - Host elastic2063 is DOWN: PING CRITICAL - Packet loss =
	    100%  [08:19]
<icinga-wm> PROBLEM - Host cp2032 is DOWN: PING CRITICAL - Packet loss = 100%
<icinga-wm> PROBLEM - Host elastic2064 is DOWN: PING CRITICAL - Packet loss =
	    100%
<icinga-wm> PROBLEM - Host elastic2057 is DOWN: PING CRITICAL - Packet loss =
	    100%
<icinga-wm> PROBLEM - Host lvs2008 is DOWN: PING CRITICAL - Packet loss = 100%
<icinga-wm> PROBLEM - Host elastic2077 is DOWN: PING CRITICAL - Packet loss =
	    100%
<icinga-wm> PROBLEM - Host elastic2078 is DOWN: PING CRITICAL - Packet loss =
	    100%
<icinga-wm> PROBLEM - Host mc2042 is DOWN: PING CRITICAL - Packet loss = 100%
<icinga-wm> PROBLEM - Host ms-fe2010 is DOWN: PING CRITICAL - Packet loss =
	    100%
<icinga-wm> PROBLEM - Host ms-be2041 is DOWN: PING CRITICAL - Packet loss =
	    100%
<icinga-wm> PROBLEM - Host ml-cache2002 is DOWN: PING CRITICAL - Packet loss =
	    100%
<icinga-wm> PROBLEM - Host elastic2042 is DOWN: PING CRITICAL - Packet loss =
	    100%
<icinga-wm> PROBLEM - BGP status on cr1-codfw is CRITICAL: BGP CRITICAL -
	    AS64600/IPv4: Connect - PyBal
	    https://wikitech.wikimedia.org/wiki/Network_monitoring%23BGP_status
<icinga-wm> PROBLEM - Router interfaces on cr1-codfw is CRITICAL: CRITICAL:
	    host 208.80.153.192, interfaces up: 127, down: 2, dormant: 0,
	    excluded: 0, unused: 0:
	    https://wikitech.wikimedia.org/wiki/Network_monitoring%23Router_interface_down
<icinga-wm> PROBLEM - Juniper virtual chassis ports on asw-b-codfw is
	    CRITICAL: CRIT: Down: 7 Unknown: 0
	    https://wikitech.wikimedia.org/wiki/Network_monitoring%23VCP_status
<icinga-wm> PROBLEM - BGP status on cr2-codfw is CRITICAL: BGP CRITICAL -
	    AS64600/IPv4: Connect - PyBal
	    https://wikitech.wikimedia.org/wiki/Network_monitoring%23BGP_status
<jinxer-wm> (virtual-chassis crash) firing: Alert for device
	    asw-b-codfw.mgmt.codfw.wmnet - virtual-chassis crash   -
	    https://alerts.wikimedia.org/?q=alertname%3Dvirtual-chassis+crash
</pre>

The initial DOWN alerts came before alerting picked up the switch failure; in an ideal world we would have picked up the switch failure and not separately alerted about the dependent hosts.

It's not clear if a switch failing should page, given we are able to continue without one.","''All times in UTC.''

*08:18 asw-b2-codfw fails, hosts in B2 alert as DOWN '''OUTAGE BEGINS'''
*08:21 asw-b-codfw.mgmt.codfw.wmnet - virtual-chassis crash alert
*08:28 asw-b-codfw.mgmt.codfw.wmnet recovered from virtual-chassis crash [failover to B7 complete?]
*08:35 volunteer (RhinosF1) pings a SRE on IRC (_joe_)
*08:43 volunteer opens https://phabricator.wikimedia.org/T327001 at UBN
*08:58 second volunteer (taavi) announces they will Klaxon
*09:01 VO pages batphone
*09:08 first SRE (godog) responds
*09:17 Emperor depools ms-fe-2010 and thanos-fe2001
*09:47 having determined we don't have a switchable PDU that would enable a powercycle, godog issues `request system reboot member 2` as a last-ditch attempt to restart the failed device. This fails.
*10:00 godog emails dc-ops about the failure, noting lack of user impact; notes SRE can't remote power-cycle the device, suggests it can probably wait until Monday, asks for confirmation.
*10:21 XioNoX confirms switch console nonresponsive, all remote options exhausted, next step is replace and RMA switch (since even if we could power it back on at this point, it should be considered likely-to-fail-again).
*10:38 confirmed normal operations with reduced redundancy, no further action needed until next business day '''OUTAGE ENDS'''
*18:48 replacement switch put into place, but not configured",,"* Decide whether switch failure should page or not
* Cookbook for rack downtime - https://phabricator.wikimedia.org/T327300

<mark>Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFIRE (Pending Review & Scorecard)]]  Phabricator tag to these tasks.</mark>",2023-01-14,Unknown,Monitoring,Logging,Unknown,Unknown,2023-01-14_asw-b2-codfw_failure.wikitext
"{{Incident scorecard
| task = T327001
| paged-num = ?
| responders-num = ?
| coordinators = effie, inflatador
| start = 2023_01_17 13:00:00
| end = not ongoing/follow up stage
| impact = All hosts in codfw row B lost network connectivity. End-user impact TBD.
}}

Two main issues happened during this window:

# The prep work to bring online the replacement switch from the [[labsconsole:Incidents/2023-01-14_asw-b2-codfw_failure|prior incident]] triggered a Junos bug which brought instability in row B (mostly impacting connectivity between the different switches in row B)
# Another Junos bug (triggered by an operator error) broke IPv6 connectivity for the whole row

The first point was quickly fixed, while the 2nd required onsite work to minimize downtime as it required B2 to come back up to be able to reboot B7 (those two are the uplinks to the core routers).

[[File:Screen Shot 2023-01-17 at 1.51.45 PM.png|alt=Switch status during incident|thumb|Switch status during incident]]

{{TOC|align=right}}",,"* 12:49: LibreNMS alerts fired. Example verbiage: ''Alert for device asw-b-codfw.mgmt.codfw.wmnet - virtual-chassis crash''
* The issues were triggered by Netops work, so engineers were already looking and identified the bugs quickly

<mark>Did the appropriate alert(s) fire? Was the alert volume manageable? Did they point to the problem with as much accuracy as possible?</mark>

Yes and yes","''All times in UTC.''

* 12:49: #pages for hosts down in codfw
* 12:57: codfw frontend depooled  geoip/generic-map/codfw => DOWN
* 13:00  Incident opened. effie becomes IC.
* 13:01: conftool action : set/pooled=false; selector: dnsdisc=restbase-async,name=codfw
* 13:01 : conftool action : set/pooled=true; selector: dnsdisc=restbase-async,name=.*
* 13:12: Klaxxoned all SREs
* 13:13: START - Cookbook sre.discovery.service-route check citoid: maintenance
* 13:14: START - Cookbook sre.discovery.service-route depool mobileapps in codfw: maintenance
* 13:14: <Emperor> depool swift from codfw [ sudo confctl --object-type discovery select 'dnsdisc=swift,name=codfw' set/pooled=false] 
* 13:16: misconfiguration on router fixed, IPv4 works, IPv6 still not working
* 13:26 _joe_: depooling all services in codfw
* 13:27: jynus: restarting manually replication on es2020, may require data check afterwards
* 13:35: mvernon: conftool action : set/pooled=false; selector: dnsdisc=thanos-swift,name=codfw
* 13:35: mvernon: conftool action : set/pooled=false; selector: dnsdisc=thanos-query,name=codfw
* 13:37: jiji: conftool action : set/pooled=false; selector: dnsdisc=recommendation-api,name=codfw
* 13:40: claime: investigating unreachable etcds on ganeti host ganeti2020.codfw.wmnet
* 14:10: restart cassandra on aqs2005 (didn’t help)
* 14:39: topranks: I added a static ARP entry for the secondary IPv4 addresses belonging to restbase2013 on cr2-codfw and they are reachable again 
* 14:39: so the issue is similar to the IPv6 problem, in that the switch is not forwarding certain multicast/broadcasts (some ARP, all ICMPv6 ND)
* 15:26: XioNoX: we're going to bring B2 into the row B virtual chassis,
* 15:29: :XioNoX: rack B2 comming up, looks stable
* 15:42: claime: etcd clusters OK
* 16:20 inflatador becomes IC
* 16:59 effie:  !log pooling back depooled mw servers in codfw
* 17:07 bblack: confirmed that all B2 hosts seem to be on private1-b-codfw, and yeah all have the :118: issue (except the one I manually fixed)
* 17:16 bblack: │ trying the cumin run
* 17:17: bblack: [done]","===What went well?===

* Alerts helped pinpoint the root cause almost immediately
* The prep work was done in advance of scheduled onsite work
* Most services continued to work over IPv4 when IPv6 was not working","* Create a cookbook to properly depool (or switchover) all services from a datacenter [[phab:T327665|T327665]]
** Cookbook for rack depool - https://phabricator.wikimedia.org/T327300
* Visually represent (grafana) where a service is being served from any given time (eqiad, codfw, or both), so we can return to the same state before the incident [[phab:T327663|T327663]]
* Data check es2020 {{phab|T327770}}
* Upgrade all of eqiad and codfw rows - https://phabricator.wikimedia.org/T327248
* (longer term) Plan codfw row A/B top-of-rack switch refresh - https://phabricator.wikimedia.org/T327938",2023-01-17,Unknown,Unknown,Unknown,Unknown,Unknown,2023-01-17_asw-b2-codfw_failure_redux.wikitext
"{{Incident scorecard
| task = T327196
| paged-num = Sukhbir Singh
| responders-num = 2 volunteer sysadmins, 1 SWE, plus some SREs arriving later
| coordinators = Taavi (de facto)
| start = 2023-01-17 18:31:11
| end = 2023-01-17 18:44:23
| metrics = not sure
| impact = For roughly 15 minutes, all wikis were unreachable for logged-in users and non-cached pages.
}}

An issue with inconsistent state of deployed code during a [[Backport windows|backport deployment]] caused MediaWiki to crash for all logged-in page views.

The root cause of this issue was MediaWiki re-reading and applying changes to extension.json before a php-fpm restart would have picked up changes to the PHP code.

{{TOC|align=right}}",,"Humans and automated alerts detected the issue quickly, and the alert volume was manageable.","''All times in UTC.''

*18:16 A volunteer starts to backport five patches (3 config changes, one core change and one DiscussionTools change)
*18:31 The patches get deployed to the canaries. Some errors are returned, although the canary checks don't prevent the patch from moving forwards presumably due to the short time between the file sync and php-fpm restarts.
*18:33 Sync to the entire cluster starts. '''OUTAGE BEGINS'''
*18:34:51 First user report on IRC
*18:34:55 First automated alert: <+icinga-wm> PROBLEM - MediaWiki exceptions and fatals per minute for appserver on alert1001 is CRITICAL: 2.651e+04 gt 100
*18:35:42 Backport is cancelled by the deployer. This was during the k8s sync phase, before php-fpm restarts started. This leaves the cluster in an inconsistent state between code on disk and code running until the revert is deployed.
*18:37:43 First paging alert: <+jinxer-wm> (FrontendUnavailable) firing: varnish-text has reduced HTTP availability #page
*18:39 Revert sync starts
*18:48 Revert sync is done '''OUTAGE ENDS'''
*~19:30 A revised patch is synced successfully","===What went well?===

* The issue was detected early and the offending patch could be isolated early
** Technically in this case time-to-recovery would have been faster if the backport would not have been cancelled. However, in general, the author believes that it is a good thing that the deployer noticed the issue quickly and the first instinct was to deploy a revert, since that is the fastest way to recovery in most cases of faulty patches in this process.
* The revert worked as is",* TODO: can canary checks detect this issue?,2023-01-17,Unknown,Unknown,Unknown,Unknown,Unknown,2023-01-17_MediaWiki.wikitext
"{{Incident scorecard
| task = T327815
| paged-num = 5
| responders-num = 6
| coordinators = [[User:BCornwall|Brett Cornwall]]
| start = 20:55
| end = 21:15
| impact = Wikipedia's session storage suffered an outage of about 15 minutes (eqiad). This caused users to be unable to log in or edit pages.
| metrics=No SLO exists. centrallogin and session loss metrics spiked as a result
}}

{{TOC|align=right}}

Session storage is provided by an HTTP service ([[Kask]]) that uses [[Cassandra]] for persistence. As part of routine maintenance, one of the Cassandra hosts in eqiad (sessionstore1001) was rebooted.  While the host was down, connections were removed (de-pooled) by Kask, and requests rerouted to the remaining two, as expected.  However, once the host rejoined the cluster, clients that selected sessionstore1001 as coordinator encountered errors (an inability to achieve <code>LOCAL_QUORUM</code> consistency).

This is likely (at least) similar to [[Incidents/2022-09-15 sessionstore quorum issues]] (if not in fact, the same issue).",,"Monitoring did not alert; A manual page was issued once <code>TheresNoTime</code> noticed an issue/Users started reporting issues:

<code>
21:57:42 <TheresNoTime> Successful wiki edits has just started to drop, users reported repeated ""loss of session data"" persisting a refresh
</code>

As alerts did not fire, manual debugging was used to determine the issue at hand. It took little time to determine that [[SessionStorage]] was the issue.","''All times in UTC.''

<gallery mode=""packed"">
File:2023-01-24 sessionstore outage - save failures.png|Save failures due to ""session loss""
File:2023-01-24 outage successful wiki edits - grafana.png|Successful edits halving (one of two main DCs being affected)
File:Sessionstore - Grafana Dashboard 2023-01-24.png|Overview of traffic
</gallery>

<syntaxhighlight lang=""json"">
...
{""msg"":""Error writing to storage (Cannot achieve consistency level LOCAL_QUORUM)"",""appname"":""sessionstore"",""time"":""2023-01-24T21:10:27Z"",""level"":""ERROR"",""request_id"":""ea9a0eef-256d-4eb1-bfd5-863a66aacee9""}
{""msg"":""Error reading from storage (Cannot achieve consistency level LOCAL_QUORUM)"",""appname"":""sessionstore"",""time"":""2023-01-24T21:10:27Z"",""level"":""ERROR"",""request_id"":""58ad97ee-0025-4701-b288-4df39a38eb8a""}
...
</syntaxhighlight>

<syntaxhighlight lang=text>
...
INFO  [StorageServiceShutdownHook] 2023-01-24 20:50:26,403 Server.java:179 - Stop listening for CQL clients
INFO  [main] 2023-01-24 20:54:52,649 Server.java:159 - Starting listening for CQL clients on /10.64.0.144:9042 (encrypted)...
...
</syntaxhighlight>

* 20:50 [[User:Eevans|urandom]] [https://sal.toolforge.org/log/pS-L5YUB8Fs0LHO5xI5F reboots sessionstore1001.eqiad.wmnet] ({{PhabT|325132}})
* 20:54 Cassandra on sessionstore1001 comes back online; Successful wiki edits drop to below half of expected number ('''OUTAGE BEGINS''')
* 20:57 [[User:Samtar|TheresNoTime]] notices drop in successful wiki edits, mentions in <code>#wikimedia-operations</code>
* 21:07 [[User:Eevans|urandom]] rolling restarts sessionstore service (Kask)
* 21:09 Manual [https://portal.victorops.com/ui/wikimedia/incident/3327/details Critical page in VictorOps] from <code>taavi</code>
* 21:10 Successful wiki edits climb back up to expected levels ('''OUTAGE ENDS''')
* 21:14 De-pooling eqiad suggested but not yet executed
* 21:15 urandom notices cessation of issues, announces to <code>#wikimedia-operations</code>","===What went well?===

* Once the issue was identified, resolution was relatively quick","* <s>Setup notifications for elevated 500 error rate (sessionstore) ({{PhabT|327960}})</s> {{Done}}
* <s>Notifications from service error logs(?) ({{PhabT|320401}})</s> {{Done}}
* <s>Determine root cause of unavailable errors (i.e. ''""cannot achieve consistency level""'') ({{PhabT|327954}})</s> {{Done}}
* <s>De-pool datacenter prior to hosts reboots (as an interim to properly fixing the connection pooling)</s> {{Done}}",2023-01-24,Unknown,Monitoring,Unknown,Unknown,Unknown,2023-01-24_sessionstore_quorum_issues.wikitext
"{{Incident scorecard
| task = T323920
| paged-num = unknown
| responders-num = 5
| coordinators = Adam Wight
| start = 2023-01-30 12:46:00
| end = 2023-01-30 13:31:00
| metrics = Maps performance: https://grafana.wikimedia.org/goto/t9kVNV04z?orgId=1
| impact = No maps were rendered during the outage.
}}

An upgrade to Kartotherian caused spurious Icinga alerts so was rolled back.  Rollback fails and the service goes down, due to non-robustness in the deployment configuration templates.

{{TOC|align=right}}",,"Icinga alerts for ""kartotherian endpoints health"" in #wikimedia-operations began to fire, for example:
 13:27 <+icinga-wm> PROBLEM - kartotherian endpoints health on maps2006 is CRITICAL: /{src}/{z}/{x}/{y}.{format} (Untitled test) is CRITICAL: Test Untitled test returned the 
                    unexpected status 301 (expecting: 200): /{src}/{z}/{x}/{y}@{scale}x.{format} (Untitled test) is CRITICAL: Test Untitled test returned the unexpected 
                    status 404 (expecting: 200): /{src}/info.json (Untitled test) is CRITICAL: Test Untitled test returned the unexpected status 404 (expecting

User claime correctly guessed that the alerts were related to awight's deployment and pinged them in IRC.

We learned later that these alerts were spurious, caused by an automatically-generated monitoring job ({{PhabT|328437}}).

When rollback failed, we monitored full service failure through its dedicated dashboard, https://grafana.wikimedia.org/d/000000305/maps-performances.","''All times in UTC.''

* 12:21 Update kartotherian service to [kartotherian/deploy@42a07d3] on the Beta Cluster.  The build is smoke-tested and is healthy.
* 12:25 Deploy to production [kartotherian/deploy@42a07d3]: Disable traffic mirroring from codfw to eqiad (duration: 02m 44s).  The service remains functional.
* 12:27 Icinga begins alerting about ""kartotherian endpoints health"", with nonsense template variables in the URLs like ""/{src}/info.json""
* 12:46 Attempt to roll back kartotherian. Finished deploy [kartotherian/deploy@5c58f8f]: Roll back kartotherian (duration: 01m 27s).  However, the service fails to restart, and we start looking for an explanation.
* 12:46 '''OUTAGE BEGINS'''
* 13:16 We learn that kartotherian configuration is broken only if deploying without the ""--env"" flag, now {{PhabT|328406}}.
* 13:31 Finished deploy [kartotherian/deploy@5c58f8f] (codfw and eqiad).  Kartotherian is successfully rolled back everywhere.
* 13:31 '''OUTAGE ENDS'''
[[File:Maps service errors 2023-01-30.png|none|frame]]",A more robust or fully automated and containerized deployment for the maps service would have prevented an outage.,"* Improve documentation for maps production deployment.
* {{Done}} Fix Icinga monitoring for maps, {{PhabT|328437}}
* Make kartotherian configuration robust enough to deploy without the ""--env"" flag, {{PhabT|328406}}
* Improve documentation about how the OpenAPI specification is automatically wired into CI and monitoring, {{PhabT|328524}}.
* Containerize the kartotherian service, {{PhabT|198901}}",2023-01-30,Unknown,Monitoring,Unknown,Unknown,Unknown,2023-01-30_kartotherian.wikitext
"{{Incident scorecard
| task = T329064
| paged-num = 
| responders-num = 2
| coordinators = Andrew Otto
| start = 2022-10-31
| end = 2023-02-07
| impact = On 2022-10-31, the Event Platform team merged a change to the EventBus extension to produce the new mediawiki.page-change event stream.  This change accidentally unregistered the older hook handler that resulted in the mediawiki.page-undelete stream being produced.
}}


We are not aware of all consumers of this stream.  No one noticed this change for over 3 months.  

'''Root Cause'''

The developer (Andrew Otto) and reviewers did not catch the [[gerrit:c/mediawiki/extensions/EventBus/+/821776/30/extension.json|accidental change to extension.json]] that unregistered the EventBusHooks:onPageUndelete hook handler.

'''Affected Datasets and Services'''

The main fallout is that WDQS and WCQS will have [[wikidata:Wikidata:Report_a_technical_problem/WDQS_and_Search#Coolidge_Auditorium_(Q115608572)_strange_behavior_in_query|inconsistencies]] in their downstream datastores: any wiki pages that were undeleted during this time period will not be available in WDQS. There may be exceptions to this, the WDQS updater is supposed to detect inconsistencies (i.e. getting an edit on a deleted page) and apply some reconciliation but apparently this system did not work as expected here. Resolving the inconsistencies for WDQS will be achieved via full data-reload (something that was already in progress).

There may be other affected services as well.  The event.mediawiki_page_undelete table in Hive will be empty for this time.  We also expose this stream publicly via stream.wikimedia.org, so if there are external consumers (Internet Archive?) they will also have missed these page undelete events.

{{TOC|align=right}}",,,"''All times in UTC.''

*'''2022-10-31''' Andrew Otto merges a [[gerrit:c/mediawiki/extensions/EventBus/+/821776/30/extension.json|change to EventBus extension]] that causes mediawiki.page-undelete events to not be sent.  This is deployed over the next week as part of the MediaWiki deployment train.
*'''2023-02-07''' - A [[wikidata:Wikidata:Report_a_technical_problem/WDQS_and_Search#Coolidge_Auditorium_(Q115608572)_strange_behavior_in_query|user reports inconsistencies]] in WDQS results.  David Cause asks Andrew Otto about any known issues with mediawiki.page-undelete.
*'''2023-02-07''' - Andrew Otto discovers the mistake, [[gerrit:c/mediawiki/extensions/EventBus/+/887332|pushes a fix]], and has the fix deployed in a backport deploy window.
*'''2023-02-07 -'''  '''OUTAGE ENDS'''

<!-- Reminder: No private information on this page! -->==Links to relevant documentation ==

* [[Event*#EventBus]]",,"* [[phab:T329070|Add automated stream throughput alerting: T329070]]
* Review the “reconciliation” mechanism of the WDQS updater to understand why it was not able to recover these missing events after further edits on the affected entities and fix it: [[phab:T329089|T329089]] {{done}}",2023-02-07,Unknown,Unknown,Unknown,Unknown,Unknown,2023-02-07_mediawiki.page-undelete_event_stream.wikitext
"{{Incident scorecard
| task = T331461
| paged-num = 0
| responders-num = 0
| coordinators = n/a
| start = 2023-02-11 10:44
| end = 2023-02-11 12:58
| metrics = [https://wikitech.wikimedia.org/wiki/SLO/logstash Logstash's latency SLO] in eqiad was affected.
| impact = Logstash messages were delayed for 2h14m. At peak (10:58), up to 45% of messages were delayed.
}}

eqiad's Logstash experienced message congestion that exhausted the latency budget. This incident consumed 160% of the quarterly budget for delayed messages.

{{TOC|align=right}}",,"One IRC-only alert fired:
 [10:45:55] <jinxer-wm> (LogstashKafkaConsumerLag) firing: Too many messages in kafka logging - <nowiki>https://wikitech.wikimedia.org/wiki/Logstash#Kafka_consumer_lag</nowiki> - <nowiki>https://grafana.wikimedia.org/d/000000484/kafka-consumer-lag?var-cluster=logging-eqiad&var-datasource=eqiad%20prometheus/ops</nowiki> - <nowiki>https://alerts.wikimedia.org/?q=alertname%3DLogstashKafkaConsumerLag</nowiki>
It was Saturday, and there was no page, so nobody saw or responded to the alert. The problem wasn't noticed until the end of the SLO quarter, when we discovered in the normal reporting process that Logstash had missed its latency SLO in eqiad.","<mark>Write a step by step outline of what happened to cause the incident, and how it was remedied.  Include the lead-up to the incident, and any epilogue.</mark>

<mark>Consider including a graphs of the error rate or other surrogate.</mark>

''All times in UTC.''

*<mark>TODO: Triggering event for the Parsoid errors.</mark>
*10:41 Beginning of a flood of exceptions logged from Parsoid, with message: <code>[92ed9dea-d7d4-4b36-9398-202a2e4ccb4c] /w/rest.php/www.mediawiki.org/v3/page/pagebundle/Extension%3ATemplateStylesExtender/5769267   PHP Notice: Trying to get property 'nextSibling' of non-object</code> [https://logstash.wikimedia.org/goto/9ab958316b499d6fd129a43ef9f78204 (logstash)]
*10:44 As events accumulate on the Kafka queue faster than Logstash can consume them, the fraction of events affected by consumer lag (measured by Burrow) rises from its normal value of zero. [https://grafana-rw.wikimedia.org/d/slo-Logstash/logstash-slo-s?orgId=1&from=1669881600000&to=1677657599000 (dashboard)] '''OUTAGE BEGINS'''
*10:45 Alert fires: ''(LogstashKafkaConsumerLag) firing: Too many messages in kafka logging'' [https://wm-bot.wmcloud.org/logs/%23wikimedia-operations/20230211.txt (IRC)]
*10:49 Exception logging hits its peak: 54.6k messages/sec.
*10:54 Last exception is logged from Parsoid. In total, 8,643,149 messages were logged. Logstash continues to work through the queue. [https://logstash.wikimedia.org/goto/9ab958316b499d6fd129a43ef9f78204 (logstash)]
*10:58 Kafka lag hits its peak: 45.8% of messages are lagged. [https://grafana-rw.wikimedia.org/d/slo-Logstash/logstash-slo-s?orgId=1&from=1669881600000&to=1677657599000 (dashboard)]
*12:55 Alert resolves: ''(LogstashKafkaConsumerLag) resolved: Too many messages in kafka logging'' [https://wm-bot.wmcloud.org/logs/%23wikimedia-operations/20230211.txt (IRC)]
*12:56 The last backlogged Parsoid events are processed, and the fraction of lagged events returns to zero. [https://grafana-rw.wikimedia.org/d/slo-Logstash/logstash-slo-s?orgId=1&from=1669881600000&to=1677657599000 (dashboard)] '''OUTAGE ENDS'''

<!-- Reminder: No private information on this page! -->
<gallery>
File:2023-02-11 incident parsoid fatals.png|Parsoid fatals
File:2023-02-11 incident logstash latency sli.png|Logstash latency
</gallery>",<mark>OPTIONAL: General conclusions (bullet points or narrative)</mark>,"* <mark>TODO: Decide whether to include an action item for a paging alert on Kafka lag/backlog</mark>
* ...

<mark>Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.</mark>

<mark>Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFIRE (Pending Review & Scorecard)]]  Phabricator tag to these tasks.</mark>",2023-02-11,Unknown,Unknown,Logging,Unknown,Unknown,2023-02-11_logstash_latency.wikitext
"{{Incident scorecard
| task = T330300
| paged-num = 0
| responders-num = ~7
| coordinators = Jcrespo
| start = 2023-02-22 11:03:25 (major impact starts at 2023-02-22 12:16:21)
| end = 2023-02-22 12:18:48
| metrics = ?
| impact = For approximately 2 minutes, editing was disabled site-wide. For approximately 54 minutes, editing failed for some users in the codfw datacenter (around 1-2% of all edits)
}}

While performing a [[phab:T330271|live switchover test]] in advance of the [[phab:T327920|2023 WMF datacenter switchover]], an existing logical bug on the switchover test script accidentally set the secondary datacenter in read-only mode. While this didn't disrupt most users, mobile editing for people geolocated to codfw app servers (mostly, people in the Americas, and part of Asia and Oceania) had the editing interface disabled (while desktop users were redirected to edit through eqiad). While trying to fix this issue, an tooling interface issue caused all datacenters to be set in read-only mode, disabling editing for all users. This was quickly reverted for both datacenters and editing was restored.

{{TOC|align=right}}",,"Editing issue from mobile + codfw:
* No alerting went off because of this
* Reports from #wikimedia-tech surfaced ongoing issues when editing from the mobile interface (read only disabled the edit button, while on desktop edits were sent to codfw)

Full read only mode issue:
* [12:20:17] <jinxer-wm>	 (MediaWikiHighErrorRate) firing: (4) Elevated rate of MediaWiki errors - appserver - https://wikitech.wikimedia.org/wiki/Application_servers/Runbook  - https://alerts.wikimedia.org/?q=alertname%3DMediaWikiHighErrorRate

Although by this time the issue had been already corrected.

Specifically, failing to set codfw as read-write wasn't detected as failing until some time passed and reports confirm the issue persisted.","[[File:Incident editing disruption 2023-02-22.png|thumb|right|Editing disruption]]
[[File:DBReadOnlyError_exceptions_during_the_2023-02-22_read_only_incident.png|thumb|right|Codfw read only exceptions]]
[[File:Read only false exceptions during the 2023-02-22 read only incident.png|thumb|right|read_only=false exceptions]]

''All times in UTC.''

*11:03 <+logmsgbot> !log cgoubert@cumin1001 START - Cookbook sre.switchdc.mediawiki.02-set-readonly
*11:03 <+logmsgbot> !log cgoubert@cumin1001 [DRY-RUN] MediaWiki read-only period starts at: 2023-02-22 11:03:19.149671 ''Mediawiki is now read-only in codfw only -'' '''Minor editing outage starts now'''
*11:13 <+logmsgbot> !log cgoubert@cumin1001 END (PASS) - Cookbook sre.switchdc.mediawiki.07-set-readwrite (exit_code=0)
''Only sets read-write in eqiad - Codfw is still read-only with the switchover message''

User reports warn of ongoing issues (most edits from eqiad app servers and desktop-codfw can flow normally):
*11:39 <Yahya> Hello, bnwiki is now read-only. Some users can edit and some can't. Can anyone tell me if any maintenance work is going on! Never seen a wiki is read-only for so long.
*11:42 <taavi> I ma about to leave but -tech has a report of users seeing read-only errors
*11:42 <jynus> taavi: which wiki? en?
*11:42 <taavi> bn
*11:42 <Bsadowski1> yeah bn
*11:43 <jynus>	 that's s3
*11:43 <claime> that's not normal, we should not be changing the RO status in the live DC during the live-test
*11:49 <taavi> the timing matches with the read-only cookbook
Debugging ensues, as well as potential unrelated causes.
*12:09 <claime> cgoubert@cumin1001:/var/log/spicerack/sre/switchdc$ sudo confctl --object-type mwconfig select name=ReadOnly get
*12:09 <claime> {""ReadOnly"": {""val"": ""false""}, ""tags"": ""scope=codfw""}
*12:09 <claime> {""ReadOnly"": {""val"": false}, ""tags"": ""scope=eqiad""}
*12:13 <@taavi> why is the other false a string and the other a boolean?
The cause of read-only is confctl not setting the right type and putting a string instead of a boolean

Multiple combinations of confctl set tried:
*12:15 <claime> sudo confctl --object-type mwconfig select name=ReadOnly,scope=codfw set/val=false
*12:15 <claime> sudo confctl --object-type mwconfig select name=ReadOnly,scope=codfw set/val=False
*12:16 <claime> sudo confctl --object-type mwconfig select name=ReadOnly,scope=codfw set/val=no
*12:16 <+logmsgbot> !log akosiaris@cumin1001 conftool action : set/val=false; selector: name=ReadOnly
This last one sets eqiad read-only by the same mechanism, the variable is now a string instead of a boolean, which is interpreted by mw as being ""true""

''Eqiad is now read-only too -'' '''Major editing outage starts now'''

* 12:18 Incident opened.  '''Jaime''' becomes IC.

*12:18 <+logmsgbot> !log cgoubert@cumin1001 START - Cookbook sre.switchdc.mediawiki.07-set-readwrite
*12:18 <+logmsgbot> !log cgoubert@cumin1001 MediaWiki read-only period ends at: 2023-02-22 12:18:11.451680
*12:18 <+logmsgbot> !log cgoubert@cumin1001 END (PASS) - Cookbook sre.switchdc.mediawiki.07-set-readwrite (exit_code=0)
*12:18 <+logmsgbot> !log cgoubert@cumin1001 START - Cookbook sre.switchdc.mediawiki.07-set-readwrite
*12:18 <+logmsgbot> !log cgoubert@cumin1001 MediaWiki read-only period ends at: 2023-02-22 12:18:45.829060
*12:18 <+logmsgbot> !log cgoubert@cumin1001 END (PASS) - Cookbook sre.switchdc.mediawiki.07-set-readwrite (exit_code=0) '''- Outage stops now'''
Using the sre.switchdc.mediawiki.07-set-readwrite cookbook to set the right value type, running it once with codfw -> eqiad and once with eqiad -> codfw to set them both.
*''Both codfw and eqiad are now back to readwrite status''
*12:22 - 12:26: Double checking with users the issue is gone
*12:39 Issue declared as resolved","===What went well?===

* Test running gave an early heads up to people on call in case something went wrong/monitoring happened
* Several volunteers quickly and effectively rised issues on #wikimedia-tech, and collaborated to help resolve the issue, specially when error rate was low
* While there were not necessary in this scenario, there are multiple layers preventing a split-brain between datacenters (writes happening on two datacenters at the time, independently)","* {{bug|T330300}}: sre.switchdc.mediawiki.07-set-readwrite doesn't reset both datacenter to rw {{done}}
* Stricter conftool data type validation?
* Uniformize mobile and desktop behaviour when in read only?
* {{bug|T330304}}: Globalize mwconfig ReadOnly (would avoid unpredictable behaviour when one DC is RO and not the other)",2023-02-22,Unknown,Unknown,Mediawiki,Unknown,Unknown,2023-02-22_read_only.wikitext
"{{Incident scorecard
| task = 
| paged-num = 2
| responders-num = 2
| coordinators = Filippo Giunchedi
| start = 2023-02-22 9:18
| end = 2023-02-22 9:45
| impact = For approximately 18 minutes, around 17% of incoming, non-multimedia Wikimedia traffic received a 503, 500 error or were missing (most requests coming from eqiad and esams geolocated clients and using our cache layer: wikis, Phabricator, Grafana, ...)
}}

During a routine maintenance consisting of upgrading HAProxy on cache hosts, all of the backends (ATS) in the text cache cluster in esams and eqiad were accidentally depooled due to a mismatch on the maintenance run between depooling the hosts individually and pooling back the cdn. This caused both cached and uncached traffic requests for wikis and other ATS-backed services to fail and return errors to clients, mostly in parts of Europe, Africa and Asia. Approximately 17 million HTTP requests (according to varnish) / 5 million user requests (according to NEL estimation) errored out in total. Editing rate was reduced to less than half. Upload cluster, clients geolocated to drmrs, codfw, ulsfo or eqsin, and GET requests cached in memory were not affected.

{{TOC|align=right}}",,"Automated alerts / pages fired (FrontendUnavailable)

* FrontendUnavailable cache_text ()
* FrontendUnavailable (varnish-text)
* [5x] ProbeDown (probes/service eqiad)","08:57 vgutierrez@cumin1001:~$ sudo -i cumin -b1 -s60 'A:cp-text_esams' ''''depool''' && sleep 30 && DEBIAN_FRONTEND=noninteractive apt-get -q -y --assume-no -o DPkg::Options::=""--force-confdef"" install haproxy && run-puppet-agent -q && systemctl restart haproxy && sleep 5 && '''pool cdn'''<nowiki/>' ''# note the mismatch between depool and pool cdn''

09:18 Esams fully depooled '''Outage starts here'''

09:20 pages start rolling in

09:24: <hashar> I am going to rollback to rule out the train

09:25 Updated <nowiki>https://www.wikimediastatus.net/</nowiki>

09:30  <vgutierrez> esams isn't able to reach appservers-ro or api-ro for some reason

09:31 <hashar> (train rolled back)

09:34 <logmsgbot> !log vgutierrez@puppetmaster1001 conftool action : set/pooled=yes; selector: dc=esams,service=ats-be,cluster=cache_text

<logmsgbot> !log vgutierrez@puppetmaster1001 conftool action : set/pooled=yes; selector: dc=eqiad,service=ats-be,cluster=cache_text

09:36 '''Outage stops here'''

09:35  <vgutierrez> I basically depooled ats-be in eqiad and esams by accident

09:45 Incident declared resolved

09:48 Updated status page","===What went well?===

* Automated alerts fired as expected
* Oncall was engaged quickly, other folks joined the investigation too
* A train deployment was suspected as the cause and quickly rolled back","* Update tunnelencabulator, some SREs had trouble accessing graphs during the outage
** <nowiki>https://github.com/cdanis/tunnelencabulator/pull/6</nowiki>
* [[phab:T330272|T330272]] Provide a cookbook to perform HAProxy upgrades on CDN nodes
* [[phab:T330405|T330405]] Improve FrontendUnavailable alerts with more information/context of what's failing",2023-02-22,Unknown,Unknown,Unknown,Unknown,Unknown,2023-02-22_wiki_outage.wikitext
"{{Incident scorecard
| task = T330422
| paged-num = 6 ([https://portal.victorops.com/ui/wikimedia/incident/3435/details VictorOps incident 1], [https://portal.victorops.com/ui/wikimedia/incident/3436/details VictorOps incident 2])
| responders-num = 4
| coordinators = [[User:BCornwall|Brett Cornwall]]
| start = 17:17
| end = 17:42
| impact = Minimal user-facing errors served
}}

db1127 was too busy to respond even to the simple statistics queries that the mysql prometheus exporter runs. The MariaDB query killer has a bug in 10.6.10 where queries were not properly killed. This was fixed in 10.6.12. db1127 was one of the remaining hosts to still be on 10.6.10.

{{TOC|align=right}}",,"Automated alert via [[VictorOps]]:

<pre>
Critical: PHPFPMTooBusy api_appserver (php7.4-fpm.service eqiad)

https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=54&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=eqiad%20prometheus/ops&var-cluster=api_appserver The MediaWiki cluster api_appserver in eqiad is experiencing saturation of php7.4-fpm.service workers 9.851% https://bit.ly/wmf-fpmsat Not enough idle php7.4-fpm.service workers for Mediawiki api_appserver at eqiad #page
Alerts Firing:
Labels:
 - alertname = PHPFPMTooBusy
 - cluster = api_appserver
 - prometheus = ops
 - service = php7.4-fpm.service
 - severity = page
 - site = eqiad
 - source = prometheus
 - team = sre
Annotations:
 - dashboard = https://grafana.wikimedia.org/d/RIA1lzDZk/application-servers-red-dashboard?panelId=54&fullscreen&orgId=1&from=now-3h&to=now&var-datasource=eqiad%20prometheus/ops&var-cluster=api_appserver
 - description = The MediaWiki cluster api_appserver in eqiad is experiencing saturation of php7.4-fpm.service workers 9.851%
 - runbook = https://bit.ly/wmf-fpmsat
 - summary = Not enough idle php7.4-fpm.service workers for Mediawiki api_appserver at eqiad #page
Source: https://prometheus-eqiad.wikimedia.org/ops/graph?g0.expr=sum+by%28cluster%2C+service%29+%28phpfpm_statustext_processes%7Bcluster%3D~%22%28api_appserver%7Cappserver%7Cparsoid%29%22%2Cstate%3D%22idle%22%7D%29+%2F+sum+by%28cluster%2C+service%29+%28phpfpm_statustext_processes%7Bcluster%3D~%22%28api_appserver%7Cappserver%7Cparsoid%29%22%7D%29+%3C%3D+0.3&g0.tab=1
</pre>","[https://sal.toolforge.org/production?p=2&q=&d=2023-02-23 SAL log]

''All times in UTC.''

* 17:17 - Splunk on-call page
* 17:17 - brett acknowledges
* 17:23 - Page automatically resolved by SYSTEM
* 17:24 - Second page fires
* 17:24 - brett acknowledges
* 17:25 - DBs identified as culprit (<code><hnowlan> seems all s7</code>)
* 17:26 - <code><cdanis> […] we're not [serving many errors] *yet*, but we need to figure out what's causing this and get it to stop</code>
* 17:27 - Brett becomes IC
* 17:29 - <code><cdanis> […] looks like db1127 is a bit out to lunch</code>
* 17:31 - cdanis lowers db1127 weight from 400 to 200 via https://wikitech.wikimedia.org/wiki/Dbctl#Changing_weights_for_a_host (<+logmsgbot> !log cdanis@cumin1001 dbctl commit (dc=all): 'db1127 running very hot', diff saved to https://phabricator.wikimedia.org/P44752 and previous config saved to /var/cache/conftool/dbconfig/20230223-173127-cdanis.json)
* 17:32 - <code><cdanis> if it doesn't improve shortly I'll depool it entirely</code>
* 17:33 - <code><cdanis> […] I think it's mostly under control probably</code>
* 17:36 - cdanis depools db1127 entirely (<code><+logmsgbot> !log cdanis@cumin1001 dbctl commit (dc=all): 'so hot right now', diff saved to https://phabricator.wikimedia.org/P44753 and previous config saved to /var/cache/conftool/dbconfig/20230223-173608-cdanis.json</code>)
* 17:42 - Errors drop to pre-incident rates","===What went well?===

* Detection of the issue
* Identification of the issue
* Remedy of the issue before it became a larger problem",* Report this issue upstream ([https://jira.mariadb.org/browse/MDEV-30760 MDEV-30760]) {{done}},2023-02-23,Unknown,Unknown,Mediawiki,Unknown,Unknown,2023-02-23_PHP_worker_threads_exhaustion.wikitext
"{{Incident scorecard
| task = T329931
| paged-num = 0
| responders-num = 
| coordinators = 
| start = 2022-02-28 00:04:00
| end = 2022-02-28 02:20:00
| impact = Data loss on GitLab production host for one and a half hours, reduced availability for 20 minutes
}}

<!-- Reminder: No private information on this page! -->GitLab was switched to the other data center as a planned maintenance in [[phab:T330717|T329931]]. During the switchover some configuration had to be changed depending on the instance state (production or replica). The daily restore job was not disabled on the new production host in codfw, resulting in a backup being restored on the production host. So all actions between the backup (Feb 28th 00:04:00) and the restore (Feb 28th 02:00:00) are lost.

{{TOC|align=right}}",,"The issue was detected by automatic monitoring and task creation (probeDown)  in https://phabricator.wikimedia.org/T330717<syntaxhighlight lang=""bash"">

    alertname: ProbeDown
    instance: gitlab2002:443
    job: probes/custom
    prometheus: ops
    severity: task
    site: codfw
    source: prometheus
    team: serviceops-collab

</syntaxhighlight>","''All times in UTC.''

*27 Feb 10:00 gitlab1004 is switched over to gitlab2002 as part of the planned maintenance
*27 Feb 12:00 maintenance is over
*28 Feb 00:04 backup is triggered on gitlab2002 '''- Incident begins'''
*28 Feb 02:00 restore is triggered on gitlab2002
*28 Feb 02:03 monitoring ""probe down"" for production host: [[phab:T330717|T330717]]
*28 Feb 02:20: restore is finished - '''Incident ends'''
<!-- Reminder: No private information on this page! -->","===What went well?===

* automatic alerting caught the restore on the production host (probe down)
* production instance was working normally after the restore (beside data loss)
* rsync server was disabled on the production host, preventing restore of replica-backups (which would cause more data loss)","* Enable and disable restore automatically depending on instance status (https://gerrit.wikimedia.org/r/c/operations/puppet/+/892892) - done
*Automate failover/switchover using a cookbook - [[phab:T330771|T330771]]
*Add check of timers after a failover/switchover (manual or automated)?
*Add safeguard to restore script for production/active host - [[phab:T331295|T331295]]",2023-02-28,Unknown,Monitoring,Unknown,Unknown,Unknown,2023-02-28_GitLab_data_loss.wikitext
"{{Incident scorecard
| task = T331626
| paged-num = 0
| responders-num = 6?
| coordinators = n/a
| start = 2023-03-07 14:35:31
| end = 2023-03-09 16:02:59
| metrics = No relevant SLOs exist
| impact = For 2 days, Mailman did not deliver any mail, affecting public and private community, affiliate, and WMF mailing lists
}}

<!-- Reminder: No private information on this page! -->During network maintenance, the Mailman runner process for delivering emails out of the queue crashed because it couldn't connect to the MariaDB database server and was not automatically restarted. As a result, Mailman continued to accept and process incoming email, but outgoing mail was queued. This was first reported in [[phab:T331626|T331626]], via [[mw:Git/Reviewers|Gerrit Reviewer Bot]] being broken. It was determined that the mediawiki-commits list was not delivering mail, leading to discovery of a growing backlog of 4k queued outgoing emails in Mailman. The <code>mailman3</code> systemd service was restarted, causing all of the individual runner processes to be restarted, including the ""out"" runner, which began delivering the backlog. It took slightly over 5 hours for the backlog to be cleared.

The network maintenance in question was [[phab:T329073|T329073: eqiad row A switches upgrade]]. <code>lists1001.wikimedia.org</code> (the Mailman server) was not listed on the task but it was affected and downtimed (see [[phab:T329073#8672655|T329073#8672655]]). icinga monitoring correctly detected the issue (see [[phab:T331626#8680354|T331626#8680354]]), but was not noticed by humans.",,"Automated monitoring was the first to detect the issue, less than 10 minutes after the runner crashed:

14:43: <+icinga-wm> PROBLEM - mailman3_runners on lists1001 is CRITICAL: PROCS CRITICAL: 13 processes with UID = 38 (list), regex args /usr/lib/mailman3/bin/runner <nowiki>https://wikitech.wikimedia.org/wiki/Mailman/Monitoring</nowiki>

The correct alert fired, as it explicitly checks that the expected number of runner processes are actually running.

However, it was not investigated until a human reported it, nearly 2 days later.","''All times in UTC.''

*2023-03-07
**14:09: lists1001 and 237 other hosts downtimed for switches upgrade ([[phab:T329073#8672655|T329073#8672655]])
**14:20ish: network maintenance happens
**14:35: ""out"" runner crashes '''OUTAGE BEGINS''' (see [[phab:T331626#8681134|stack trace]])
**14:43: <+icinga-wm> PROBLEM - mailman3_runners on lists1001 is CRITICAL: PROCS CRITICAL: 13 processes with UID = 38 (list), regex args /usr/lib/mailman3/bin/runner <nowiki>https://wikitech.wikimedia.org/wiki/Mailman/Monitoring</nowiki>
*2023-03-09
**13:29 kostajh files [[phab:T331626|T331626: reviewer-bot is not working]]
**14:24 valhallasw says no email is coming in via the mediawiki-commits@ mailing list
**15:15 JJMC89 files [[phab:T331633|T331633: Not receiving posts or moderation messages]], ""The last message I received was at 7 Mar 2023 11:18:24 +0000, but I can see posts after that in list archives.""
**15:53 hashar posts about the large out queue backlog in Mailman: [[phab:T331626#8680273|T331626#8680273]] 
**16:03 marostegui restarts <code>mailman3</code> systemd service, the out runner begins processing the queue
**18:40 legoktm sends [[listarchive:list/listadmins@lists.wikimedia.org/message/5UYLM2TGOMF4P6KABZE6Q2BXTIR4MQ53/|notification]] to listadmins@ mailing list
**23:34 out queue reaches zero '''OUTAGE ENDS'''","===What went well?===

*Automated monitoring correctly identified the issue pretty quickly","*Identify how lists1001 got missed during the eqiad row A switch upgrade preparation
*Add monitoring to out queue size
*Consider making the runner crashed monitoring page? If a runner crashes, it definitely needs manual intervention. And crashes are much much rarer than during the initial MM3 deployment. 
*Someone should probably figure out why lists1001's web service is flaky and randomly going down

<mark>Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.</mark>

<mark>Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFIRE (Pending Review & Scorecard)]]  Phabricator tag to these tasks.</mark>",2023-03-09,Unknown,Monitoring,Unknown,Unknown,Unknown,2023-03-09_mailman.wikitext
"{{Incident scorecard
| id = eqiad/LVS
| task = T337497
| paged-num = 2
| responders-num = 7
| coordinators = Janis
| start = 2023-05-25 14:04
| end = 2023-05-25 14:25
| metrics = edits per second, rps in general, 5xx responses from CDN, appserver latency
| impact = For approximately 15-20 minutes logged in users connecting to to Wikimedia wikis through our Ashburn datacenter and editors in general may have received 503 errors
}}

…

<!-- Reminder: No private information on this page! -->During scap deploys one of the LVS servers in eqiad was taken down which resulted in multiple servers being depooled making eqiad unable so serve traffic.

{{TOC|align=right}}",,The issue was detected immediately by SRE doing maintenance work. Multiple alerts as well as pages immediately followed.,"''All times in UTC.''

*Scap runs at 13:52, 14:08, 14:10
*14:04 PyBal disabled on lvs1019 as part of maintenance but a deploy was ongoing '''OUTAGE BEGINS'''
*14:05 PROBLEM - PyBal backends health check on lvs1019 is CRITICAL: PYBAL CRITICAL - Bad Response from pybal: 500 Cant connect to localhost:9090 (Connection refused) <nowiki>https://wikitech.wikimedia.org/wiki/PyBal</nowiki>
*14:08 <icinga-wm> PROBLEM - PyBal backends health check on lvs1020 is CRITICAL: PYBAL CRITICAL - CRITICAL - appservers-https_443: Servers mw1418.eqiad.wmnet, mw1417.eqiad.wmnet, mw1416.eqiad.wmnet, mw1415.eqiad.wmnet, mw1414.eqiad.wmnet are marked down but pooled: parsoid-php_443: Servers parse1017.eqiad.wmnet, parse1011.eqiad.wmnet are marked down but pooled: api-https_443: Servers mw1447.eqiad.wmnet, mw1448.eqiad.wmnet, mw1449.eqiad.wmnet, mw1450.eqiad.wmnet  marked down but pooled <nowiki>https://wikitech.wikimedia.org/wiki/PyBal</nowiki>
*14:09: PyBal re-started on lvs1019
*14:14 bblack started repooling parsoid in
*14:17  Incident opened.  '''Janis''' becomes IC.
*14:21 appservers and api_appservers repooled
*14:22 jobrunners and videoscalers repooled
*14:25 recoveries coming in '''OUTAGE ENDS'''
*14:42 still elevated latencies on api_appservers, might be unrelated as there is a spike of read traffic on s7
*14:58 s7 query throughput is high due to db maintenance, most likely unrelated
*14:58 api_appserver latency trending down
*15:08] <jinxer-wm> (MediaWikiLatencyExceeded) resolved: Average latency high: ...
<!-- Reminder: No private information on this page! -->","===What went well?===

* Automated alerting detected the issue quickly",* https://phabricator.wikimedia.org/T334703,2023-04-17,Unknown,Unknown,Unknown,Unknown,Unknown,2023-04-17 eqiad LVS.wikitext
"{{Incident scorecard
| task = T333347
| paged-num = 0
| responders-num = 4
| coordinators = Eoghan, Jelto
| start = 2023-05-02 08:14
| end = 2023-05-02 08:24
| impact = For approximately 10 minutes Phabricator tasks were not loading when GitLab was in maintenance mode.
}}


During a planned GitLab maintenance to switch the main GitLab host from codfw to eqiad Phabricator users noticed that they were not able to load tasks, instead seeing an """"Unhandled Exception (""RuntimeException"")"" message. 

The following errors were observed in the logs:
<syntaxhighlight lang=""text"">
Invalid argument supplied for foreach()` `called at [<wmf-ext-misc>/src/customfields/GitLabPatchesCustomField.php:113]
</syntaxhighlight>

<syntaxhighlight lang=""text"">
fatal: unable to access 'https://gitlab.wikimedia.org/toolforge-repos/toolpilot.git/': The requested URL returned error: 502
</syntaxhighlight>


Based on the above findings a decision was made to abandon the planned maintenance and take GitLab out of maintenance mode. This resulted in Phabricator tasks loading as expected, other than a short caching period for already open tasks.


The most likely cause of the incident was a Phabricator widget deployed in [[phab:T324149|T324149]]{{TOC|align=right}}",,"The issue was brought to our attention by users in the #wikimedia-operations IRC channel. There were no automated alerts as the Phabricator service was up and pages were loading, but displaying an error instead of expected contents","''All times in UTC.''

*2023-03-15 21:36: GitLab Phabricator widget is deployed [[phab:T324149#8700128|T324149#8700128]]
*2023-05-02 08:14 '''OUTAGE BEGINS:''' first notification of Phabricator outage on IRC
*2023-05-02 08:20 Decision is made to stop the GitLab maintenance, GitLab is restarted to bring up all the services
*2023-05-02 08:24 '''OUTAGE ENDS:''' confirmation that service has been restored
*2023-05-04 17:12 Fix is deployed for Phabricator widget [[phab:T333347#8827402|T333347#8827402]]","===What went well?===

* Stoping the maintenance and restoring the service was quick and straightforward
* Fix was found and deployed fast","* [[phab:T333347|T333347]] will be used to troublehoot the widget behavior.
* Maybe create blackbox checks for certain tasks? (like https://phabricator.wikimedia.org/T1)",2023-05-02,Unknown,Unknown,Unknown,Unknown,Unknown,2023-05-02_Phabricator_outage_during_GitLab_maintenance.wikitext
"{{Incident scorecard
| task = T335406
| paged-num = 0
| responders-num = 1
| coordinators = Filippo Giunchedi, Andrea Denisse
| start = 2023-05-05 00:04:00
| end = 2023-05-05 08:15:00
| impact = Prometheus was down in ulsfo and eqsin for 8 hours
}}",,Automated monitoring detected the alert but a human noticed the outage and triaged it with the alert.,"''All times in UTC.''

Step by step outline of what happened:

'''May 2, 2023'''

'''16:00''' Data is synchronized from prometheus4001 to prometheus4002 for a Bullseye upgrade

'''16:43''' Failover DNS from prometheus5001 to prometheus5002 in ulsfo [Patch #913194] 

'''21:00''' prometheus4002 prometheus@ops[2098503]: level=error ts=2023-05-02T21:00:04.686Z caller=db.go:745 component=tsdb msg=""compaction failed"" err=""WAL truncation in Compact: get segment range: segments are not sequential

'''May 4, 2023'''

'''23:00''' Data is synchronized from prometheus5001 to prometheus5002 for a Bullseye upgrade

'''May 5, 2023'''

'''00:27 Outage starts:''' Failover DNS from prometheus5001 to prometheus5002 in eqsin [Patch #913196]

'''01:32''' [https://sal.toolforge.org/log/KemJ6YcBGiVuUzOdxAE0 denisse@cumin1001: END (PASS) - Cookbook sre.ganeti.reboot-vm (exit_code=0) for VM prometheus4002.ulsfo.wmnet]

'''01:39''' [https://sal.toolforge.org/log/O0eQ6YcBxE1_1c7sEyUW denisse@cumin1001: END (PASS) - Cookbook sre.ganeti.reboot-vm (exit_code=0) for VM prometheus5002.eqsin.wmnet]

'''08:15''' godog: delete wal and chunks_head from prometheus5002 and prometheus4002 to let prometheus start back up and not crashloop 

'''08:15 Outage ends'''

[[File:Datacenter-overview-ulsfo.png|thumb|590x590px|Datacenter overview of ulsfo showing the loss of visibility during the time Prometheus was down https://grafana.wikimedia.org/goto/IEjpsQy4k]]
[[File:Datacenter-overview-eqsin.png|thumb|592x592px|Datacenter overview of eqsin showing the loss of visibility during the time Prometheus was down https://grafana.wikimedia.org/goto/K4NNywy4k]]","To prevent similar incidents from happening in the future, the team reviewed their upgrade and alerting procedures to ensure that all necessary checks and tests are performed before updates are applied in production.","* ThanosCompactHalted error on overlapping blocks. Find and nuke the non-aligned blocks. [[phab:T335406|T335406]]
* Ensure that the replica label is set for all Prometheus hosts. Make puppet fail when replica=unset. [[phab:T335406|T335406]]
* Alert when no data is received from Prometheus in a certain amount of time. [[phab:T336448|T336448]]
* Update the migration procedure on Wikitech. [[phab:T309979|T309979]]",2023-05-05,Unknown,Monitoring,Unknown,Unknown,Unknown,2023-05-05_prometheus_down_in_ulsfo_and_eqsin.wikitext
"{{Incident scorecard
| task = T336134
| paged-num = 0
| responders-num = 3
| coordinators = None
| start = 2023-05-04T10:00
| end = 2023-05-10T10:30
| metrics = WDQS update lag
| impact = End users accessing WDQS from the CODFW region received stale results.
}}

…

<!-- Reminder: No private information on this page! -->The rdf-streaming-updater application in CODFW became unstable and stopped sending updates, resulting in stale data for users connecting through CODFW.

{{TOC|align=right}}",,"Prometheus alerts for the WCQS cluster fired starting at <code>2023-05-04T1030</code> . Alerts were dispatched via email, with subject <code>RdfStreamingUpdaterFlinkJobUnstable .</code>

WDQS cluster alerts started a bit later, at <code>2023-05-05T1908.</code>

In addition to the above subject,  WDQS alerts also included the subject  <code>RdfStreamingUpdaterHighConsumerUpdateLag.</code>

The alerts correctly identified the problem and [[labsconsole:Wikidata_Query_Service/Streaming_Updater|linked to the appropriate documentation]].","* <code>2023-05-04T10:00</code>: the streaming updater flink job stopped to function in codfw for both WDQS and WCQS
** user impact starts: stale results are seen when using WDQS from a region that hits CODFW
** reason is likely <nowiki>https://issues.apache.org/jira/browse/FLINK-22597</nowiki>
* <code>2023-05-05T16:22</code>: the problem is reported by Bovlb via <nowiki>https://www.wikidata.org/wiki/Wikidata:Report_a_technical_problem/WDQS_and_Search</nowiki>
* <code>2023-05-05T19:00</code>: the flink jobmanager container is manually restarted and the jobs resume but the WDQS one is very unstable (k8s is heavily throttling cpu usage and taskmanager mem usage grows quickly)
** (assumptions) because the job was backfilling 1day of data it required more resources than usual, though this is not the first time that a backfill happens (e.g. k8s cluster upgrades went well)
** (assumptions) because the job was resource constrained rocksdb resource compaction did not happen in a timely manner
* <code>2023-05-05T21:00</code>: the job fails again
* <code>2023-05-06T10:00</code>: the job resumes (unknown reasons)
* <code>2023-05-06T19:00</code>: the job fails again
** Seeing jvm OutOfMemoryError
** The checkpoint it tries to recover from is abnormally large (6G instead of 1.5G usually), assumption is that rocksdb compaction did not occur properly
* <code>2023-05-07T17:27</code>: this ticket is created as UBN
* <code>2023-05-08T16:00</code>: wdqs in CODFW is depooled
** user impact ends
* <code>2023-05-09T14:00</code>: increasing taskmanager memory from 1.9G to 2.5G did not help
* <code>2023-05-09T14:00</code>: starting the job from yarn using across 12 containers with 5G did help
** the job recovered and started to produce reasonable checkpoint sizes
* <code>2023-05-10T00:00</code>: lag is back to normal on all wdqs servers
* <code>2023-05-10T10:30</code>: the job is resumed from k8s@codfw","===What went well?===

* The community recognized and alerted us to the issue.","* [[phab:T336577|Update WDQS Runbook following update lag incident]]
* [[phab:T336574|Review alerting around Wikidata Query Service update pipeline]]
* [[phab:T337801|WDQS: Document procedure for switching between Kubernetes and Yarn Streaming Updater]]",2023-05-05,Unknown,Unknown,Wikidata,Unknown,Unknown,2023-05-05_wdqs_not_updating_in_codfw.wikitext
"{{Incident scorecard
| task = T279100
| paged-num = 2
| responders-num = 1
| coordinators = Dzahn
| start = 2023-05-19 19:04:00
| end = 2023-05-19 19:49:00
| metrics = No relevant SLOs exist
| impact = The user who uploaded those videos has to wait a bit longer to get different formats. Possibly other users waited a bit longer for other jobs.
}}

<!-- Reminder: No private information on this page! -->A large video-scaling job made server mw1469 so busy that its capacity was maxed out by ffmpeg processes. Since mw1469 was both a jobrunner and a videoscaler this led to alerts for both jobrunner and videoscaler services.

For the first part of the incident alerts could be seen on IRC but there had been no pages yet. Alerts were flapping, also on mw1469 specifically. Around 19:24 it eventually triggered a page. Dzahn and Aokoth were paged and

started looking at it and kept an eye on it for a while. Since it kept flapping the runbook was followed ([[Application servers/Runbook#Jobrunners]].) and mw1469 was depooled from videoscaler, but pooled in jobrunner. The ffmpeg processes on mw1469 were killed. This protected the jobrunner which is much more important than the videoscaling (quoting runbook). Jobrunner alerts recovered. 

A little while later, server mw1495 was depooled from jobrunner and turned into a dedicated videoscaler. Videoscaler alerts recovered.

{{TOC|align=right}}",,"First Icinga started reporting via icinga-wm on IRC, a little later SRE on duty got paged via Alertmanager.","<mark>Write a step by step outline of what happened to cause the incident, and how it was remedied.  Include the lead-up to the incident, and any epilogue.</mark>

<mark>Consider including a graphs of the error rate or other surrogate.</mark>

<mark>Link to a specific offset in SAL using the SAL tool at https://sal.toolforge.org/ ([https://sal.toolforge.org/production?q=synchronized&d=2012-01-01 example])</mark>

''All times in UTC.''

*19:04 flapping of IRC alerts begins
*19:24 page is sent
*19:25 Dzahn ACKs alert, starts investigating, watches the situation
*19:45 Since alerts are still flapping, Dzahn depools mw1469 from videoscaler, to protect jobrunner
*19:46 Dzahn kills ffmpeg processes on mw1469 (as instructed per runbook)
*19:47 alerts recover on mw1469
*20:23 alerts start on mw1495
*20:52 mw1495 is depooled from jobrunner, made dedicated videoscaler, so it can finish mmpeg processes eventually
*20:53 alerts for videoscaler recover
<!-- Reminder: No private information on this page! -->
<mark>TODO: Clearly indicate when the user-visible outage began and ended.</mark>",<mark>OPTIONAL: General conclusions (bullet points or narrative)</mark>,"* reopen https://phabricator.wikimedia.org/T279100 ?
* dedicated alert for videoscalers https://phabricator.wikimedia.org/T338220",2023-05-19,Unknown,Unknown,Unknown,Unknown,Unknown,2023-05-19 videoscaler jobrunner.wikitext
"{{Incident scorecard
| task = T337327
| paged-num = 
| responders-num = 5
| coordinators = Brian King
| start = 2023-05-23 10:48:00
| end = 2023-05-23 19:24:00
| metrics = WDQS SLO
| impact = A subset of users accessing the WDQS service from our CODFW datacenter received errors and timeouts.
}}

An expensive query sent over and over again by an external user(s) caused errors and timeouts for users accessing the WDQS service from our CODFW datacenter. A requestctl rule was put in place to mitigate the issue.

[[File:Wdqs outage 2023 05 23.jpg|thumb]]
{{TOC|align=right}}",,"<mark>Write how the issue was first detected.  Was automated monitoring first to detect it? Or a human reporting an error</mark>

Detected by monitoring

<mark>Copy the relevant alerts that fired in this section.</mark>

See above for example.

<mark>Did the appropriate alert(s) fire?</mark> 

Yes

<mark>Was the alert volume manageable?</mark> 

Yes

<mark>Did they point to the problem with as much accuracy as possible?</mark>

No. We saw increased 5xx errors and lots of time spent in old garbage collection, but it's still difficult to troubleshoot this type of abuse.","<mark>Link to a specific offset in SAL using the SAL tool at https://sal.toolforge.org/ ([https://sal.toolforge.org/production?q=synchronized&d=2012-01-01 example])</mark>

''All times in UTC''

*1047 first Icinga alerts fire. Example verbiage: - ''PyBal backends health check on lvs2010 is CRITICAL: PYBAL CRITICAL - CRITICAL - wdqs-heavy-queries_8888: Servers wdqs2009.codfw.wmnet are marked down but pooled''
*1536 Rolling restart of the WDQS service in CODFW temporarily stabilizes the service
*1630 Time spent in old garbage collection (metric that closely correlates with the outage) starts to rise again (see graph).
*1821 Requestctl rule to mitigate abuse deployed, followed by a rolling restart of the WDQS service in CODFW.  Service starts to stabilize.
*1924 Old GC is back down to the levels of the non-affected datacenter (eqiad). Incident closed.","===What went well?===

* Alerts fired promptly 
* Other SREs provided lots of help.","* …

<mark>Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.</mark>

<mark>Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFire]]  Phabricator tag to these tasks.</mark>",2023-05-23,Unknown,Monitoring,Unknown,Unknown,Unknown,2023-05-23_wdqs_CODFW_5xx_errors.wikitext
"{{Incident scorecard
| task = T337446
| paged-num = 0
| responders-num = 3
| coordinators = none
| start = 2023-05-25
| end = 2023-06-01
| metrics = WMCS services (bots and tools) were affected, mostly used by the community. No production services affected.
| impact = [https://wikitech.wikimedia.org/wiki/Help:Toolforge/Database Wikireplicas] had outdated data and were unavailable for around one week. There were periods where not even old data was available. Tools most likely experienced intermittent unavailability or outdated data from 2023-05-25 to 2023-06-01. Replication of multiple sections (s1, s2, s5, s7) broke most probably due to a bug in mariadb version 10.4.29. A version downgrade to 10.4.26 was made and all hosts where recloned.
}}",,"Replication lag was detected first by Icinga alerts. [[phab:T337446|Phabricator task]] was created and triaged.<syntaxhighlight lang=""text"">
2023-05-25 05:11:16 <icinga-wm> PROBLEM - MariaDB Replica SQL: s1 on db1154 is CRITICAL: CRITICAL slave_sql_state Slave_SQL_Running: No, Errno: 1032, Errmsg: Could not execute Delete_rows_v1 event on table enwiki.user_properties: Cant find record in user_properties, Error_code: 1032: handler error HA_ERR_KEY_NOT_FOUND: the events master log db1196-bin.001099, end_log_pos 654625806 https://wikitech.wikimedia.org/wiki/MariaDB/troubleshooting%23Depooling_a_replica
2023-05-25 05:11:44 <icinga-wm> PROBLEM - MariaDB Replica SQL: s2 on db1155 is CRITICAL: CRITICAL slave_sql_state Slave_SQL_Running: No, Errno: 1032, Errmsg: Could not execute Delete_rows_v1 event on table plwiki.user_properties: Cant find record in user_properties, Error_code: 1032: handler error HA_ERR_KEY_NOT_FOUND: the events master log db1156-bin.003729, end_log_pos 633898246 https://wikitech.wikimedia.org/wiki/MariaDB/troubleshooting%23Depooling_a_replica

</syntaxhighlight>

Broken replication on sanitarium host:<syntaxhighlight lang=""text"">
Broken replication on sanitarium host
PROBLEM - MariaDB Replica SQL: s5 on db1154 is CRITICAL: CRITICAL slave_sql_state Slave_SQL_Running: No, Errno: 1032, Errmsg: Could not execute Delete_rows_v1 event on table dewiki.flaggedpage_pending: Cant find record in flaggedpage_pending, Error_code: 1032: handler error HA_ERR_KEY_NOT_FOUND: the events master log db1161-bin.001646, end_log_pos 385492288

</syntaxhighlight>MariaDB crashing when inserting the missing row example log:","''All times in UTC.''

'''Outage begins'''

*2023-05-25 05:11: Icinga alert fires: ''<icinga-wm> PROBLEM - MariaDB Replica SQL: s1 on db1154 is CRITICAL,'' replication lag on db1154, db1155 and clouddb start to increase
*2023-05-25 05:11: Replication breaks on two different sanitarium hosts (db1154 and db1155): ''<icinga-wm> PROBLEM - MariaDB Replica SQL: s1 on db1154 is CRITICAL: CRITICAL slave_sql_state Slave_SQL_Running: No, Errno: 1032, Errmsg: Could not execute Delete_rows_v1 event on table enwiki.user_properties: Cant find record in user_properties, Error_code: 1032: handler error HA_ERR_KEY_NOT_FOUND: the events master log db1196-bin.001099, end_log_pos 654625806''
*2023-05-25 05:18:  Manuel starts checking and communicates so on IRC
*2023-05-25 05:35: Incident opened by alert of broken replication.  Amir opens a task on Phabricator [[phab:T337446|T337446]] and the relevant teams and individuals tagged. Ways to mitigate this were explored, such as adding some transaction to get the replication flowing back but discarded as it would compromise data integrity.
*2023-05-25 05:47: Manuel tries to fix a row in s5, host crashes
*2023-05-25 05:51: Manuel: The first attempts to manually fix the rows result in the [[phab:T337446#8878826|hosts crashing]] 
*2023-05-25 06:13: Fix row of S1 fails, host crashes
*2023-05-25 06:27: The decision to rebuild everything is made and work starts on sanitarium hosts
*2023-05-25 14:40: The initial for broken sections are done in sanitarium and start the sanitizing process.
*2023-05-26 05:10 Sanitarium host is still being worked on for s1,s2, s5 and s7
*2023-05-26 13:23 Manuel pings btullis on IRC and phabricator asking for data engineering to [[phab:T337446#8882660|prioritize the outage]]
*2023-05-26 15:08 Cookbook cookbooks.sre.wikireplicas.update-views run by nskaggs: Started updating wikireplica views
*2023-05-26 16:50 Manuel calls for help on IRC from WMCS and other people who have owned or own the replicas to run cookbooks responsible for updating views. There’s some response but nothing gets done by the end of day.
*2023-05-26 replication on s3 section also breaks
'''Replication breaks again'''
*2023-05-27 05:12 replication on s1 section breaks again
*2023-05-27 05:16 replication on s5 section breaks again
*2023-05-27 05:33 replication on s7 section breaks again
*2023-05-27 19:44 Manuel realizes that the fixed sections got broken again ([[phab:T337446#8885162|at pretty much the same time]]) 
*2023-05-28 06:10 upgrade mysql version from 10.4.26 to 10.4.28 [[phab:T337446#8885816|T337446#8885816]] Stopping sections.
*2023-05-28 06:12 SAL <marostegui>  Change innodb_fast_shutdown to 0 on db1154 before downgrading
*2023-05-29 05:08 <marostegui> I have had to kill them, it's been more than 24h waiting to stop. Going to downgrade + rebuild
*2023-05-29 05:32 recloning db1154:3311 db1154:3313 db1154:3315 db1155:3312 clouddb1021:3317
*2023-05-29 07:58 clouddb1021 (s7) has been recloned
*2023-05-29 08:08 Started to reclone 1014 and 1018
*2023-05-27 / 2023-05-28: More investigation about why the second breakage happened starts on phabricator and the decision to reclone everything again + downgrade is made (from 10.4.29 to 10.4.26)
'''Email sent to wikitech concerning outage'''
*2023-05-29 13:59 [[listarchive:list/wikitech-l@lists.wikimedia.org/thread/RPBN7F3LZJDPFFN6U2A2QWWIL57HCIS3/#WR36DST3Z6NV4YDZ4ODVPKHEK7VEGM4B|Email sent to wikitech-l about outage.]] 
*2023-05-29 14:21 s7 is fully recloned
*2023-05-29 14:35 clouddb1021:s2 is fully ready with views, grants, users etc
*2023-05-29 15:46 clouddb1021:s3 is fully ready with views, grants, users etc
*2023-05-29 16:52 And issue with dbproxy1018 and dbproxy1019 (WMCS wikireplicas proxies) and Healthcheck for unable wikireplicas overwhelm pybal, almost causing a general outage, is detected by Valentin but there is no more context on why this is happening. several extra steps have been missing on rebuilding cloud dbs documentations. Fixing that.
*2023-05-29 19:10 WMCS pinged again to help restore some missing tables, indexes and data because the Data Persistence team did not have enough context to proceed confidently.
*2023-05-29 21:59 A response is sent on another thread on the same subject to reassure users that work was ongoing to restore the replicas. Users complained about the lack of access to meta_p and heartbeat_p databases.
*2023-05-28 22:45: Replication lag in dewiki is mentioned on [[listarchive:list/wikitech-l@lists.wikimedia.org/thread/RPBN7F3LZJDPFFN6U2A2QWWIL57HCIS3/|wikitech-l]]
*2023-05-30 04:38 Manuel detects s4 has broken too and [[phab:T337446#8887483|manually fixes it]] inserting the missing row but will reclone later 
*2023-05-30 05:19 db1154 s4 fixed by inserting the missing row
*2023-05-30 06:33 s5 is fully recloned
*2023-05-30 09:21 Arturo fixes indexes/permissions (maintain-meta_p --all-databases --bootstrap) [[phab:T337446#8887809|T337446#8887809]]
*2023-05-30 09:58 clouddb1014:3312 is now catching up
*2023-05-30 09:59 https://phabricator.wikimedia.org/T337721 is created to investigate the proxy issue affecting pybal
*2023-05-30 10:43 clouddb1021:3311 (s1) is fully ready with grants, views etc. Once it has caught up I will clone the other two s1 hosts.
*2023-05-30 12:56  IdleConnection monitor is disabled is disabled via <nowiki>https://gerrit.wikimedia.org/r/c/operations/puppet/+/924342</nowiki>

*2023-05-30 14:34 Users complained that while the clouddbs are accessible and updated, they are extremely slow to query.  [[phab:T337734|T337734]] was filed and WMCS was pinged for help. It turned out that the script that’s adding indexes is broken, WMCS couldn’t figure out why. Amir started investigating, fixed and started running the script.
*2023-05-30 15:17 s2 is fully recloned
*2023-05-30 18:44 s3 is fully recloned and it is now catching up (it is 8h behind)
*2023-05-31 06:58 Manuel starts recloning s4
*2023-05-31 08:10 s4 has been fully recloned, clouddb1019:3314 is now catching up with its master
*2023-05-31 17:51:31 WMCS (Nicholas) checks in and Manuel flags some items that needed attention: still pending help with https://phabricator.wikimedia.org/T337734 and communication with users.
'''Databases are restored, but accessing data is very slow for tools'''
*2023-06-01 08:47 replication caught up on all sections [[phab:T337446#8894523|T337446#8894523]] Manuel reduces priority on task because all the broken sections had been successfully recloned.
*2023-06-01 Amir finishes creating indexes using the fixed script.
'''Outage ends'''<!-- Reminder: No private information on this page! -->","* mariadb version 10.4.29 maybe a replication bug - hard/impossible to reproduce
* wikireplicas is quite critical for the community although it's not considered a ""production"" service","* Add more documentation of wikireplicas setup
* Alerting for wikireplicas lag
* Discuss SLO for wikireplicas and level of ""production"", per Leon Ziemba
** ""I wanted to ask something I've genuinely been curious about for years -- since the wiki replicas are relied upon so heavily by the editing communities (and to some degree, readers), should we as an org treat their health with more scrutiny? This of course is insignificant compared to the production replicas going down, but nonetheless the effects were surely felt all across the movement (editathons don't have live tracking, stewards can't query for global contribs, important bots stop working, etc.). I.e. I wonder if there's any appetite to file an [[Incident status|incident report]], especially if we feel there are lessons to be learned to prevent similar future outages? I noticed other comparatively low-impact incidents have been documented, such as PAWS outages."" [https://phabricator.wikimedia.org/T337446#8898661]",2023-05-28,Unknown,Unknown,Unknown,Unknown,Unknown,2023-05-28_wikireplicas_lag.wikitext
"{{Incident scorecard
| task = 
| paged-num = 0
| responders-num = Amir
| coordinators = 
| start = 2023-05-26
| end = 2023-05-30
| impact = A patch to mediawiki-config was accidentally +2'd and not immediately deployed
}}

<!-- Reminder: No private information on this page! -->https://gerrit.wikimedia.org/r/c/operations/mediawiki-config/+/923650 was accidentally +2'd without intent to deploy (like on a software repo rather than a config repo). This could have led to confusion for a future deployer.

{{TOC|align=right}}",,"==Conclusions ==

<mark>OPTIONAL: General conclusions (bullet points or narrative)</mark>","''All times in UTC.''

*05-30 10:00 https://gerrit.wikimedia.org/r/c/operations/mediawiki-config/+/923650 is +2'd
*05-30 10:01 patch was reverted by an observer.

<!-- Reminder: No private information on this page! -->==Detection==",<mark>OPTIONAL: General conclusions (bullet points or narrative)</mark>,"* …

<mark>Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.</mark>

<mark>Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFire]]  Phabricator tag to these tasks.</mark>",2023-05-30,Unknown,Unknown,Unknown,Unknown,Unknown,2023-05-30_Unintentional_%2B2_on_a_config_patch_without_deployment.wikitext
"{{Incident scorecard
| id = dawiktionary/dawiki/svwiktionary partial outage
| task = T338193
| paged-num = 14
| responders-num = 4
| coordinators = cwhite
| start = 2023-06-06 00:30:00
| end = 2023-06-06 03:49:00
| impact = All users of dawiktionary, dawiki, and svwiktionary sometimes received fatal exceptions while browsing.
}}

All users of dawiktionary, dawiki, and svwiktionary sometimes received fatal exceptions while browsing.  Logs indicated MediaWiki was encountering a data  issue which also entered the object cache.  The source of the problem was traced back to a data migration implementation bug in how bash interprets double-quotes ("") characters.

{{TOC|align=right}}",,The issue was noticed by a volunteer and reported after business hours via IRC.  Another volunteer saw the IRC message and escalated to SRE via Klaxon.,"''All times in UTC.''

*2023-05-31 15:45 low level of 500 errors on dawiktionary - data encoding migration begins: {{PhabT|128155}}
*2023-06-06 00:30 errors dramatically increase - '''OUTAGE BEGINS'''
*2023-06-06 01:35 UBN {{PhabT|338193}} filed
*2023-06-06 02:36 Problem escalated to SRE via Klaxon - investigation begins
*2023-06-06 03:36 Investigation yields possible data issue - Escalated to DBA running the encoding migration
*2023-06-06 03:49 Responding DBA detects and implements the fix - Client-facing errors stop - '''OUTAGE ENDS'''","===What went well?===

* Klaxon notified SRE and a number of engineers responded quickly.
* VO made it easy to escalate the issue to another engineer.
* Once a DBA arrived, the issue was fixed quickly and the root cause identified.
* We intentionally started with small wikis on legacy encoding. Dutch Wikipedia and English Wikipedia could have had the outage instead.",* TODO,2023-06-06,Unknown,Unknown,Unknown,Unknown,Unknown,2023-06-06 dawiktionary dawiki svwiktionary partial outage.wikitext
"[[File:CirrusSearch reject failures 2023-06-18.png|thumb|Number of rejections per minute during the outage]]
{{Incident scorecard
| id = 2023-06-18 search broken on wikidata and commons
| task = T339810
| paged-num = 2
| responders-num = 3
| coordinators = David Causse, Antoine Musso
| start = 2023-06-17 11:30:00
| end = 2023-06-18 10:02:00
| metrics = No relevant SLO exists, https://grafana.wikimedia.org/d/000000455/elasticsearch-percentiles?orgId=1&from=1686984075233&to=1687091044344&viewPanel=9
| impact = Search broken on wikidata and commons
}}

…

As part of the work done in [[phab:T334194|T334194]] a reindex of the Elasticsearch indices for Wikibase enabled wikis (wikidata and commons) was scheduled. Reindexing is a routine task the search teams uses to enable new settings at the index level (generally to tune how languages are processed). For this particular task the reason was to optimize the number of analyzers created on these wikis by de-duplicating them (300+ languages). De-duplicating analyzers means that any code referring to a particular analyzer might now possibly reference one that was de-duplicated and thus non-existent. The Search Team analyzed such cases and found nothing problematic scanning the code-base. This was untrue, after the Wikidata reindex was done when the new index was automatically promoted to production the fulltext search queries started to fail. Reason is that the ''token_count_router'' query was still referencing the ''text_search'' analyzer directly which was now nonexistent because de-duplicated. The ''token_count_router'' is a feature that counts the number of token in a query to help not run costly phrase queries on queries that has too many tokens.

Mitigations that were evaluated:

* disabling the ''token_count_router'' could have fixed the immediate problem but could have put the whole cluster under the risk of being overloaded by such pathological queries.
* reverting the initial feature was not possible since it requires a full re-index of the wiki (long procedure, 10+hours)
* adding the ''text_search'' analyzer manually on the wikidata and commons indices could have fixed the issue but required closing the index which is a heavy maintenance task (search traffic switching).
* fix the ''token_count_router'' to not reference the ''text_search'' analyzer directly, one-liner. This approach was preferred.

{{TOC|align=right}}",,The problem was detected by users and then raised on IRC via the #mediawiki_security channel.,"''Friday, June 16:''

*21:40 The CirrusSearch reindex procedure is started on all Wikibase related wikis [[phab:T334194|T334194]]
Saturday, June 17:
*11:30 '''OUTAGE STARTS''' The number of CirrusSearch failures starts to rise. Users start to report the problem [[Wikidata:Wikidata:Project_chat#Search_broken]], [[phab:T339810|T339810]]. Users can no longer search on wikidata and commons.
*22:07 End user Snowmanonahoe files [[phab:T339810|T339810]].
Sunday, June 18
*05:39 Legoktm in #mediawiki_security IRC channel: search is apparently broken on both Commons and Wikidata? T339811 & T339810
*06:37 Hashar casually browse IRC, become aware of the problem and start investigating ""sounds like a Sunday Unbreak Now"".
*06:37 - 9:00 Hashar does a first pass investigation (logs on T339810) and concludes: MediaWiki errors dashboard barely shows anything beside a few errors to Special:MediaSearch. looks like the ElasticSearch index `wikidatawiki_content` is borked: <code>All shards failed for phase: [query]</code> <code>[Unknown analyzer [text_search]]; nested: IllegalArgumentException[Unknown analyzer [text_search]];</code>  <code>Caused by: java.lang.IllegalArgumentException: Unknown analyzer [text_search]</code>
*07:00 Hashar explicitly skips calling SRE on call and call directly the search team members in Europe: Gehel and dcausse 
*<sync delay to reach people, reach out a computer etc)
*08:00 dcausse says that a revert is not possible (would require a re-index of the wikidata and commons index), a one-liner fix is preferred
*8:15 patches proposed to Gerrit
*<delay due to IRL things>
*9:20 Hashar and Dcausse jump in a video call to synchronize the backport and deployment of fixes
*9:29 fixes pulled on mwdebug1001 and are tested there
*9:50 WikibaseCirrusSearch fix is deployed. The error traffic is dramatically reduced.
*10:02 CirrusSearch fix is deployed. The few remaining errors (Special:Mediasearch) vanishes.
*10:02 '''OUTAGE ENDS''' A fix is deployed to the production wikis

<!-- Reminder: No private information on this page! -->==Detection==
The problem was detected by users and then raised on IRC via the #mediawiki_security channel.",The reindex maintenance procedure can promote a ''broken'' index to production causing immediate failures on the affected wikis. It could possibly try to generate a couple representative queries and make sure that they run before doing such promotion?,"* [[phab:T339939]] Add alerting on the number of CirrusSearch failures (rejected), the problem was detected by users
* [[phab:T339938]] Consider running some test queries before switch index aliases$
* [[phab:T339935]] Consider testing a few wikibase queries from the integration tests 

<mark>Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.</mark>

<mark>Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFire]]  Phabricator tag to these tasks.</mark>",2023-06-18,Unknown,Unknown,Mediawiki,Unknown,Unknown,2023-06-18_search_broken_on_wikidata_and_commons.wikitext
"{{Incident scorecard
| task = 
| paged-num = 0
| responders-num = 2
| coordinators = GModena
| start = 2023-07-19 18:50:30
| end = 2023-07-19 20:30:00
| metrics = Flink Taskmanager uptime, service availability.
| impact = The mw-page-content-change-enrich application (eqiad) has not been producing enriched events during the outage.
}}",,GModena and TChin reacted to alerts triggered by degrading SLIs.,"* 20:10:38 UTC: GModena opens thread to ACK the outage in #data-platform-engineering
* 20:20:24 UTC: TChin identifies the root cause of the issue (
* 20:24:00 UTC: GModena manually restarts the application
* 20:44:00 UTC: GModena silences Kafka Consumer lag alerts while the application catches up with queues messages.","* Application is running.
* We know what needs to be fixed.","* increase max message size allowed by Kafka
* Filter out messages larger than the max allowed size; {{phab|T342399}}",2023-07-19,Unknown,Unknown,Unknown,Unknown,Unknown,2023-07-19_enrich_job_outage.wikitext
"{{Incident scorecard
| task = 
| paged-num = 2
| responders-num = jynus, jayme
| coordinators = -
| start = 2023-08-09 11:41
| end = 2023-08-09 12:16
| metrics = Mediawiki web and api availability
| impact = For approximately 30 minutes 1% of traffic received errors (120 requests/s)
}}A change to the global k8s defaults was merged that made the next mediawiki on kubernetes deployment pick up a wrong certificate for TLS termination. {{TOC|align=right}}",,"The issue was detected by a page:

 (ProbeDown) firing: (6) Service mw-api-ext:4447 has failed probes (http_mw-api-ext_ip4)
 (ProbeDown) firing: Service mw-api-int:4446 has failed probes (http_mw-api-int_ip4)

There was also secondary failures:

 [12:51] <jinxer-wm> (KubernetesAPILatency) resolved: (4) High Kubernetes API latency

 [12:02] <icinga-wm> PROBLEM - Check unit status of httpbb_kubernetes_mw-api-int_hourly on cumin2002 is CRITICAL","''All times in UTC.''
*09:37 A change to the global k8s defaults got merged, switching to cert-manager certificates for supported charts (this is what caused the issue later on)
*11:41 deployment of mw config change https://sal.toolforge.org/log/fswa2okBGiVuUzOdKxgQ
*11:41 '''OUTAGE BEGINS'''
*11:46 <code><jinxer-wm> (KubernetesAPILatency) firing: High Kubernetes API latency (POST pods) on k8s@codfw - https://wikitech.wikimedia.org/wiki/Kubernetes - https://grafana.wikimedia.org/d/000000435?var-site=codfw&var-cluster=k8s - https://alerts.wikimedia.org/?q=alertname%3DKubernetesAPILatency</code>
*11:47 <code><jinxer-wm> (ProbeDown) firing: (2) Service mw-api-ext:4447 has failed probes (http_mw-api-ext_ip4) #page  - https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=All - https://alerts.wikimedia.org/?q=alertname%3DProbeDown</code> /  <code><jinxer-wm> (ProbeDown) firing: Service mw-api-int:4446 has failed probes (http_mw-api-int_ip4) - https://wikitech.wikimedia.org/wiki/Runbook#mw-api-int:4446 - https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=All - https://alerts.wikimedia.org/?q=alertname%3DProbeDown</code>
*11:56 <jayme> jynus: I think it's k8s related, let me check something
*11:58 Issue was identified and a patch prepared https://gerrit.wikimedia.org/r/947331
*12:08 Re-deploy of mediawiki deployments started
*[[File:Ats backend errors.jpg|thumb|ATS backend errors]]12:16 '''OUTAGE ENDS'''
*12:17 <code><jinxer-wm> (ProbeDown) resolved: (6) Service mw-api-ext:4447 has failed probes (http_mw-api-ext_ip4) #page  - https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=All - https://alerts.wikimedia.org/?q=alertname%3DProbeDown</code> / <code><jinxer-wm> (ProbeDown) resolved: (6) Service mw-api-ext:4447 has failed probes (http_mw-api-ext_ip4)  - https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/service&var-module=All - https://alerts.wikimedia.org/?q=alertname%3DProbeDown</code>","===What went well?===

* The root of the problem could be detected quickly because the person that issued the breaking change was a responder
*","* Because of the low traffic (relatively small amount of errors) it took some time to pin this to k8s- does that need some actionables, or will it resolve itself as k8s becomes the majority of requests?
* Review if some deployment procedures/testing should be strengthen (e.g. surprising changes on next deployment, canary deployment for k8s, etc)
* Some metrics become unavailable or unhealthy during deployment- could something be done about that (either for the metrics or mitigation of deployment impact)
* Should depooling k8s have been done earlier?",2023-08-09,Unknown,Unknown,Api,Unknown,Unknown,2023-08-09_mw-on-k8s_outage_due_to_wrong_tls_cert.wikitext
"{{Incident scorecard
| task = 
| paged-num = data-engineering-alerts@lists.wikimedia.org list.
| responders-num = 3 (2 Data Engineers + 1 Data platform SRE)
| coordinators = Joseph Allemandou
Antoine Quhen
Steve Munene
| start = 2023-08-29 11:02
| end = 2023-08-30 16:22
| impact = Partial log aggregation for yarn applications running on Hadoop worker nodes re-imaged between 2023-08-22 and 2023-08-30 namely an-worker11[13-28]
}}

…

<!-- Reminder: No private information on this page! -->Log aggregation was failing on recently re-imaged Hadoop worker nodes. This was due to having the wrong value for the compression type to be used by the log aggregator service. This was spotted when some specific Airflow tasks were failing randomly when Airflow tried to fetch the aggregated logs. These were non-idempotent Airflow tasks (eg rm a tmp dir, or mv an previously created archive on HDFS). The Yarn applications were in a SUCCESS state, but the Airflow process failed to retrieve the logs and marked some of the task attempts as failed. This was fixed by correcting the compression type to ''gz'' from the erroneously submitted value of ''gzip.''{{TOC|align=right}}",,"Automated email alerts from Airflow

Log aggregation alerts similar to:<syntaxhighlight lang=""text"">
[Data-engineering-alerts] Airflow alert: <TaskInstance: projectview_hourly.move_data_to_archive scheduled__2023-08-29T13:00:00+00:00 [failed]>

Try 1 out of 1

Exception:

No logs found. Log aggregation may have not completed, or it may not be enabled.

Log: Link

Host: an-launcher1002.eqiad.wmnet

Mark success: Link
</syntaxhighlight>Remove temp directory Alert<syntaxhighlight lang=""text"">
[Data-engineering-alerts] Airflow alert: <TaskInstance: druid_load_webrequest_sampled_128_daily.remove_temporary_directory scheduled__2023-08-29T00:00:00+00:00 [failed]>

Try 6 out of 6
Exception:
SkeinHook druid_load_webrequest_sampled_128_daily__remove_temporary_directory__20230829 application_1692895131960_27376
Log: Link
Host: an-launcher1002.eqiad.wmnet
Mark success: Link
</syntaxhighlight>


<mark>Write how the issue was first detected.  Was automated monitoring first to detect it? Or a human reporting an error?</mark>

<mark>Copy the relevant alerts that fired in this section.</mark>

<mark>Did the appropriate alert(s) fire? Was the alert volume manageable? Did they point to the problem with as much accuracy as possible?</mark>

<mark>TODO: If human only, an actionable should probably be to ""add alerting"".</mark>","<mark>Write a step by step outline of what happened to cause the incident, and how it was remedied.  Include the lead-up to the incident, and any epilogue.</mark>

<mark>Consider including a graphs of the error rate or other surrogate.</mark>

<mark>Link to a specific offset in SAL using the SAL tool at https://sal.toolforge.org/ ([https://sal.toolforge.org/production?q=synchronized&d=2012-01-01 example])</mark>

''All times in UTC.''

*2023-08-29 
*11:02 Airflow alert: <TaskInstance: projectview_hourly.move_data_to_archive scheduled__2023-08-29T09:00:00+00:00 [failed]> 'No logs found. Log aggregation may have not completed, or it may not be enabled.'
*16:07 Airflow alert: <TaskInstance: pageview_hourly.move_data_to_archive scheduled__2023-08-29T13:00:00+00:00 [failed]>'No logs found. Log aggregation may have not completed, or it may not be enabled.'
*16:13 Airflow alert: <TaskInstance: projectview_hourly.move_data_to_archive scheduled__2023-08-29T13:00:00+00:00 [failed]> 'No logs found. Log aggregation may have not completed, or it may not be enabled.'
*20:23 Airflow alert: <TaskInstance: druid_load_webrequest_sampled_128_hourly.remove_temporary_directory scheduled__2023-08-29T18:00:00+00:00 [failed]> 'No logs found. Log aggregation may have not completed, or it may not be enabled.'
*21:53 Airflow alert: <TaskInstance: druid_load_pageviews_hourly.remove_temporary_directory scheduled__2023-08-29T19:00:00+00:00 [failed]> 'No logs found. Log aggregation may have not completed, or it may not be enabled.'
*23:30 Airflow alert: <TaskInstance: pageview_hourly.move_data_to_archive scheduled__2023-08-29T20:00:00+00:00 [failed]> 'No logs found. Log aggregation may have not completed, or it may not be enabled.' 2023-08-30
*00:42 Airflow alert: <TaskInstance: projectview_geo.move_data_to_archive scheduled__2023-08-29T22:00:00+00:00 [failed]> 'No logs found. Log aggregation may have not completed, or it may not be enabled.'
*04:11 Airflow alert: <TaskInstance: pageview_hourly.move_data_to_archive scheduled__2023-08-30T02:00:00+00:00 [failed]> 'No logs found. Log aggregation may have not completed, or it may not be enabled.'
*04:29 Airflow alert: <TaskInstance: druid_load_webrequest_sampled_128_daily.remove_temporary_directory scheduled__2023-08-29T00:00:00+00:00 [failed]> Exception: SkeinHook druid_load_webrequest_sampled_128_daily__remove_temporary_directory__20230829 application_1692895131960_27376
*08:03 Airflow alert: <TaskInstance: pageview_hourly.move_data_to_archive scheduled__2023-08-30T06:00:00+00:00 [failed]> 'No logs found. Log aggregation may have not completed, or it may not be enabled.'
*09:11 Airflow alert: <TaskInstance: projectview_geo.move_data_to_archive scheduled__2023-08-30T07:00:00+00:00 [failed]> 'No logs found. Log aggregation may have not completed, or it may not be enabled.'
*10:02 Airflow alert: <TaskInstance: projectview_hourly.move_data_to_archive scheduled__2023-08-30T08:00:00+00:00 [failed]> 'No logs found. Log aggregation may have not completed, or it may not be enabled.'
*10:36 Airflow alert: <TaskInstance: druid_load_pageviews_hourly.remove_temporary_directory scheduled__2023-08-30T08:00:00+00:00 [failed]> Exception: SkeinHook druid_load_pageviews_hourly__remove_temporary_directory__20230830 application_1692895131960_29339
*12:14 Meeting to diagnose exact cause
*12:26 Test fix for log compression algorithm
*13:06 Airflow alert: <TaskInstance: projectview_hourly.move_data_to_archive scheduled__2023-08-30T11:00:00+00:00 [failed]> 'No logs found. Log aggregation may have not completed, or it may not be enabled.'
*13:48 [[gerrit:c/operations/puppet/+/953599|Fix hadoop-yarn log aggregation compression]] patch submitted.
*14:02 Airflow alert: <TaskInstance: projectview_hourly.move_data_to_archive scheduled__2023-08-30T12:00:00+00:00 [failed]> 'No logs found. Log aggregation may have not completed, or it may not be enabled.'
*14:08 restart hadoop-yarn-nodemanager.service on an-worker10[78-99].eqiad.wmnet in batches of 2 with 3 minutes in between
*14:46 restart hadoop-yarn-nodemanager.service on an-worker11[00-28].eqiad.wmnet in batches of 2 with 3 minutes in between
*15:43 restart hadoop-yarn-nodemanager.service on an-worker11[29-48].eqiad.wmnet in batches of 2 with 3 minutes in between
*16:22 All restarts complete and the whole cluster is now using the right compression-type.
<!-- Reminder: No private information on this page! -->","<mark>OPTIONAL: General conclusions (bullet points or narrative)</mark>

* We need to test config changes on the Hadoop test cluster first before merging.
* We need to remember to restart the services after a verified config change commit","* Define an alert if the yarn config file modification time is newer than the service start time by more than 24 hours
** Apply this new alert pattern to other (HDFS, Namenode etc)

<mark>Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.</mark>

<mark>Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFire]]  Phabricator tag to these tasks.</mark>",2023-08-30,Unknown,Monitoring,Unknown,Unknown,Unknown,2023-08-30_hadoop-yarn.wikitext
"{{Incident scorecard
| task = T346945
| paged-num = 0
| responders-num = 6
| coordinators = Guillaume Lederrey
| start = 2023-09-20 14:02:00
| end = 2023-09-20 14:26:07
| metrics = Elasticsearch latency
| impact = For the duration of the incident, on-wiki search was delayed or unavailable for a subset of end users.
}}

On-wiki search for all wikis was delayed or unavailable for a subset of end users.

{{TOC|align=right}}",,"<mark>'''Was automated monitoring first to detect it? Or a human reporting an error?'''</mark>

Monitoring caught it, but not quickly enough for humans to prevent user-facing impact.

<mark>'''Copy the relevant alerts that fired in this section.'''</mark>

''** PROBLEM alert - graphite1005/CirrusSearch codfw 95th percentile latency - more_like is CRITICAL'' (Grafana alert sent to IRC and Search Platform email list)

<mark>'''Did the appropriate alert(s) fire? Was the alert volume manageable? Did they point to the problem with as much accuracy as possible?'''</mark>

Yes to all questions.","14:00 UTC Primary datacenter switches from EQIAD to CODFW.

14:00 A portion of queries (up to 25% at peak) begin being rejected by the PoolCounter during the [https://grafana-rw.wikimedia.org/d/qrOStmdGk/elasticsearch-pool-counters?orgId=1&from=1695217312990&to=1695221080550&viewPanel=2 incident window]; rejections will continue for the next half hour. The spike in rejections was caused by long request response times that were themselves worsened by the unexpected [https://grafana-rw.wikimedia.org/d/000000455/elasticsearch-percentiles?from=1695214800000&to=1695222000000&var-cirrus_group=eqiad&var-cluster=elasticsearch&var-exported_cluster=production-search&var-smoothing=1&viewPanel=55&editPanel=55&orgId=1 decline in cache hit rate] from '''71.6%''' to '''55.5%'''

14:08 Alert ""CirrusSearch codfw 95th percentile latency - more_like"" fires

14:14 Hashar pings in #wikimedia-search. Dcausse, inflatador and pfischer begin troubleshooting.

14:26 Grafana shows extremely high load on ''elastic2044'', inflatador bans this node from the cluster and latency begins to drop back to normal.

14:34 claime confirms impact has passed.

14:35 Poolcounter rejections are now at zero.","===What went well?===

* SREs were at a heightened state of awareness due to DC switchover, so impact was recognized immediately.
* Mitigation was quick and easy (banning a single node). Incident resolved within ~30m, whether due to self-healing or due to the aforementioned node ban.","* Determine what Elasticsearch-related steps are needed prior to a datacenter switchover.
* Update documentation as mentioned in ""Links to relevant documentation"" section above.
* Review https://phabricator.wikimedia.org/T285347 (suggestions for switchover improvements) and decide whether to implement.
* [[phab:T347034]] Update the RESTBase <code>/related/{article}</code> endpoint to make a GET request and not a POST in order to help having a CirrusSearch query cache warm in both DC when running multi-DC
* Review current shard balance for busy wikis, particularly ''enwiki_content''. In my haste to mitigate, I (inflatador) did not check the shards of the highly-loaded host. Dcausse theorized that two enwiki shards may have been on the same host, which is extremely expensive, particularly when ''more_like'' queries are involved. That would suggest that the problem was worsened by the DC switchover, but not directly caused by it.",2023-09-20,Unknown,Monitoring,Search,Unknown,Unknown,2023-09-20_Elasticsearch_unavailable.wikitext
"{{Incident scorecard
| task = https://phabricator.wikimedia.org/T346877
| paged-num = 
| responders-num = 1
| coordinators = 1
| start = 2023-09-19 15:00:00
| end = 2024-09-20 12:00:00
| metrics = https://wikitech.wikimedia.org/wiki/MediaWiki_Event_Enrichment/SLO/Mediawiki_Page_Content_Change_Enrichment
| impact = There was no impact on the application SLO. This incident manifested on the passive DC deployment. The active DC deployment had no SLI degradation. Clients were not affected.
}}

…

<!-- Reminder: No private information on this page! -->The mw-page-content-change-enrich (flink) app is failing to start in eqiad (passive DC because of a network timeout on a dependent service (thanos-swift) <nowiki>https://logstash.wikimedia.org/goto/ce1765e186329ed74f179d375f8df182</nowiki>.

The app needs swift for HA. The connection failure caused the k8s operator to fail startup. The incident was caused by incorrect egress rules. Since the app is DC agnostic, egress rules must be set for both DCs ip ranges for all deployments.

This issue was fixed by applying the proper egress rules, a re-deploying the application. The active DC deployment was not affected.{{TOC|align=right}}",,"Alerts where fired based on prometheus metrics. 

The issue was escalated to the DRI.","''All times in UTC.''

*2023-09-19 15:00:00 alerts fire
*2023-09-20 09:10:05 gehel escalates to gmodena (#wikimedia-analytics). Troubleshooting starts.
*2023-09-20 09:42:23 issue is identified
*2023-09-20 09:46:14 triage with claime (#wikimedia-data-persistence), patch is prepared for review
*2023-09-20 12:00:00 patch has been deployed 
*

<!-- Reminder: No private information on this page! -->There was no user visible outage.","* there was no user visible outage.
* only passive DC was affected.
* mw-page-content-change-enrich egress rules to thanos were misconfigured.
* we never encountered this issue before, because swift-thanos was pooled in both eqiad and codfw.
* during DC switchover swift-thanos was depooled from eqiad.
* the network route issue manifested.","* …

<mark>Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.</mark>

<mark>Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFire]]  Phabricator tag to these tasks.</mark>",2023-09-20,Unknown,Unknown,Unknown,Unknown,Unknown,2023-09-20_mw-page-content-change-enrich.wikitext
"{{Incident scorecard
| task = T347481
| paged-num = 0
| responders-num = 7
| coordinators = Ben Tullis
| start = 2023-09-26 20:00:00
| end = 2023-09-27 14:05:00
| metrics = No relevant SLOs exist
| impact = Minimal user facing outages. Wikidata Query Service update pipline was stuck, which led to significant update lag on WDQS. Possible delay to downstream pipeline processing, but no data loss expected.
}}",,"The issue was first detected by Steve Munene and other engineers checking the #wikimedia-analytics IRC channel

We saw lots of Icinga messages like this:

<code>PROBLEM - Kafka MirrorMaker main-eqiad_to_jumbo-eqiad@0 on kafka-jumbo1001 is CRITICAL: PROCS CRITICAL: 0 processes with command name java, regex args kafka.tools.MirrorMaker.+/etc/kafka/mirror/main-eqiad_to_jumbo-eqiad@0/producer\.properties [[Kafka/Administration#MirrorMaker|https://wikitech.wikimedia.org/wiki/Kafka/Administration%23MirrorMaker]]</code>

<code>PROBLEM - Kafka MirrorMaker main-eqiad_to_jumbo-eqiad average message consume rate in last 30m on alert1001 is CRITICAL: 0 le 0 [[Kafka/Administration#MirrorMaker|https://wikitech.wikimedia.org/wiki/Kafka/Administration%23MirrorMaker]] <nowiki>https://grafana.wikimedia.org/d/000000521/kafka-mirrormaker?var-datasource=eqiad+prometheus/ops&var-lag_datasource=eqiad+prometheus/ops&var-mirror_name=main-eqiad_to_jumbo-eqiad</nowiki></code>

<code>PROBLEM - Kafka MirrorMaker main-eqiad_to_jumbo-eqiad average message produce rate in last 30m on alert1001 is CRITICAL: 0 le 0 [[Kafka/Administration|https://wikitech.wikimedia.org/wiki/Kafka/Administration]] <nowiki>https://grafana.wikimedia.org/d/000000521/kafka-mirrormaker?var-datasource=eqiad+prometheus/ops&var-lag_datasource=eqiad+prometheus/ops&var-mirror_name=main-eqiad_to_jumbo-eqiad</nowiki></code>

We also saw AlertManager alerts like this:

<code>(SystemdUnitCrashLoop) firing: (15)  crashloop on kafka-jumbo1001:9100 - TODO - <nowiki>https://grafana.wikimedia.org/d/g-AaZRFWk/systemd-status</nowiki> - <nowiki>https://alerts.wikimedia.org/?q=alertname%3DSystemdUnitCrashLoop</nowiki></code>","''All times in UTC.''

* 20:00 UTC First set of crashes recorded by Icinga, affecting all main_eqiad_to_jumbo_eqiad@0 services
* Processes restarted by puppet on each of the servers of the next 30 minutes
* 20:30 UTC Another slew of crashes followed by recoveries.
* 20:42 UTC we also received a ‘message rate too low’ alert
* PROBLEM - Kafka MirrorMaker main-eqiad_to_jumbo-eqiad average message produce rate in last 30m on alert1001 is CRITICAL: 0 le 0 
* The topics were effectively blocked from replicating between 20:00 and around 08:08
* 07:08 UTC Steve Munene reponds on IRC and requests assistance. Luca Toscano and Balthazar Rouberol also begin investigating.

Tests seem to show that running with 9 mirror maker processes is stable, but if we increase the number of mirror maker processes to 15 (which is what puppet wants to do) then it becomes unstable again.

We have seen RecordTooLargeException messages on kafka-main1003. Only this broker.

First and last RecordTooLargeException messages from the <code>/var/log/kafka/server.log</code> file on kafka-main1003.<syntaxhighlight lang=""shell-session"">
[2023-09-26 19:54:46,039] ERROR [GroupMetadataManager brokerId=1003] Appending metadata message for group kafka-mirror-main-eqiad_to_jumbo-eqiad generation 15610 failed due to org.apache.kafka.common.errors.RecordTooLargeException, returning UNKNOWN error code to the client (kafka.coordinator.group.GroupMetadataManager)

[2023-09-27 08:28:05,100] ERROR [GroupMetadataManager brokerId=1003] Appending metadata message for group kafka-mirror-main-eqiad_to_jumbo-eqiad generation 19944 failed due to org.apache.kafka.common.errors.RecordTooLargeException, returning UNKNOWN error code to the client (kafka.coordinator.group.GroupMetadataManager)
</syntaxhighlight>These generation numbers appear generally in sequence, but there are some missing. I don’t know if this is relevant, but here is the list of missing generation numbers from .

* 10:30 UTC We have been running on 9 mirror maker processes for some time with no problems observed. There have been no more RecordTooLargeException errors on kafka-main1003 since the last one at 08:28
* 12:30 UTC elukey slowly started re-enabling the mirrormakers over the next 30 minutes
* 13:00 UTC as the mirrormaker for kafka-jumbo1015 was re-enabled it triggered another failure, with the same logs at the broker side.
* 13:10 We tried another test by stopping the mirrormaker on kafka-jumbo1001 and starting it on kafka-jumbo1015 - This was stable, which indicates that it is the 15th mirrormaker that causes the instability.
* 13:45 After some research, documented https://phabricator.wikimedia.org/T347481#9203313 we reason that the only way to fix this bug would be to increase the max.message.size on the kafka-main brokers. However, we don’t want to change this setting on the other clusters, so we have decided instead to limit the number of mirrormaker instances to 9 instead of 15. 
* 14:00 Luca proposes a patch https://gerrit.wikimedia.org/r/c/operations/puppet/+/961397 to exclude mirrormaker from the soon-to-be-decommissioned hosts, kafka-jumbo100[1-6]
* 14:05 Ben merges and deploys the patch, runs puppet on all kafka-jumbo brokers and removes downtime. Incident resolved.","* We think that the root cause is a bug in our version of Kafka and/or Mirrormaker
* This triggers when we increase the number of mirrormaker instances pulling from kafka-main-eqiad to kafka-jumbo to exactly 15 (or probably more)
* There is a proposed workaround, but this requires changing the settings of kafka-main, which we did not want to do.","* [[phab:T300102|Upgrade Kafka to 2.x or 3.x]]
* We could consider whether we want to change the settings on kafka-main but for now we have just selected a smaller number of mirrormakers.
* [[phab:T347515]]: better sandboxing of the WDQS updater test instance",2023-09-27,Unknown,Unknown,Analytics,Unknown,Unknown,2023-09-27_Kafka-jumbo_mirror-makers.wikitext
"{{Incident scorecard
| task = T347676
| paged-num = ?
| responders-num = 4
| coordinators = Andrew Otto
| start = 2023-09-28 14:33:00
| end = 2023-10-03
| metrics = https://wikitech.wikimedia.org/wiki/MediaWiki_Event_Enrichment/SLO/Mediawiki_Page_Content_Change_Enrichment
| impact = No messages should have been lost, but they have been delayed by several days.
}}

…

<!-- Reminder: No private information on this page! -->As part of T336044, @brouberol began copying Kafka partitions to new brokers.  Somehow, even with a throttle of 50MB/sec, this caused the mw-page-content-change Flink streaming enrichment job to fail producing messages to Kafka. 

The responders decided to shut down the streaming application until the partitions finished copying to the new brokers.  On Monday Oct 2 The streaming application was restarted. Messages were produced, but , @tchin, @joal and @ottomata noticed the backlog of messages was not decreasing.  They increased the parallelism to 4 replicas, and finally the backlog began to be processed.

{{TOC|align=right}}",,"Email alert fired.<blockquote>alertname = MediawikiPageContentChangeEnrichJobManagerNotRunning

job_name = mw_page_content_change_enrich

kubernetes_namespace = mw-page-content-change-enrich

prometheus = k8s

release = main

severity = critical

site = codfw

source = prometheus

team = data-engineering</blockquote>","<mark>Write a step by step outline of what happened to cause the incident, and how it was remedied.  Include the lead-up to the incident, and any epilogue.</mark>

<mark>Consider including a graphs of the error rate or other surrogate.</mark>

<mark>Link to a specific offset in SAL using the SAL tool at https://sal.toolforge.org/ ([https://sal.toolforge.org/production?q=synchronized&d=2012-01-01 example])</mark>

''All times in UTC.''

*2023-09-28 14:33:00 '''MediawikiPageContentChangeEnrichJobManagerNotRunning Alert Fires'''
*2023-09-28 15:08:00 @tchin restarts the job
*2023-09-28 15:51:00 Notices job restarting about ever 5 minutes, eventually dies with <code>Failed to send data to Kafka</code>
*2023-09-28 15:55:00 Message sent to #event-platform Slack channel, troubleshooting begins
*2023-09-28 16:24:00 @tchin and @joal restarts job again
*2023-09-28 17:33:00 Suspect something wrong with checkpoints since on every restart the same events are being processed. Tried unaligned checkpoints to no avail.
*2023-09-28 17:58:13 @joal suspects it's related to Kafka ingestion rather than Flink. Killed app, silenced alerts, wait for SRE
*2023-09-29 06:48:57 @brouberol suspects ongoing [[phab:T336044#9206891|Kafka broker reassignment]] as the issue as it is starving the broker threads. Group decided to wait for reassignment to finish over the weekend.
*2023-10-02 07:35:21 @joal restarts flink app after reassignment finished.
*2023-10-02 09:42:09 @joal notices average time for async operations increased, backpressure not decreasing and even slowly increasing.
*2023-10-02 12:10:36 @tchin proposes theory and suggests increasing parallelism:
<blockquote>My current theory is because of the backpressure that we’re seeing more events that have api calls that lead to bad rev id responses, and we retry those api calls just in case. By default we try 3 times with 5 seconds between each try. We also happen to have a max connection pool set to 12. If we’re constantly seeing bad events, then we’re basically decreasing the max pool size for good events by some unknown amount<ref>After more investigation, this combined with batching events inside eventutilities-python, probably meant that all events in the batch were being delayed if a single event in the batch was bad</ref></blockquote>

* 2023-10-02 14:06:21 @joal [[gerrit:c/operations/deployment-charts/+/962620/|deploys parallelism increase]]. Backpressure finally starts decreasing.
* 2023-10-02 16:14:00 @otto files [[phab:T347884|mw-page-content-change-enrich should not retry on badrevids if no replica lag]]
* 2023-10-03 12:21:51 @otto [[gitlab:repos/data-engineering/mediawiki-event-enrichment/-/merge_requests/72|deploys a patch]]. Backfill speeds up >8x
*2023-10-03 14:14:54 '''JOB CAUGHT UP, INCIDENT ENDS'''
*00:15 (post-outage cleanup finished)<!-- Reminder: No private information on this page! -->

<mark>TODO: Clearly indicate when the user-visible outage began and ended.</mark>

{{reflist}}",<mark>OPTIONAL: General conclusions (bullet points or narrative)</mark>,"* {{Check mark}}[[phab:T347884|T347884 - '''mw-page-content-change-enrich should not retry on badrevids if no replica lag''']]
** This will help with backlog processing for future backfills.
* Figure out why copying Kafka partitions caused [[phab:T347676|this issue]].
* {{Check mark}}Consider running the streaming app with more than 1 replica even in normal cases, to help with backfills or spikes. Now running with 2 replicas.
* [[phab:T345806|T345806 - '''mediawiki.page_content_change.v1 topic should be partitioned.''']]  This will also help with backfills.


<mark>Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFire]]  Phabricator tag to these tasks.</mark>",2023-09-28,Unknown,Unknown,Mediawiki,Unknown,Unknown,2023-09-28_mw-page-content-change-enrich.wikitext
"{{Incident scorecard
| task = T347665
| paged-num = 1
| responders-num = 5
| coordinators = arturo
dcaro
taavi
| start = 11:00 UTC 2023-09-28 (day before this report)
| end = 13:00 UTC 2023-09-29
| metrics = No relevant SLOs exist
| impact = Any cloud-vps hosted external service was down (including toolforge, paws, quarry, and others). Some of the VMs became unreachable through ssh.
}}

<!-- Reminder: No private information on this page! -->During a package cleanup, {{gerrit|961005}} was merged to remove some packages. This caused to bullseye VMs in the cloud realm to remove isc-dhcp-client, and once the ip leases for these started to expire the VMs started losing network connectivity.

This eventually included the proxies CloudVPS uses to serve external traffic, making any hosted project lose that traffic too.

This also took down the metricsinfra VMs that are in charge of the monitoring and alerting for CloudVPS hosted projects, so there were no alerts from it.

From there recovery included having to roll-reboot all the toolforge VMs that depend on the nfs servers as the nfs service itself got affected and clients got stuck (common procedure, but slow).

{{TOC|align=right}}",,"The issue was first detected by users, and it was not until the first admin started their work day that they noticed something was wrong.

The only page was received way after, once the recovery had started.

Note that the outage took one of the monitoring and alerting systems down, though we would not have been paged by any alert there (https://phabricator.wikimedia.org/T323510).","tools k8s nodes reboot: https://sal.toolforge.org/tools?d=2023-09-29

Alert logs (team=wmcs, there might be non-wmcs ones): https://logstash.wikimedia.org/goto/906ec4838ab338cc70e1484010ab7df2

IRC archives:

* https://wm-bot.wmcloud.org/logs/%23wikimedia-cloud-admin/20230929.txt
* https://wm-bot.wmcloud.org/logs/%23wikimedia-cloud/20230929.txt

''All times in UTC.''

28 September 2023:

*11:07 '''OUTAGE BEGINS - kinda, patch is merged''' https://gerrit.wikimedia.org/r/c/operations/puppet/+/961005
29 September 2023:
*04:14:00 - 07:06 - some users report connectivity issues on irc, no admins notice it  - First user impact
*06:24 - user reports connectivity issues https://phabricator.wikimedia.org/T347661
*06:56 - outage task created by user https://phabricator.wikimedia.org/T347665
*07:06 - admin starts looking into the issues as they notice alertmanager down (https://www.irccloud.com/pastebin/aA1NNmt1/), another admin joins
*07:11:25 - find that project-proxy is not responding
*07:16:13 - find out that dhclient is not installed on the VM (that otherwise looks ok)
*07:18:06 - found that there was a patch that deleted the package through puppet by the apt logs
*07:23:59 - revert sent, restore started (as they can't run puppet, we have to ""manually"" fix them), two efforts: one admin writing as script to automate the fix, the other admin starting to manually fix the core/critical VMs
*07:31 - first email sent to cloud-announce about the outage
*08:10 - a third admin joins, helps manually fixing the other critical VMs
*08:21:23 - metricsinfra alerts restored (manually)
*08:35 - we ran a script to fix the issues, running in parallel
*09:16:58 - script finishes a first round through the whole fleet
*09:32 - rebooting tools-nfs-2 since the network setup on nfs servers needs a reboot + puppet run ({{Phabricator|T347681}})
*09:37 - start rebooting k8s worker nodes to release stuck nfs file handles
*09:38:10 - admin paged: checker.tools.wmflabs.org/toolschecker: NFS read/writeable on labs instances
*09:42 - grid reboot cookbook is failing as the nodes are stuck and it does not try to force-reboot through openstack
*10:02 - rebooting all other NFS instances
*10:08:42 - all grid bastions and workers rebooted
*11:46 (Voila) '''OUTAGE ENDS - message on irc, all services running'''
*13:03 - email to cloud-announce declaring the outage over
<!-- Reminder: No private information on this page! -->",,"* {{phab|T347694}} - investigate why we did not get any pages, and fix/add them
* {{phab|T288053}} - add meta-monitoring for metricsinfra
* {{phab|T347683}} - create a cookbook to run commands through virsh console
* {{phab|T347681}} - improve current nfs setup so it does not require to reboot + run puppet to bring online (as it might take 30 min for puppet to run unattended)",2023-09-29,Unknown,Monitoring,Unknown,Unknown,Unknown,2023-09-29_CloudVPS_vms_losing_network_connectivity.wikitext
"{{Incident scorecard
| task = 
| paged-num = batphone
| responders-num = 8
| coordinators = [[User:Fabfur|fabfur]]
| start = 2023-11-15T14:04:10
| end = 2023-11-15T14:21
| impact = ESAMS DC was unreachable by users
}}

ESAMS DC was unreachable by users. Globally, we experienced a request drop from ~150k req/s to ~91k req/s as outlined in the [https://grafana.wikimedia.org/goto/sSa9A5IIz Grafana dashboard]. Users that were trying to reach Amsterdam DC experienced network errors and delays.

<mark>The incident doc still mentioned ""We’re still investigating why some Grafana panels shows ~1hr of requests drop instead of ~10m (the actual incident duration)""</mark>

{{TOC|align=right}}",,"The outage was detected by an SRE shortly after an accidental change. Fired alerts confirmed the outage as well:

<code>
<+icinga-wm> PROBLEM - Host ncredir3003 is DOWN: PING CRITICAL - Packet loss
</code>

<mark>Did the appropriate alert(s) fire? Was the alert volume manageable? Did they point to the problem with as much accuracy as possible?</mark>","* 14:03 <XioNoX> !log reboot fpc0 on cr1-esams - T346779 ([https://sal.toolforge.org/production?p=2&q=&d=2023-11-15 SAL])
* 14:04 <XioNoX> I restarted the linecard on the wrong router… '''Outage starts'''
* 14:05 Start receiving alerts, e.g. <+icinga-wm> PROBLEM - Host ncredir3003 is DOWN: PING CRITICAL - Packet loss
* 14:06 <vgutierrez> let's depool esams?
* 14:07 <jynus> vgutierrez: it is recovering now
* 14:07 <XioNoX> looks like it's back faster than a depool is needed
* 14:08 <vgutierrez> I can't reach text-lb.esams.wikimedia.org from here FWIW
* 14:08 depool esams https://gerrit.wikimedia.org/r/c/operations/dns/+/974537/
* 14:08 DEPOOL FAILED due to authends-update issues
* 14:09 <+icinga-wm> started to see recoveries RECOVERY
* 14:09 <sukhe> /tmp/dns-check.rudydea2/zones/netbox/4.64.10.in-addr.arpa
* 14:09 <sukhe> we need to fix this to fix authdns-update
* 14:10 <topranks> let's just merge https://gerrit.wikimedia.org/r/c/operations/dns/+/974534
* ??:?? <XioNoX> text-lb is reachable again
* 14:11 <jynus> NEL is still high
* 14:16 marostegui updates the status page
* 14:17 <jynus> http traffic volume is still elevated, though
* 14:18 <jynus> others parameters look back to normal, including NEL timeouts
* 14:21 <+jinxer-wm> (NELHigh) resolved: (2) Elevated Network Error Logging events '''Outage ends'''
* 14:25 Updated status page to monitoring
* 14:26 Incident opened; [[User:Fabfur|fabfur]] becomes IC.
* 14:35 Status page set to resolved

<gallery mode=""packed"">
2023-11-15 esams unreachable - NEL.png
2023-11-15 esams unreachable - varnish.png
2023-11-15 esams unreachable - webreqs.png
</gallery>

* [https://logstash.wikimedia.org/app/dashboards#/view/ee6432c0-82a9-11eb-9d45-739221ba7fb6?_g=h@449fbe4&_a=h@a0afe3e Network Error Logging]
* [https://superset.wikimedia.org/superset/dashboard/p/d7VvNZ8v1X0/ Webrequests live dashboard]",<mark>OPTIONAL: General conclusions (bullet points or narrative)</mark>,"* …

<mark>Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.</mark>

<mark>Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFire]]  Phabricator tag to these tasks.</mark>",2023-11-15,Unknown,Unknown,Unknown,Unknown,Unknown,2023-11-15_esams_unreachable.wikitext
"{{Incident scorecard
| task = T358636
| paged-num = 15
| responders-num = 2
| coordinators = 1
| start = 2024-02-28 01:39
| end = 2024-02-28 02:52
| metrics = None. Each of the two etcd clusters was healthy, so the etcd SLO was unaffected. Replication is not covered by an SLO.
| impact = No user impact. Near miss for split brain config scenarios, as described below.
}}

[[Conftool]] data, stored in [[etcd]], is normally replicated from one data center to the other (currently eqiad to codfw) using an in-house tool called [https://gerrit.wikimedia.org/r/plugins/gitiles/operations/software/etcd-mirror/+/refs/heads/master/etcdmirror etcd-mirror]. Etcd-mirror is intentionally fairly brittle to replication problems; when it encounters an unexpected situation, it stops replicating and pages us, rather than attempt to self-recover and risk corrupting the etcd state.

Due to an unusual sequence of data changes (detailed below) etcd-mirror's long-running watch lost track of the up-to-date state of etcd in eqiad, and replication stopped. This had no immediate user impact, but subsequent conftool changes (like pooling/depooling hosts in load-balanced clusters, pooling/depooling data centers in service discovery, or requestctl changes) would not have been propagated between data centers, leaving us without emergency tools and potentially causing a dangerous skew in production config.

{{TOC|align=right}}",,"We were paged when etcd-mirror failed. (A [[Incidents/2022-09-08_codfw_appservers_degradation|prior incident]] was exacerbated after the paging was accidentally dropped in the migration to alertmanager; that was fixed in [[phab:T317402|T317402]] as a followup. It was only a moderate factor in that incident, because SREs independently noticed errors in the etcd-dependent tools they were using. By contrast, if the alert hadn't paged us here, it's likely nobody would have noticed the problem for much longer.)

In addition to that EtcdReplicationDown alert, we also got a non-paging SystemdUnitFailed alert, which is redundant but not harmfully so (and might be a useful extra pointer for responders not familiar with etcd-mirror).

We also got five separate non-paging JobUnavailable alerts for etcd-mirror, at 1:48, 2:38, 2:38, 2:43, and 2:53, the last one being ''after'' etcd-mirror had resumed. These didn't contribute any extra information.","''All times in UTC.''
*00:00 Last conftool etcd event is successfully replicated from eqiad to codfw through etcd-mirror. (The midnight UTC timestamp is a coincidence.)

 Feb 28 00:00:26 conf2005 etcdmirror-conftool-eqiad-wmnet[2619978]: [etcd-mirror] INFO: Replicating key /conftool/v1/request-ipblocks/cloud/linode at index 3020127

*00:00-01:39 An unusual number of reimage and other cookbooks are run (acquiring and releasing locks in etcd), and there are no changes to conftool data. Unusually, this produces 1,000 consecutive ''non''-conftool etcd events.

*01:39 etcd-mirror fails:

 Feb 28 01:39:44 conf2005 etcdmirror-conftool-eqiad-wmnet[2619978]: CRITICAL: The current replication index is not available anymore in the etcd source cluster.
 Feb 28 01:39:44 conf2005 etcdmirror-conftool-eqiad-wmnet[2619978]: [etcd-mirror] CRITICAL: The current replication index is not available anymore in the etcd source cluster.
 Feb 28 01:39:44 conf2005 etcdmirror-conftool-eqiad-wmnet[2619978]: [etcd-mirror] INFO: Restart the process with --reload instead.
 Feb 28 01:39:46 conf2005 systemd[1]: etcdmirror-conftool-eqiad-wmnet.service: Main process exited, code=exited, status=1/FAILURE

*01:42 Alerts fire:

 01:42:41 <jinxer-wm> (EtcdReplicationDown) firing: etcd replication down on conf2005:8000 #page - https://wikitech.wikimedia.org/wiki/Etcd/Main_cluster#Replication - TODO - https://alerts.wikimedia.org/?q=alertname%3DEtcdReplicationDown
 01:45:25 <jinxer-wm> (SystemdUnitFailed) firing: etcdmirror-conftool-eqiad-wmnet.service on conf2005:9100 - https://wikitech.wikimedia.org/wiki/Monitoring/check_systemd_state - https://grafana.wikimedia.org/d/g-AaZRFWk/systemd-status - https://alerts.wikimedia.org/?q=alertname%3DSystemdUnitFailed

*01:46 rzl and swfrench consider restarting with <code>--reload</code> as indicated but decide to investigate first.

*02:03 swfrench curls https://<nowiki/>conf2005.codfw.wmnet:4001/v2/keys/__replication and gets a value matching the last successful read at 00:00:26:

 {
     ""action"": ""get"",
     ""node"": {
         ""key"": ""/__replication"",
         ""dir"": true,
         ""nodes"": [
             {
                 ""key"": ""/__replication/conftool"",
                 ""value"": ""3020127"",
                 ""modifiedIndex"": 5150719,
                 ""createdIndex"": 4271
             }
         ],
         ""modifiedIndex"": 2140,
         ""createdIndex"": 2140
     }
 }

*02:06 rzl and swfrench consider whether to try restarting etcd-mirror ''without'' <code>--reload</code>, in case the error is trivially recoverable; as we're talking about it, the unit restarts itself and fails with the same error. This causes some confusion before we simply note that it didn't work and move on. (We thought it might have been someone participating in the incident response without speaking up on IRC; it was just Puppet happening to run at the instant we were talking about it.)

 Feb 28 02:07:22 conf2005 systemd[1]: Started Etcd mirrormaker.
 Feb 28 02:07:23 conf2005 etcdmirror-conftool-eqiad-wmnet[3349905]: [etcd-mirror] INFO: Current index read from /__replication/conftool
 Feb 28 02:07:23 conf2005 etcdmirror-conftool-eqiad-wmnet[3349905]: [etcd-mirror] INFO: Starting replication at 3020127
 Feb 28 02:07:23 conf2005 etcdmirror-conftool-eqiad-wmnet[3349905]: CRITICAL: The current replication index is not available anymore in the etcd source cluster.
 Feb 28 02:07:23 conf2005 etcdmirror-conftool-eqiad-wmnet[3349905]: [etcd-mirror] CRITICAL: The current replication index is not available anymore in the etcd source cluster.
 Feb 28 02:07:23 conf2005 etcdmirror-conftool-eqiad-wmnet[3349905]: [etcd-mirror] INFO: Restart the process with --reload instead.
 Feb 28 02:07:23 conf2005 systemd[1]: etcdmirror-conftool-eqiad-wmnet.service: Main process exited, code=exited, status=1/FAILURE

*02:16 We track down the ""replication index is not available"" error message in [https://gerrit.wikimedia.org/r/plugins/gitiles/operations/software/etcd-mirror/+/b5823aa025b8555738f8fbe409e0e6c000e1985c/etcdmirror/main.py#186 the etcd-mirror source], and identify it as an EtcdEventIndexCleared exception. swfrench conjectures that we might be seeing the situation described at the bottom of https://etcd.io/docs/v2.3/api/#waiting-for-a-change (""Note: etcd only keeps the responses of the most recent 1000 events across all etcd keys."") in which case the correct recovery is to get and then watch, to recover the missing events, as described in the next section, https://etcd.io/docs/v2.3/api/#watch-from-cleared-event-index. etcd-mirror does not support this.

*02:21 swfrench confirms with <code>curl -v 'https://<nowiki/>conf1009.eqiad.wmnet:4001/v2/keys/conftool?wait=true&waitIndex=3020127'</code> that that's the problem: X-Etcd-Index is just over 1,000 events ahead of the wait index. That would indicate there were more than 1,000 events outside the /conftool keyspace -- such as locks for Spicerack -- without any events under /conftool. Since this was an unusually busy day for reimage cookbooks, with Spicerack locks newly added in [[phab:T341973|T341973]], this is both a new failure mode and a complete explanation of the root cause and trigger.

*02:25 swfrench considers manually advancing /__replication/conftool in codfw by 999. This would advance the replication index until just prior to the end of the ""safe"" 1,000-event window where we knew there were no conftool events, which also happened to be well within the 1,000-event retention window bounded by the then-current etcd index in eqiad (3021261). (Advancing it beyond the safe window could skip events that should have been replicated, potentially leading to a dangerous split-brain condition.) The alternative would be restarting etcd-mirror with <code>--reload</code>, which obliterates the state of etcd in the replicated cluster before recreating it, something which has [[Incidents/2022-09-08_codfw_appservers_degradation|aggravated incidents in the past]]; in this case a successful <code>--reload</code> should be safe, but if it hit a snag, we were uncertain of being able to recover quickly with the staff available.

*02:40 We decide to proceed with manually advancing the index, on the theory that if it doesn't work we can always proceed with an etcd-mirror reload; even if it ''does'' work, we can always follow up with a reload during working hours to ensure a clean state.

*02:50 swfrench runs <code>curl https://<nowiki/>conf2005.codfw.wmnet:2379/v2/keys/__replication/conftool -XPUT -d ""value=3021126""</code> on conf2005.

*02:52 swfrench restarts etcd-mirror, which resumes normally.

*02:55 Alerts recover:

 02:55:25 <jinxer-wm> (SystemdUnitFailed) resolved: etcdmirror-conftool-eqiad-wmnet.service on conf2005:9100 - https://wikitech.wikimedia.org/wiki/Monitoring/check_systemd_state - https://grafana.wikimedia.org/d/g-AaZRFWk/systemd-status - https://alerts.wikimedia.org/?q=alertname%3DSystemdUnitFailed
 02:57:41 <jinxer-wm> (EtcdReplicationDown) resolved: etcd replication down on conf2005:8000 #page - https://wikitech.wikimedia.org/wiki/Etcd/Main_cluster#Replication - TODO - https://alerts.wikimedia.org/?q=alertname%3DEtcdReplicationDown

*03:03 swfrench [https://sal.toolforge.org/log/jXOq7Y0BGiVuUzOd-9We depools], then [https://sal.toolforge.org/log/252s7Y0BxE1_1c7syqDY repools] mw2268, in order to test replication; the conftool events propagate via etcd-mirror successfully.","===What went well?===

* Automated monitoring detected the replication failure.
* Wikitech documentation on etcd and etcd-mirror, along with etcd's own documentation, gave us most of what we needed to root-cause and work around the problem.
* Manually advancing the index value was both feasible to do (because of the expressive REST API allowing us to modify the replication state) and an effective resolution (in part because there were less than 2,000 total unreplicated events, meaning we could advance to an available state with confidence that we weren't losing any data).","* Long-term solution: Switch conftool from etcd v2 to v3. As part of this effort, etcd-mirror will be replaced with an alternative solution for cross-site replication, a requirement for which is resilience to this class of problem. ([[phab:T350565|T350565]])
* In the meantime, consider building the recovery described at https://etcd.io/docs/v2.3/api/#watch-from-cleared-event-index into etcd-mirror, so that it can detect and handle this situation automatically. It may or may not be worth the engineering effort, depending on the timeline for moving to v3, but now that there's much more traffic to etcd keys outside of /conftool (notably because of Spicerack locking) this situation is more likely than it used to be, so we may want to be resilient to it even in the short term. Alternatively, we could replicate the entire keyspace and not just /conftool, if etcd-mirror can sustain the load. ([[phab:T358636|T358636]])
* Update [[Etcd/Main cluster#Replication]] documentation with safe restart conditions and information. Populate the ""TODO"" playbook link for EtcdReplicationDown in the process. ([[phab:T317537|T317537]])",2024-02-28,Unknown,Unknown,Unknown,Unknown,Unknown,2024-02-28_etcd-mirror.wikitext
"{{Incident scorecard
| task = T361288
| paged-num = 0
| responders-num = 3
| coordinators = Brian King
| start = Wed Mar 27 21:45:03 2024
| end = Wed Mar 27 23:59 2024
| impact = From Wed Mar 27 21:34:03 2024 to Wed Mar 27 23:59 2024 , users connecting to the CODFW omega cluster (comprised of ~1600 smaller wikis) were unable to search these wikis. Edits to the pages during this time were not added to search indices. Larger wikis were not affected. CODFW was the non-primary datacenter and thus was not receiving the bulk of user traffic.
}}

{{TOC|align=right}}",,"Humans noticed the problem immediately, as it was directly caused by operator error.

No alerts had time to fire.","''All times in UTC.''

*2024-03-27 21:34:03 2024 Brian King (Search Platform SRE) merges a puppet patch that removes omega masters in preparation for decom. Once Puppet is run, soon-to-be-decommed master hosts are firewalled off from the cluster, making it impossible for them to participate in leader election.
*2024-03-27 21:45:42 Brian King (Search Platform SRE) [https://phabricator.wikimedia.org/T358882#9667270 runs] sre.hosts.decommission cookbook for elastic2037-2054.
*~2024-03-27 22:00  Ryan Kemper (Search Platform SRE) notices a ""master not discovered exception"" (503) from the CODFW omega endpoint
*2024-03-27 23:04:41,721 realizing the problem is related to decom work, Brian types ""abort"" into the cookbook prompt. It stops the network change that it was displaying, but it continues to wipe the disk of one of the active masters (elastic2052).
*After several attempts to fix cluster state, Brian, Ryan, and Erik (Search Platform SWE) decide to depool CODFW omega and reconvene the next day.
*~Wed Mar 27 23:59 2024 [[gerrit:c/operations/mediawiki-config/+/1015157|Patch]] is merged to force omega traffic to eqiad; impact ends
*Thurs Mar 28 1300 UTC CODFW Brian restores quorum to the cluster using [[Search#Cluster Quorum Loss Recovery Procedure|this procedure]]
*TBA CODFW omega repooled (depends on [[gerrit:c/operations/mediawiki-config/+/1015393|this patch]] being merged/deployed)","===What went well?===

* Humans noticed the problem immediately and were able to mitigate it.","* Update docs (done, see ""relevant documentation"" section above)
* All other action items listed in https://phabricator.wikimedia.org/T361288",2024-03-27,Unknown,Unknown,Unknown,Unknown,Unknown,2024-03-27_Elasticsearch_Omega_Cluster_Failure.wikitext
"{{Incident scorecard
| task = T362766
| paged-num = 
| responders-num = 5
| coordinators = 1
| start = 20240417-04-17 09:10:00
| end = 20240417-04-17 09:50:00
| impact = According to Traffic's graphs, from HAproxy's POV, non 5XX requests (text+upload) dropped from 138K rps, to 120K - 130K rps for 20'
}}

[https://phabricator.wikimedia.org/T346690 mcrouter daemonset on mw-on-k8s:] The mediawiki pod has 9 containers. We were working on reducting this number to 7, by introducing the mw-mcrouter service. In practice, our end goal was that each mw-on-k8s pod would use a standalone mcrouter pod running within the same node, instead of its own mcrouter container. From mediawiki's POV, mcrouter's location would be <code>mcrouter-main.mw-mcrouter.svc.cluster.local:4442</code> instead of <code>127.0.0.1:11213</code>. The same change was deployed on codfw the day before, but codfw has less traffic. 

This change increased the number of DNS requests towards CoreDNS, from an average of 40k req/s to 110k req/s, overwhelming the pods.",,Antoine noticed an elevated number of events coming from the mediawiki channel on logstash. A few minutes later we got our first alert that we are running out of available php workers.,"SAL entry: https://sal.toolforge.org/log/yk9Q644BGiVuUzOdxNwu

''All times in UTC.''

* '''09:08''' effie runs scap sync-world to deploy mediawiki deployments: use mcrouter daemonset for both DCs T346690
* '''09:10''' antoine observes a higher level of mw related events arriving to logstash
* '''09:18 OUTAGE BEGINS'''
* '''09:18''' ALERT: (PHPFPMTooBusy) firing: (3) Not enough idle PHP-FPM workers for Mediawiki mw-api-ext at eqiad: 30.2% idle
* '''09:26''' ALERT: (MediaWikiLatencyExceeded) firing: (4) p75 latency high: eqiad mw-api-ext (k8s) 6.79s
* '''09:44''' claime depools eqiad in mw-web-ro, mw-api-ext-ro, mw-api-int-ro
* '''09:44 OUTAGE ENDS'''
* '''10:00''' akosiaris manually bumps coredns pods to 6 (eqiad+codfw)
* '''10:16''' effie merges and deploys any relevant code changes and reverts <nowiki>https://gerrit.wikimedia.org/r/1020768</nowiki> and <nowiki>https://gerrit.wikimedia.org/r/1020774</nowiki>
* '''10:53''' effie pools back eqiad for mw-web-ro, mw-api-ext-ro, mw-api-int-ro","===What went well?===

* Everyone in ServiceOps was around
* Janis quickly figured out that we were missing the final dot in the FQDN","TBA

<mark>Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFire]]  Phabricator tag to these tasks.</mark>",2024-04-17,Unknown,Unknown,Mediawiki,Unknown,Unknown,2024-04-17_mw-on-k8s_eqiad_outage.wikitext
"{{Incident scorecard
| task = T363516
| paged-num = 0
| responders-num = 5
| coordinators = Brian King
| start = 2024-04-24 19:27 UTC
| end = 2024-04-26 13:31 UTC
| impact = During the outage window, users connected to the primary (eqiad) datacenter may have had incomplete search results, messages saying ""try your search again,"" and/or a lack of autocomplete.
}}

</mark>

{{TOC|align=right}}",,"<mark>Write how the issue was first detected.  Was automated monitoring first to detect it? Or a human reporting an error?</mark>

Humans reported an error via [[phab:T363516|this Phab task]]","''All times in UTC.''

*[[File:Envoy errors.png|alt=graph showing increased error rates for requests to the Elastic cluster|thumb|graph showing increased error rates for requests to the Elastic cluster]]''2024-04-24 20:22'' Search platform SREs merge [[gerrit:c/operations/puppet/+/1023937|this change]] and pool the new hosts, bringing <code>elastic110[3-7]</code> into the search cluster. Upstream error rate (as recorded from Envoy) begins to rise from < 0.1% to above 1%.
*''2024-04-25 03:13'' Daily index update job for English Wikipedia fails at 44% due to connection problems from the maintenance host ''mwmaint1002'' to the production eqiad Search cluster. Unfortunately, the incomplete index is promoted to production. Missing search results are reported by users in [[phab:T363516|this Phab task]].
*''2024-04-25 21:18'' English Wikipedia index rebuild finishes successfully. Users now get a complete set of results, but error rate remains unacceptably high at ~1%.
*''2024-04-26 09:36''  Dcausse (Search Platform SWE) points autocomplete Search traffic from the primary to secondary datacenter (codfw).
*''2024-04-26 13:31'' At Dcausse's request, Akosiaris (SRE) depools the hosts that were pooled in step 1. Error rate drops back to normal, '''ending user impact'''. Ebernhardson, Dcausse and Bking (Search platform SWE/SRE) continue troubleshooting.

*''2024-04-26 16:34'' Bking repools ''elastic1105'' , which causes the error rate to shoot up again. Ebernhardon (Search Platform SWE) runs connectivity tests from ''mwmaint1002'';  they show a ~5% failure rate.
*''2024-04-26 16:55'' Cmooney (SRE, Network SME) joins troubleshooting call. With his help, we're able to isolate the connection problems down to 2 hosts: elastic1105 and 1107. He identifies missing VLAN sub-interfaces on LVS load-balancer hosts and [[gerrit:c/operations/puppet/+/1024776|pushes a puppet patch to correct this]].
*''2024-04-26 17:19'' Cmooney completes push of changes to lvs1019 and validates that the required connectivity is now in place and working so elastic1105 and elastic1107 are reachable from it at L2.
*''2024-04-26 17:28'' Bking repools elastic1103-1107 and confirms that no new errors have been logged. Ebernhardson and Bking decide to wait until Monday to repool Search autocomplete at the datacenter level.
*''2024-04-29 13:30'' Switch autocomplete Search traffic back to eqiad","===What went well?===

* Part of the problem (incomplete search results) was detected and mitigated quickly.","* [[phab:T363702|Phab task for follow-up]]
** [[phab:T363609|Elasticsearch: Alert on Downstream Errors]]
** [[phab:T363702|Create alert for LVS if it is configured for unreachable back-end server IPs.]]",2024-04-26,Unknown,Monitoring,Unknown,Unknown,Unknown,2024-04-26_Search_unavailable_for_some_eqiad_users.wikitext
"{{Incident scorecard
| task = T363709
| paged-num = 1
| responders-num = 2
| coordinators = [[User:FNegri]]
| start = 2024-04-28 13:39
| end = 2024-04-28 14:36
| metrics = No relevant SLOs exist
| impact = Toolforge tools using Redis suffered intermittent connection errors for the duration of the incident.
}}

Toolforge Redis refused new connections because there were too many active connections. This happened intermittently for 1 hour until the service was restarted manually.

{{TOC|align=right}}",,Toolschecker detected a problem with Redis and Icinga fired an alert. This was routed to Alertmanager and to VictorOps. VictorOps sent a page to the on-call engineer ([[User:FNegri]]).,"''All times in UTC.''

* 13:39 '''OUTAGE BEGINS'''. Toolschecker detects a problem and sends a page
* 13:44 Francesco Negri acks the page and starts investigating. During the investigation (no actions performed) the alert resolves by itself, then triggers again, for 6 times.
* 14:15 redis-cli info replication shows ""slave"" on 2 hosts (tools-redis-[56]), and ""ERR max number of clients reached"" on the third host (tools-redis-7)
* 14:24 Taavi Väänänen suggests restarting the service
* 14:26 A Toolforge user (Sohom Datta) reports in #wikimedia-cloud that their tool is unable to connect to Redis
* 14:35 Francesco Negri fails over Redis from tools-redis-7 to tools-redis-6
* 14:36 Francesco Negri restarts the service on tools-redis-7 with systemctl restart redis-instance-tcp_6379.service
* 14:36 '''OUTAGE ENDS.'''","===What went well?===

* The alert fired instantly, and we started looking at the issue before receiving any reports from end users.",* {{done}} [[phab:T363709]],2024-04-28,Unknown,Unknown,Unknown,Unknown,Unknown,2024-04-28_WMCS_Toolforge_Redis_refusing_connections.wikitext
"{{Incident scorecard
| task = T366094
| paged-num = 4
| responders-num = effie, hugh, alex, kamila, marostegui, cdanis, rzl
| coordinators = Kamila, Reuven
| start = 2024-05-28 12:23
| end = 2024-05-28 17:00
| metrics = No relevant SLOs exist. The only metric that comes to mind as relevant is deployment failures. A possible future SLO is Kubernetes API availability.
| impact = No impact for wiki users. 4 MediaWiki deployments failed. A global scap lock was put into place as a precautionary measure against bad UX for deployers
}}

<!-- Reminder: No private information on this page! -->Triggered by a failed scap deployment of MediaWiki, the team dug into figuring out the underlying causes, unearthing a network and CPU saturation at the WikiKube Kubernetes cluster api servers. Aside from the issue being witnessed by 2 deployers, no other discernible impact was observed. As an inadvertent result of the investigation, scap deployments became faster.

They were multiple interrelated causes that contributed and made debugging slower

# For the past few months, during any MW deploy, we've been saturating k8s control plane CPU in both [https://grafana-rw.wikimedia.org/goto/UzvUYZsIR?orgId=1 eqiad] and [https://grafana-rw.wikimedia.org/goto/GxZlYWyIR?orgId=1 codfw]. [[File:2024-05-28 k8s codfw cpu saturation.png|alt=A graph showing slowly increase CPU saturation for codfw api servers|thumb|A graph showing slowly increasing CPU saturation for codfw api servers]][[File:2024-05-28 api eqiad.png|alt=A graph showing slowly increase CPU saturation for eqiad api servers|thumb|A graph showing slowly increasing CPU saturation for eqiad api servers]]The saturation also manifested as [https://grafana.wikimedia.org/goto/hIvtEZsSg?orgId=1 slowly increasing latencies for blackbox probes]
# More and more worker servers were being added to the WikiKube clusters
# 1 of the kubemaster VMs was on a 1G Ganeti node, the other one on a 10G ganeti node. After the first incident, the second kubemaster (the one on 1G ganeti node) took over part of the load. Network wise, it was unable to sustain the level of traffic and ended up having ~6k TCP retransmits per second.
# At the end of the week before the incident, we had enabled OpenTelemetry collector in eqiad, following a successful deployment in codfw
# We were in the process of adding more kubernetes api-servers in both datacenters.[[File:2024-05-28-blackbox-probe-latency.png|alt=Slowly increasing blacbox probe latency|thumb|Slowly increasing blackbox probe latency]]
{{TOC|align=right}}",,"The issue was detected due to a failed deployment and this necessitates right a human witness. There were alerts of the following nature<syntaxhighlight lang=""bash"">
FIRING: ProbeDown: Service kubemaster1002:6443 has failed probes
</syntaxhighlight>These alerts have been firing for some time every now and then. However, 

# Due to the very slow increasing nature of the underlying cause (latency due to CPU saturation) it wasn't possible to correlate it. Furthermore when they accompanied caused an incident, corrective action was already being taken (moving Kubernetes masters to dedicated machines)
# The CPU saturation events were short enough to not show up in the ""Host Overview"" Grafana dashboard. This effect was exacerbated by the fact the CPU panels in that dashboard use a resolution of 1/3.","''All times in UTC. Incident investigation spans 3 days.'' 

* 12:23 <TimStarling> ^ trying to deploy a single-file change but k8s is very slow
* 12:23  <TimStarling> 12:23:11 Command '['helmfile', '-e', 'eqiad', '--selector', 'name=main', 'apply']' returned non-zero exit status 1.

* 12:35:09  <TimStarling> Started sync-prod-k8s [retry]
* First page: [12:49:12] '''<+jinxer-wm>''' FIRING: ProbeDown: Service kubemaster1002:6443 has failed probes (http_eqiad_kube_apiserver_ip6) #page - [[Runbook#kubemaster1002:6443|https://wikitech.wikimedia.org/wiki/Runbook#kubemaster1002:6443]] - <nowiki>https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/custom&var-module=All</nowiki> - <nowiki>https://alerts.wikimedia.org/?q=alertname%3DProbeDown</nowiki> '''INCIDENT BEGINS'''
* [12:52:08] '''<hnowlan>''' effie and I are looking at this, unclear what's wrong. pybal is failing to connect to the masters 
* [12:52:12] A recovery arrives but it is not yet clear why, '''INCIDENT ENDS'''
* Another page: [13:23:12] '''<+jinxer-wm>''' FIRING: ProbeDown: Service kubemaster1002:6443 has failed probes (http_eqiad_kube_apiserver_ip6) #page - [[Runbook#kubemaster1002:6443|https://wikitech.wikimedia.org/wiki/Runbook#kubemaster1002:6443]] - <nowiki>https://grafana.wikimedia.org/d/O0nHhdhnz/network-probes-overview?var-job=probes/custom&var-module=All</nowiki> - <nowiki>https://alerts.wikimedia.org/?q=alertname%3DProbeDown</nowiki> '''INCIDENT BEGINS''' 
* 13:27:52:  Incident opened. '''Kamila''' becomes IC
* 13:46:28 <cdanis> kamila_: scap lock is now in place per effie
* 13:50  <akosiaris> ok first thing's first, apparently firewall for etcd isn't open on wikikube-ctrl1001
* 13:54 akosiaris manually opened firewall port, cluster-health happy now – but probably a red herring
* 14:10 cdanis notices TCP retransmits, new issue, time seems to correlate – https://grafana.wikimedia.org/d/000000377/host-overview?orgId=1&refresh=5m&var-server=kubemaster1002&var-datasource=thanos&var-cluster=kubernetes&from=1716895422330&to=1716906222330&viewPanel=31
* 14:24 <akosiaris> the VM maxed out the veth https://grafana.wikimedia.org/d/000000377/host-overview?orgId=1&refresh=5m&var-server=kubemaster1002&var-datasource=thanos&var-cluster=kubernetes&viewPanel=8&from=1716895728975&to=1716906528975
* 14:22 effie deploys https://gerrit.wikimedia.org/r/1036680
* 14:30 cdanis looks at retransmits
* 14:30 <cdanis> so that looks like traffic to *other* k8s api servers and also from etcd that is being retransmitted
* 14:30 <cdanis> so kinda everything
* 14:56 <akosiaris> !log migrate kubemaster1002 to ganeti1037 [10G NIC, was 1G]
* <akosiaris>	Tue May 28 15:00:28 2024 * memory transfer progress: 113.26 %
* 15:02:20	<cdanis>	kubemaster1001 just saturated
* 15:02:23	<cdanis>	briefly
* 15:03:04	<akosiaris>	all I did was to restart kube-apiserver on kubemaster1002 fwiw
* 15:03:20	<cdanis>	ok so just startup then probably
* 15:03:20	<akosiaris>	which adds credence to your theory that we were maxing out on both
* 15:03:53	<akosiaris>	VM migration done, I 'll start up the test shortly
* 15:12	akosiaris rolling-restarts kube-apiserver; NIC no longer saturated
* 15:24	rzl becomes IC
* 15:58	cdanis identifies that the kubemaster IPs are hardcoded in the otelcol values file, as of the end of last week, so it can only talk to the legacy hosts (10.64.0.117, 10.64.32.116)
* 16:00	cdanis uninstalls opentelemetry-collector in eqiad (later merged as https://gerrit.wikimedia.org/r/1036707)
* 16:08	Amir runs a scap backport, which saturates kubemaster1001 CPUs again but not for long enough that they either fail probes or reach scap’s timeout
* 17:00	Decision to leave things as-is over the EU nighttime, since they’re not ideal but workable for deployments. rzl stands down as IC '''INCIDENT ENDS'''
[[File:2024-05-28 k8s api tests.png|alt=Graphs showing behavior during tests|thumb|Graphs showing behavior during tests]]","Growing pains for the WikiKube cluster during the mw-on-k8s project were expected. There had already been Kubernetes API related incidents and the team, after an investigation, concluded that adding more etcd capacity, in the form of 3 new dedicated stacked API servers would alleviate those problems. However,

* It appears there were also short CPU saturation problems that were never directly detected that caused eventually a few failed deployments.
* The default configuration of the OpenTelemetry collector with [https://github.com/open-telemetry/opentelemetry-collector-contrib/blob/main/processor/k8sattributesprocessor/README.md k8sattributesprocessor] enabled increased the level of network traffic egressing API servers by > 50%. Cumulative traffic egressing from all API servers reached 272MB/s (~2.1Gbps), enough to saturate heavily any 1Gbps link.
* There are 2 different methods of load balancing regarding traffic to the API servers. One from workloads outside the cluster itself (i.e. kubelet, scap, kube-proxy, rsyslog) and one from workloads inside the cluster, i.e. OpenTelemetry collector, Calico, eventrouter (we call these cluster components). The former go via LVS, the latter via Kubernetes probabilistic DNAT. The result is that the 2 methods might end up sending traffic to different sets of nodes, complicating things.","* A decision was made to add 10G cards to all new WikiKube api servers. [[phab:T366204|T366204]] and [[phab:T366205|T366205]]. 
* Poke otelcol upstream about using the Kubelet /pods interface  {{Done}}  https://github.com/open-telemetry/opentelemetry-collector-contrib/issues/14475#issuecomment-2145825911 
* Investigate whether the resolution of https://grafana.wikimedia.org/d/000000377/host-overview?orgId=1&refresh=5m&viewPanel=3 can be 1/1 instead of 1/3?

<mark>Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFire]]  Phabricator tag to these tasks.</mark>",2024-05-28,Unknown,Unknown,Unknown,Unknown,Unknown,2024-05-28_wikikube-api.wikitext
"{{Incident scorecard
| paged-num = 2
| responders-num = 7
| coordinators = kamila_
| start = 2024-05-31 10:47:4
| end = 2024-05-31 12:39:53
| metrics = SLOs exist for Varnish and ATS, but these were still met
| impact = Increase in response times for users using eqsin
}}

Hotlinking of an image on Commons caused link saturation in the eqsin datacentre. 

{{TOC|align=right}}",,"This incident was detected via paging for port utilisation in eqsin: <syntaxhighlight lang=""irc"">
<+jinxer-wm> FIRING: Primary outbound port utilisation over 80%  #page: Alert for device asw1-eqsin.mgmt.eqsin.wmnet - Primary outbound port utilisation over 80%  #page
</syntaxhighlight>Additionally FastNetMon detected what it perceived as a DDoS. This was more or less correct as the behaviours witnessed are similar to a simple DDoS attack.","<mark>Link to a specific offset in SAL using the SAL tool at https://sal.toolforge.org/ ([https://sal.toolforge.org/production?q=synchronized&d=2012-01-01 example])</mark>

''All times in UTC.''

*10:47 Page for port utilisation arrives  '''OUTAGE BEGINS'''
*11:14: VictorOps page: DDos Detected (eqsin)
*12:01 hnowlan adds requestctl/request-actions/cache-upload/hotlink_from_jio_blomen.yaml and requestctl/request-patterns/req/cache_buster_nnn.yaml. No effect
*12:08 Incident opened. Kamila becomes IC
*12:33 hnowlan manually deploys [[gerrit:c/operations/puppet/+/1037779|varnish frontend rule]] 
*12:48 All damaging requests for URL in question are receiving HTTP 429 in response 
*12:39 <+jinxer-wm> RESOLVED: DDoSDetected: FastNetMon has detected an attack on eqsin #page - <nowiki>https://bit.ly/wmf-fastnetmon</nowiki> - <nowiki>https://w.wiki/8oU</nowiki> - <nowiki>https://alerts.wikimedia.org/?q=alertname%3DDDoSDetected</nowiki> '''OUTAGE ENDS'''","This was a somewhat familiar pattern, as we have seen similar issues in the past on a larger scale.","* …

<mark>Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.</mark>

<mark>Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFire]]  Phabricator tag to these tasks.</mark>",2024-05-31,Unknown,Unknown,Unknown,Unknown,Unknown,2024-05-31_Image_hotlinking.wikitext
"{{Incident scorecard
| task = T367113
| paged-num = 0
| responders-num = 3
| coordinators = 
| start = 2024-04-27 00:00:00
| end = 2024-06-10
| impact = Unknown.
}}


<!-- Reminder: No private information on this page! -->Something happened between 2024-04-27 00:00 and 2024-04-27 00:08 UTC, and the rsync clients of most puppetmasters/puppetservers to sync data from puppetmaster1001 hung indefinitely.  On 2024-06-10, they were killed and restarted manually.

This means that new data from puppet 'volatile' was only rarely/intermittently synced to much of the fleet during this window.

The problem was particularly bad in codfw, where all 3 puppetservers had failed to rsync data for the entire duration.

Getting a firm idea of the impact of this is difficult.  The new GeoLite2 files were unavailable, but also not in use in production yet(?).  The older 'enterprise' file was still being used for many uses, however, and would have grown stale.  Analytics (a heavy user of GeoIP data) was probably not very affected because it is eqiad-only.  Any CheckUser calls would have likely been affected by stale data.  

On the other hand, aside from the one service wishing to use newly-added files ... no one noticed?  So this puts a sort of upper bound on the potential impact, however unsatisfying.  

{{TOC|align=right}}",,"Manual. 

Kosta Harlan asked on #wikimedia-sre-foundations IRC to confirm that the new GeoLite2 files were available, as part of work on https://phabricator.wikimedia.org/T366272.  cdanis began investigating and discovered that the files were missing on most hosts in codfw where they were expected to exist: https://phabricator.wikimedia.org/P64540","<mark>Write a step by step outline of what happened to cause the incident, and how it was remedied.  Include the lead-up to the incident, and any epilogue.</mark>

<mark>Consider including a graphs of the error rate or other surrogate.</mark>

<mark>Link to a specific offset in SAL using the SAL tool at https://sal.toolforge.org/ ([https://sal.toolforge.org/production?q=synchronized&d=2012-01-01 example])</mark>

''All times in UTC.''

*00:00 (TODO) '''OUTAGE BEGINS'''
*00:04 (Something something)
*00:06 (Voila) '''OUTAGE ENDS'''
*00:15 (post-outage cleanup finished)

<!-- Reminder: No private information on this page! -->
<mark>TODO: Clearly indicate when the user-visible outage began and ended.</mark>","===What went well?===

*","* Add a generous default TimeoutStartSec in systemd::timer::job.  It cannot be infinite.
* Enable monitoring and logging for the systemd::timer::jobs defined in [[git:operations/puppet/+/production/modules/puppetmaster/manifests/rsync.pp|modules/puppetmaster/manifests/rsync.pp]]
* Ensure that timer failures for sync-puppet-volatile get reported somewhere (#-sre-foundations IRC?)
*

<mark>Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.</mark>

<mark>Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFire]] Phabricator tag to these tasks.</mark>",2024-06-10,Unknown,Unknown,Unknown,Unknown,Unknown,2024-06-10_puppet_volatile_data_broken_sync.wikitext
"{{Incident scorecard
| task = T367191
| paged-num = 1
| responders-num = 6
| coordinators = taavi
| start = 2024-06-11 14:58
| end = 2024-06-11 15:23
| metrics = WMCS services do not have SLOs, so no relevant SLOs exist.
| impact = For about 25 minutes Cloud VPS and all services hosted on it (incl. Toolforge) were completely inaccessible.
}}

A faulty optic on one of the fiber links between WMCS racks caused packet loss between nodes in the Cloud VPS [[Ceph]] storage cluster. This made writes any writes stall since Ceph could not confirm those writes had been committed on all nodes they were supposed to on. Cloud VPS VMs cannot handle their storage hanging like this and stalled too, which made any services hosted on Cloud VPS inaccessible for the duration of the incident.

{{TOC|align=right}}",,"Automated alerting noticed the issue - the first alert was a warning that pointed towards a Ceph issue of some sort at 14:58:09, and a page was sent out from a toolschecker NFS alert about four minutes later (15:01:50). The first human report arrived on IRC several minutes later:<syntaxhighlight lang=""irc"">
15:08:35 <Lucas_WMDE> it looks like there might be connection issues at the moment? I can’t connect to tools via HTTPS nor SSH
</syntaxhighlight>

The initial alerts located the issue well, although they were followed by a high volume of generic ""VPS instance"" down alerts (on IRC and via email).

<mark>TODO: did metricsinfra or its meta check send any pages? if not, why?</mark>","[[File:T367191 faulty optic.png|thumb|Interface errors on the affected switch interface as seen on LibreNMS]]
''All times in UTC.''
*Starting from ~2024-06-10 23:00, there's an increase on errors reported on [https://librenms.wikimedia.org/device/device=185/tab=port/port=23292/ cloudsw1-d5-eqiad:et-0/0/53] (connected to [https://librenms.wikimedia.org/device/device=242/tab=port/port=25230/ cloudsw1-f4-eqiad:et-0/0/54]). As well as linking hosts in d5 and f4 (<mark>TODO: and possibly E4 and F4? need to check</mark>), this is the [[Network design - Eqiad WMCS Network Infra#Cloud Instances Vlan|active link used to connect the stretched cloud-instances VLAN]] to the F4 rack. These errors are happening in bursts, until..
*2024-06-11 14:58:09 First alert: <+jinxer-wm> FIRING: CephSlowOps: Ceph cluster in eqiad has 4 slow ops - <nowiki>https://wikitech.wikimedia.org/wiki/Portal:Cloud_VPS/Admin/Runbooks/CephSlowOps</nowiki> - <nowiki>https://grafana.wikimedia.org/d/P1tFnn3Mk/wmcs-ceph-eqiad-health?orgId=1&search=open&tag=ceph&tag=health&tag=WMCS</nowiki> - <nowiki>https://alerts.wikimedia.org/?q=alertname%3DCephSlowOps</nowiki>  '''OUTAGE STARTS'''
*15:01:50 First page: <+icinga-wm_> PROBLEM - toolschecker: NFS read/writeable on labs instances on checker.tools.wmflabs.org is CRITICAL: HTTP CRITICAL: HTTP/1.1 504 Gateway Time-out - string OK not found on <nowiki>http://checker.tools.wmflabs.org:80/nfs/home</nowiki> - 324 bytes in 60.004 second response time <nowiki>https://wikitech.wikimedia.org/wiki/Portal:Toolforge/Admin/Toolschecker</nowiki>
*~15:02 Taavi notices the alerts during the Toolforge monthly meeting and asks David to investigate. That meeting is subsequently postponed by a week.
*15:04 David pings Cathal on IRC about a possible network issue. Cathal is in the middle of an another switch upgrade but starts looking
*15:09 Taavi declares an incident and becomes an IC
*15:10 Meet room is opened for incident response coordination
*15:15 David sets the Ceph cluster in norebalance mode to prevent the cluster from moving things around for now
*15:18 Alert for OOM killer activating on cloudcephmon1001. <mark>TODO: not sure for the impact of this?</mark>
*15:20 Arturo runs script on all Ceph nodes to try to determine patterns to pin down the issue. <mark>TODO: was this succesful in locating the issue?</mark>
*15:2x Cathal notices high numbers of errors on the affected interfaces, and disables <mark>TODO: the interfaces? BGP?</mark> to move traffic to other links. This isn't immediately communicated to the WMCS team debugging the issue on the Meet room. '''OUTAGE ENDS'''
*15:23 Alerts start recovering.
*15:26 Cathal moves the cloud-instances VLAN links to E4 and F4 from D5 to C8
*15:50 Taavi starts cookbook to reboot all Toolforge NFS-enabled workers nodes
*16:40-17:10 DC-Ops replaces faulty optic","===What went well?===
* Automated monitoring noticed the issue fast and provided useful pointers where to look
* Ceph handled the network degradation relatively well and quickly recovered once traffic was shifted to alternative links
* After previous Toolforge NFS issues, the tooling built for recovering from those (by restarting worker nodes) worked very well","* {{done}} [[phab:T367199|T367199]] Replace the faulty optic
* Figure out if we should alert for interface errors like this one
* {{done}} [[phab:T367336]] Add both sides of the links to discard/error graphs for router connectivity to the ceph dashboards

<mark>Create a list of action items that will help prevent this from happening again as much as possible. Link to or create a Phabricator task for every step.</mark>

<mark>Add the [[phab:project/view/4758/|#Sustainability (Incident Followup)]] and the [[phab:project/profile/4626/|#SRE-OnFire]]  Phabricator tag to these tasks.</mark>",2024-06-11,Unknown,Unknown,Unknown,Unknown,Unknown,2024-06-11_WMCS_Ceph.wikitext
"{{Incident scorecard
| task = T367348
| paged-num = N/A
| responders-num = 5 people: Taavi, David, Francesco, Andrew, Arturo
| coordinators = Andrew Bogott
| start = 2024-06-12 16:35
| end = 2024-06-12 17:13
| metrics = No relevant SLOs exist.
| impact = All Toolforge users were impacted. While running webservices/jobs were somewhat alive and reachable, no jobs or webservices could be created, or operated in any way.
}}

* System-wide renaming of Kyverno policies overloaded Toolforge Kubernetes control nodes rendering API unresponsive
* Lack of working Kubernetes API prevented rollback
{{TOC|align=right}}",,"This issue was detected by Arturo -- during ongoing deployment work he noticed that the k8s API had become unresponsive.

Some alerts followed but only after the incident was open and fixes in progress.","14:00 (approximately) resource limits for kyverno are lifted in response to frequent kyverno pod death https://gitlab.wikimedia.org/repos/cloud/toolforge/toolforge-deploy/-/commit/682256e974c5f17f01c1838bb057de9eefeeb492

16:35 Arturo applies https://gitlab.wikimedia.org/repos/cloud/toolforge/maintain-kubeusers/-/merge_requests/42 which results in an epic (and slow) renaming of many resources. This combined with lifted resource limits on Kyverno allows '''Kyverno to overload the k8s api server'''

16:40 Incident opened. Andrew  becomes IC.

16:44 Arturo gets shell access on tool control nodes, removes kyverno policies

16:46 Some recovery (test services load) but API remains unresponsive due to CPU overload

16:50 Taavi resizes k8s control nodes from 4 CPUs to 8 CPUs (tools-k8s-control-[7,8,9])

17:00 Lots of kubelet errors saying ‘node not found,’ services failing to register, all services fail on the registry admission webhook

17:04 taavi fixes the registry webhook (maybe?)

17:09 taavi forces everything onto tools-k8s-control-7 which was working, that results in all nodes recovering

17:13 '''incident resolved'''

17:25: maintain-kubeusers re-enabled (but kyverno left turned off for now)","===What went well?===

* Quick response, good teamwork, relatively quick diagnosis of the issue","* Fix HA proxy load-balancer health check monitor to not poll nodes where the API is not responding ([[phab:T367349]])
* Fix incorrect registry controller config  https://gitlab.wikimedia.org/repos/cloud/toolforge/registry-admission/-/merge_requests/5
* Do some more load testing with kyverno before deploying
* scale up coredns replicas ([[phab:T333934]])
* Verify that kyverno policies match our namespace ([[phab:T367350]])",2024-06-12,Unknown,Unknown,Api,Unresponsive,Unknown,2024-06-12_WMCS_toolforge_k8s_control_plane.wikitext
